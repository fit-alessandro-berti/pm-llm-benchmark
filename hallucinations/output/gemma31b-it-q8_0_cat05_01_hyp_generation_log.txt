{
  "categories": {
    "1a_instruction_override": {
      "count": 2,
      "instances": [
        {
          "snippet": "The suggestion to add \"Query Type Guidance\" ... contradicts the original instruction: '...without any hints or guidance provided here.'",
          "why": "The evaluation proposes adding guidance to the LLM prompt which explicitly prohibits any hints or guidance, thus overriding the original prompt constraint.",
          "severity": "medium"
        },
        {
          "snippet": "Suggesting the LLM be prompted for 'ways to resolve' potential errors ... goes beyond the explicitly stated goals ...",
          "why": "The evaluation recommends adding error resolution steps, which exceeds the prompt's defined scope of identification, hypothesis generation, and investigation, ignoring the explicit task boundaries.",
          "severity": "medium"
        }
      ]
    },
    "1b_context_omission": {
      "count": 1,
      "instances": [
        {
          "snippet": "Doesn't strongly emphasize that sample data already contain significant deviations ...",
          "why": "By omitting explicit discussion of anomalies already present in sample data, the evaluation drops crucial contextual information relevant to the prompt's anomaly detection task.",
          "severity": "medium"
        }
      ]
    },
    "1c_prompt_contradiction": {
      "count": 1,
      "instances": [
        {
          "snippet": "The suggestion to add 'Query Type Guidance' ... slightly contradicts the original instruction forbidding any guidance.",
          "why": "The evaluation contradicts the prompt by simultaneously respecting and ignoring the 'no guidance' instruction, effectively stating opposing positions.",
          "severity": "medium"
        }
      ]
    },
    "2a_concept_fabrication": {
      "count": 0,
      "instances": []
    },
    "2b_spurious_numeric": {
      "count": 0,
      "instances": []
    },
    "2c_false_citation": {
      "count": 0,
      "instances": []
    },
    "3a_unsupported_leap": {
      "count": 0,
      "instances": []
    },
    "3b_self_contradiction": {
      "count": 0,
      "instances": []
    },
    "3c_circular_reasoning": {
      "count": 0,
      "instances": []
    },
    "4a_syntax_error": {
      "count": 0,
      "instances": []
    },
    "4b_model_semantics_breach": {
      "count": 0,
      "instances": []
    },
    "4c_visual_descr_mismatch": {
      "count": 0,
      "instances": []
    }
  },
  "totals": {
    "hallucinations_overall": 4
  }
}