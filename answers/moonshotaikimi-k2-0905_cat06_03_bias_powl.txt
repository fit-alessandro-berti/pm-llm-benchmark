The XOR “Check-Local-Affiliation / skip” branch quietly enriches some applicants with extra information that the final scoring algorithm can see, while withholding it from others.  
Because the check is optional, the lender can (and usually will) code the result as one more predictor:  

- Local-resident-and-community-member = 1  
- All others = 0 (missing)  

Modern ML pipelines treat missing values as “zero information”, but here the absence is **not** random; it is systematically produced when the applicant’s post-code or club-membership does not look familiar to the caseworker.  
The model therefore learns an implicit rule:

> “If LocalAffiliation-flag is missing  lower proximity  raise rate or decline.”

The loop is closed: the very people who are least likely to be offered the check are now penalised for not having the flag, while the ones who are offered it receive a small score uplift—“small” enough to escape regulatory dashboards, yet large enough to change the decision for borderline cases.

What makes the bias “subtle” is that **no legally protected attribute (race, nationality, religion, sex, age, etc.) is ever written on the file**.  
Instead the proxy is a **voluntary community-group membership**—a trait that is:

- Correlate-heavy: the same ethnic, migrant or religious networks that organise local credit-union branches, sports clubs or place-of-worship groups;  
- Correlate-weak: correlation  perfection, so the pattern never reaches the 0.8 p-value that would trigger a Fair-Lending probe;  
- Concealment-friendly: the variable can be labelled “community engagement score” and sold to auditors as “behavioural” rather than demographic.

Consequences for fairness and equity

1. Red-lining by another name  
   Whole postcode areas where few residents pass the “local affiliation” filter gradually see higher decline rates. The map of rejected applications re-creates historical red-line boundaries without ever mentioning race or nationality.

2. Feedback loop with economic stratification  
   Once the model starts declining more applicants from neighbourhood X, fewer loans are issued there, property prices stagnate, community groups lose fee income, and the next year’s data contains even fewer “1” flags—ratifying the original bias.

3. Individual harm without legal remedy  
   An applicant who is neither a local resident nor a club member cannot claim discrimination on a protected ground; the flag is formally open to anyone who “could have joined”. The burden is shifted to the consumer to prove that the club is inaccessible, an almost impossible evidentiary task.

4. Competitive distortion  
   Lenders that decline to use such soft features appear to have higher default rates (because their pool contains relatively more unknown-community borrowers) and are pressured to adopt the same proxy—an arms race that ends with universal adoption of the biased variable.

Operational mitigations  

- Make the check **mandatory** or remove the XOR: every applicant is evaluated on the same attributes.  
- If the check must stay optional, encode the result as a **three-level factor** {yes / no / not-checked} and include the third level explicitly in training so the model does not treat “missing” as “no”.  
- Calibrate the uplift so that it is **zero-sum**: any score bonus given to the “yes” group is financed by an equal discount taken from the pool, keeping the overall acceptance rate constant—this surfaces the real cost of the policy.  
- Run counterfactual fairness tests: ask whether the decision for each borderline applicant would flip if the LocalAffiliation flag were flipped; if so, scrutinise the rule.  
- Publish the *conditional acceptance rates* by geography and club-membership status; sunlight deters hidden red-lining.

Bottom line  
Optional, seemingly benign attributes such as “member of a local community group” are low-cost channels for importing historical segregation into 21st-century models. Because they are not legally protected, they evade conventional fair-lending audits, yet they produce the same disparate impact. Fairness requires either equalising the information collected or proving that the incremental benefit is uncorrelated with protected traits—an onus the current XOR structure does not satisfy.