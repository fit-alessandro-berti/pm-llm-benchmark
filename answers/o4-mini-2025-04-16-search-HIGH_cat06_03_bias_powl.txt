The XOR branch between checking local affiliation (D) and skipping that step embeds a hard-coded “preference” for applicants from certain geographic or community groups, producing a subtle form of algorithmic bias. By design, applicants routed through D receive a small but tangible score uplift—those sent down the “skip” branch do not. Though location is not a legally protected characteristic, privileging it in this way violates core fairness principles and risks disparate treatment of non-local applicants.

1. Algorithmic Bias Through Branching  
   By splitting applicants into two streams—with only one stream receiving the opportunity for a beneficial adjustment—the process embeds a systematic preference. This aligns with the definition of algorithmic bias as “systematic and repeatable errors that create unfair outcomes, such as privileging one arbitrary group of users over others” ([en.wikipedia.org](https://en.wikipedia.org/wiki/Algorithmic_bias?utm_source=chatgpt.com)). Since the choice is deterministic (XOR) rather than random, it guarantees that some individuals will always be advantaged and others disadvantaged, independent of their actual credit risk.

2. Proxy Discrimination via Non-Protected Attributes  
   Although local affiliation is facially neutral, it often correlates with income, race, or other protected traits—a phenomenon known as proxy discrimination. In proxy discrimination, “bias in the data, or a trained ML model, is expressed not directly via sensitive attributes, but indirectly, via proxy variables … residence or location attributes serving as proxies for the race sensitive attribute” ([arxiv.org](https://arxiv.org/html/2404.19371v1/?utm_source=chatgpt.com)). Thus, even without explicit use of protected characteristics, the local-check branch can perpetuate existing societal inequities.

3. Indirect Discrimination and Legal Risk  
   U.S. fair-lending laws like the Equal Credit Opportunity Act (ECOA) prohibit not only overt discrimination on protected grounds but also practices that cause a disparate impact on protected groups. “Indirect discrimination can happen … when residential zip code is … not categorized as a protected attribute, but from the zip code one may find out about race which is a protected attribute” ([sigai.acm.org](https://sigai.acm.org/main/2020/02/20/discrimination-and-bias/?utm_source=chatgpt.com), [scribd.com](https://www.scribd.com/document/662990246/Bias-and-Fairness?utm_source=chatgpt.com)). If non-local applicants disproportionately belong to protected or underserved communities, they will face lower approval rates—exposing lenders to legal liability.

4. Violation of Fairness Criteria  
   Fairness frameworks underscore the problem:
   - Demographic parity demands that approval rates be independent of protected attributes.  
   - Equal opportunity requires that true positive rates be equal across groups.  
   By awarding a local-affiliation bonus, the model undermines both criteria, since locals (even if unprotected) are systematically more likely to be approved or receive better terms. “Fairness criteria are mathematical definitions used to assess whether an algorithm is treating different groups equitably” ([library.fiveable.me](https://library.fiveable.me/digital-transformation-strategies/unit-12/algorithmic-bias-fairness/study-guide/Bvktz8luMvDYbLCw?utm_source=chatgpt.com), [bookdown.org](https://bookdown.org/paul/ai_ml_for_social_scientists/05_01_bias_fairness_ML.html?utm_source=chatgpt.com)).

5. Equity Implications  
   Giving a non-protected group an incremental advantage deepens existing divides. Local residents may enjoy lower interest rates or higher approval odds, while outsiders—potentially from marginalized communities—face tougher barriers. This not only erodes equity and trust in automated decisions but also risks amplifying socioeconomic disparities. As a result, process designers must critically assess even seemingly neutral branches to ensure they do not embed unfair advantages.