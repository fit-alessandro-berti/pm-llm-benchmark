Below is a structured approach that links what you can extract from your MES logs, the inefficiencies it will reveal, and how to turn those insights into more sophisticated scheduling strategies, with a validation and feedback loop.

---

### 1. Analyse Historical Scheduling Performance and Dynamics with Process Mining

**Reconstruct actual flows and sequences**

- **Process discovery** on the case perspective (Job ID) to visualise typical routings and variants. Given the variability, use algorithms that can cope with unstructured behaviour (e.g. Fuzzy Miner, Inductive Miner – infrequent) to derive a process map overlaid with performance.
- In the **resource perspective**, reconstruct permachine execution sequences by grouping events by `Resource` and sorting by `Timestamp`. This yields the exact order in which jobs were processed on each machine, including setups and idle gaps.

**Key techniques and metrics**

- **Flow times/lead times/makespan**: For each job, compute:
  - Flow time: `completion_time - release_time` (from “Job Released” to last “Task End”).
  - Lead time: from order acceptance to shipment if available.
  - Makespan: for batches/days, overall time to complete a set of jobs.
  - Use performance overlays to get distributions, medians, tails.
- **Task waiting/queue times**: Time from `Queue Entry` to `Setup Start`/`Task Start` at each work centre. Aggregate by machine and by priority class.
- **Utilization**:
  - Productive time: sum of `Task Duration (Actual)` per resource.
  - Setup time: sum of `Setup` events.
  - Idle time: gaps between `Task End` and next `Setup Start`/`Task Start` on the same resource.
  - Compute utilisation = productive/(productive+setup+idle) over calendar time, and the breakdown of time categories.
- **Sequencedependent setups**:
  - For each machine, build a transition matrix where each cell (prev_job_class  next_job_class) has average and variance of setup durations. Job class can be derived from part family, material, dimension, etc.
  - Extract setup durations as `Setup End - Setup Start`, keyed by the preceding job on that machine (the log already contains “Previous job: …”).
  - Analyse which sequences incur high setups; visualise with a heatmap.
- **Schedule adherence/tardiness**:
  - Tardiness per job: `max(0, completion_time - due_date)`, earliness similarly.
  - Ontime delivery rate, average/maximum tardiness, percentage of jobs with negative slack at release.
  - Adherence to planned durations: compare `Task Duration (Actual)` vs `Planned` to quantify bias/variability.
- **Impact of disruptions**:
  - Annotate timelines with breakdowns and priority changes. Use “whatif” filters to compare performance during normal periods vs periods affected by `Breakdown Start`/`Hot job` events.
  - Compute delays attributable to disruptions: increase in waiting times immediately after breakdowns, rescheduling frequency, number of preemptions.

Tools such as bottleneck analysis plugins, performance spectrum charts, dotted charts, and conformance checking (comparing asis vs designed/expected flows) can support these analyses.

---

### 2. Diagnose Scheduling Pathologies with Evidence

Based on the above metrics, you can identify and substantiate specific issues:

- **Bottleneck resources**: Machines with consistently high utilisation, long queues, and high waiting times. Quantify their contribution to flow time (e.g. % of total lead time spent waiting for/processing at MILL03).
- **Poor prioritisation**: Instances where highpriority or imminent duedate jobs (attributes: `Order Priority`, `Order Due Date`) waited behind lowerpriority jobs. Use variant analysis to compare paths/times of ontime vs late jobs and highlight where delays accrue.
- **Suboptimal sequencing/setup inflation**: Sequences on a machine that cause long setups (from the transition matrix). E.g. frequent alternation between dissimilar jobs on CUT01 causing setup overhead >X% of productive time.
- **Starvation/blocking**:
  - Starvation: periods where a downstream machine is idle while upstream WIP is low/high. Detect via idle gaps on downstream resources and simultaneous high queues upstream.
  - Blocking: upstream resources waiting because downstream queues are full (if buffer limits are known), or “Task End” to “Queue Entry” delays.
- **WIP bullwhip**: Oscillations in queue lengths across stations; sudden surges following disruptions or priority changes indicating uncoordinated release/scheduling.
- **Resource contention**: Periods where multiple urgent jobs queue simultaneously for the same bottleneck machine.

Process mining evidence:

- **Bottleneck analysis**: Colourcoded process maps showing where cases accumulate time.
- **Variant/path analysis**: Compare late vs ontime cases to see if certain routings or machine combinations correlate with lateness.
- **Resource contention graphs**: Overlaps of tasks on the same resource, highlighting peak contention periods.
- **Setup sequence analysis**: Heatmaps showing high setup times for specific job type transitions.

---

### 3. Root Causes of Scheduling Ineffectiveness

Likely root causes, and how mining helps distinguish them:

- **Static dispatching in a dynamic environment**: FCFS/EDD at local stations ignores global priorities, remaining slack, and setup implications. Evidence: highpriority jobs delayed despite capacity; excessive setups due to alternating sequences.
- **Lack of realtime visibility**: Decisions made without awareness of current queues, machine status. Evidence: starvation despite upstream WIP; jobs sent to down machines (breakdowns not accounted for).
- **Inaccurate duration/setup estimates**: Planned vs actual analysis shows bias/variance. If planned times are consistently lower than actuals, schedules are unrealistic.
- **Ignoring sequencedependent setups**: No attempt to group similar jobs. Evidence: setup time fraction high; transition matrix shows avoidable highsetup sequences executed.
- **Poor intercentre coordination**: Upstream releases jobs without considering downstream load, causing WIP piles. Evidence: downstream queues spike after upstream bursts.
- **Reactive disruption handling**: No contingency for breakdowns/hot jobs. Evidence: large performance degradation after disruptions; no rescheduling to bypass down machines.

To differentiate scheduling logic issues vs capacity constraints/inherent variability:

- If a resource’s utilisation is high (>85–90%) and queues are long, the constraint may be capacity. If utilisation is moderate but tardiness is high and there are many idle gaps interspersed with long waits, scheduling/coordination is the culprit.
- Analyse coefficient of variation of processing times. High variability imposes a stochastic penalty; if variability is low but performance is poor, scheduling is more to blame.
- Conformance checking can show deviations from intended routes causing rework or loops (process variability) vs delays on the intended path (scheduling/dispatching).

---

### 4. Develop Advanced DataDriven Scheduling Strategies

Use mined data to move beyond static rules. Three distinct strategies:

#### Strategy 1: Dynamic, MultiCriteria Dispatching (Enhanced ATCS)

**Logic:** Implement a dispatching index at each decision point that accounts for due date pressure, remaining work, priority, and sequencedependent setup.

For a job \(j\) at machine \(m\), compute a priority \(P_j\) such as:

\(P_j = \frac{w_j}{p_j} \cdot e^{-\frac{\max(0,\,d_j - t - r_j)}{k_1}} \cdot e^{-\frac{s_{prev,j}}{k_2}} \cdot e^{-\frac{L^{downstream}_j}{k_3}}\),

where:
- \(w_j\) = weight based on `Order Priority`/due date class,
- \(p_j\) = estimated processing time at \(m\) (from mined distributions),
- \(d_j\) = due date, \(t\) = current time,
- \(r_j\) = estimated remaining total processing (sum of mined means for downstream tasks),
- \(s_{prev,j}\) = estimated setup time given the current machine’s last job (from the transition matrix),
- \(L^{downstream}_j\) = proxy for downstream load/queue to avoid starving bottlenecks,
- \(k_1,k_2,k_3\) = tuning parameters.

**Data use:** Setup estimates from the sequence matrix; processing times from mined actuals; remaining work via discovered routing; downstream load from current queues (requires realtime data).

**Addresses:** Poor prioritisation (incorporates slack/priority), excessive setups (penalises high setups), starvation (accounts for downstream load).

**Impact:** Expected to reduce tardiness and WIP, especially for highpriority jobs; modest increase in utilisation due to fewer setups.

#### Strategy 2: Predictive Scheduling with Robustness to Disruptions

**Logic:** Generate a forwardlooking schedule using predicted task durations and machine availability, with periodic reoptimisation.

- Train predictive models (regression/ML) on the log to estimate processing and setup times as functions of job features (part family, material, dimensions), machine, operator, time of day, and preceding job (for setup).
- If sufficient data, build predictive maintenance models to forecast breakdown likelihood windows for critical machines. Otherwise, use empirical breakdown frequency/MTBF/MTTR from logs to model risk.
- Use these predictions in a lookahead scheduling algorithm (e.g. rolling horizon heuristic or mixedinteger program for bottlenecks) to allocate jobs, reserving slack around highrisk intervals.

**Data use:** Task time distributions and features mined from the log; breakdown events to parameterise availability models.

**Addresses:** Inaccurate duration estimates (uses datadriven predictions), poor disruption handling (plans around predicted downtimes), global coordination (optimises across routings).

**Impact:** More realistic promises, reduced schedule infeasibility, lower tardiness variance, smoother utilisation by anticipating bottlenecks.

#### Strategy 3: Setup Time Optimisation via Sequencing/Batching

**Logic:** Specifically minimise sequencedependent setups on bottleneck machines by intelligent sequencing or microbatching.

- Cluster jobs into “families” with low mutual setup times (from the setup transition matrix).
- For critical machines, solve a singlemachine scheduling problem with sequencedependent setups to minimise total setup + weighted tardiness. Use heuristics (e.g. NEH variant, genetic algorithms, or ant colony optimisation) seeded with the current queue.
- Introduce microbatches of similar jobs where possible to reduce changeovers, subject to due date constraints (do not let batching induce excessive lateness).

**Data use:** Setup transition matrix; due dates/priorities; processing time estimates; identification of bottlenecks from mining.

**Addresses:** Excessive setup overhead (reduces setups), poor sequencing (systematically orders jobs), bottleneck throughput (increases effective capacity).

**Impact:** Increased productive time on bottlenecks, reduced WIP upstream (due to faster throughput), improved ontime performance if duedate constraints are respected in batching logic.

Additional strategies could include **Controlled order release** based on WIP caps (CONWIP) informed by queue length analysis, but the above three cover dispatching, prediction, and setup optimisation.

---

### 5. Simulate, Evaluate, and Implement Continuous Improvement

**Discreteevent simulation (DES) for validation**

Build a DES model of the shop floor parameterised with mined data:

- **Process routing:** From discovered models, with probabilities for alternative paths/ rework.
- **Processing/setup times:** Distributions conditioned on job/machine/features.
- **Arrival process:** Order release patterns, priorities, due dates from historical arrivals.
- **Sequencedependent setups:** Use the transition matrix per machine.
- **Breakdowns:** MTBF/MTTR from log; optionally timeofday effects.
- **Buffers:** Queue capacities, transfer times.

Test scenarios:

- Baseline load vs peak load periods.
- Increased incidence of breakdowns.
- Influx of hot jobs with short due dates.
- Changes in mix (more highsetup jobs).

Run the baseline (current rules) and each proposed strategy; compare KPIs: average/95th percentile tardiness, ontime delivery rate, average WIP and cycle time, utilisation breakdown, number/length of setups.

**Continuous monitoring and adaptation**

- Implement the chosen strategy with **realtime data feeds** from MES. Use online process mining/conformance to monitor deviations from expected flows.
- Track KPIs in control charts; set thresholds for alerts (e.g. tardiness >X, queue length at bottleneck >Y).
- Detect **concept drift**: changes in processing time distributions, setup patterns, breakdown rates. Retrain predictive models and update dispatching parameters accordingly.
- Periodically rerun mining analyses (e.g. monthly) to identify new pathologies or emerging bottlenecks as the mix changes.
- Use A/B testing in simulation or on noncritical work centres before rolling out tweaks globally.

By grounding scheduling logic in the empirical behaviour captured in your event logs, and by validating changes in a simulated environment before deployment, you can transition from reactive, local rules to adaptive, globally informed scheduling that directly addresses tardiness, WIP, and utilisation issues.