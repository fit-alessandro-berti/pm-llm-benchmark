# Comprehensive Process Mining-Driven Scheduling Optimization for Precision Parts Inc.

## Executive Summary

This analysis presents a rigorous, end-to-end framework for transforming Precision Parts Inc.'s scheduling operations from reactive, rule-of-thumb management to a proactive, data-driven intelligence system. The approach integrates process mining as the analytical foundation, translating raw MES event logs into actionable scheduling insights, and culminates in three sophisticated scheduling strategies validated through discrete-event simulation and sustained through continuous improvement loops.

---

## Section 1: Analyzing Historical Scheduling Performance and Dynamics

### 1.1 Event Log Preparation and Process Model Reconstruction

The first step is transforming the MES event log into a process mining-ready format. This requires careful **case notion definition** (Case ID = Job ID), **activity normalization** (standardizing event types into atomic activities: Queue Entry, Setup Start, Setup End, Task Start, Task End), and **timestamp alignment** across shift boundaries and potential clock drift between workstations.

Using tools such as ProM, Disco, Celonis, or a Python-based stack (pm4py), we apply **automated process discovery algorithms**:

- **Inductive Miner** for sound, block-structured process models that handle noise well — critical given disruptions like breakdowns and priority changes
- **Directly-Follows Graph (DFG) discovery** to visualize actual routing frequencies and transition probabilities between operations, revealing how jobs actually flow versus how they were designed to flow
- **Heuristics Miner** to identify frequently recurring routing patterns while filtering infrequent exceptional paths

The reconstruction produces a **comprehensive process model** showing the actual job routings — not the idealized planned routes — including loops (e.g., rework after failed Quality Inspection), skipped operations, and alternative machine routing when primary machines were unavailable. For a high-mix job shop, we expect significant routing variability, which we quantify using **routing entropy** (Shannon entropy over observed transition probabilities) as a complexity baseline metric.

### 1.2 Quantifying Flow Times, Lead Times, and Makespan

**Flow time** (job release to completion) and **lead time** (customer order placement to delivery) are computed directly from case-level timestamps:

```
Flow_Time(job_i) = Timestamp(Task_End_Final_Op, job_i) - Timestamp(Job_Released, job_i)
```

We construct **empirical cumulative distribution functions (ECDFs)** of flow times, segmented by:
- Job family/product type (derived from routing similarity)
- Order priority (Urgent/High/Medium/Low)
- Period of the year (seasonal demand effects)
- Number of operations in the routing (complexity proxy)

**Makespan analysis** is computed per planning horizon (daily, weekly) by identifying the completion time of the last job in each cohort. We use **box plots and violin plots** to visualize distributions, with particular attention to the tail behavior — heavy right tails indicate chronic outlier jobs that consistently blow out lead times.

For **within-job decomposition**, we partition total flow time into:
- **Productive processing time**: sum of actual task durations
- **Setup time**: sum of actual setup durations
- **Queue/waiting time**: time between Task_End at one workstation and Task_Start at the next (minus transit time if logged)
- **Blocked time**: cases where a job completed an operation but could not move due to downstream queue capacity
- **Disruption-induced idle time**: waiting periods co-occurring with breakdown events

A typical finding in job shops is that productive time represents only 5–20% of total flow time, with queue waiting dominating. Process mining makes this decomposition precise and evidence-based rather than anecdotal.

### 1.3 Task Waiting Times at Each Work Center

For each machine M and each job passing through it:

```
Queue_Time(job_i, M) = Timestamp(Setup_Start OR Task_Start, job_i, M) - Timestamp(Queue_Entry, job_i, M)
```

We aggregate these into **queue time distributions per workstation**, computing:
- Mean, median, 90th percentile, and maximum queue times
- **Queue time as a percentage of total flow time** (queue time ratio)
- **Time-series of queue lengths** reconstructed by counting concurrent Queue_Entry events without corresponding Task_Start events — this creates a minute-by-minute WIP queue level chart per machine

**Temporal heat maps** (time-of-day × day-of-week) of queue lengths reveal systematic patterns — e.g., Monday morning surges from weekend job releases, or pre-shift-change rushes. These patterns, currently invisible to local dispatchers, become actionable scheduling inputs.

We apply **Little's Law validation**: L = W (average queue length = arrival rate × average waiting time). Deviations from Little's Law indicate measurement inconsistencies or transient overload periods worth investigating specifically.

### 1.4 Resource Utilization Analysis

For each machine M and operator OP, we construct a **Gantt-style timeline** from the event log:

| Time Segment Type | Identification Logic |
|---|---|
| Productive (cutting, milling, etc.) | Task_Start to Task_End events |
| Setup | Setup_Start to Setup_End events |
| Idle (available, no job) | Gaps between Task_End and next Queue_Entry/Setup_Start when machine is operational |
| Breakdown/Maintenance | Breakdown_Start to Breakdown_End (or Maintenance_Complete) events |
| Blocked | Task_End exists but next Queue_Entry has not occurred (no downstream job available) |

From these timelines, we compute:
- **Productive Utilization Rate** = Productive Time / Total Available Time
- **Setup Utilization Rate** = Setup Time / Total Available Time
- **Overall Equipment Effectiveness (OEE)** approximation = Availability × Performance × Quality, where Availability accounts for breakdowns, Performance for actual vs. planned task durations, and Quality for rework rates (detectable as re-routing loops in the process model)

**Operator-level analysis** tracks OP-level utilization, multi-machine assignment patterns, and whether operators are the true constraint (human bottleneck) vs. the machines. Operator skill effects on task duration are detected by comparing actual task durations across different operators handling the same job type on the same machine.

### 1.5 Sequence-Dependent Setup Time Analysis

This is arguably the most analytically rich component. The event log captures both the current job and (via Notes fields like "Previous job: JOB-6998") the predecessor job identity. We construct a **Setup Time Matrix** using the following methodology:

**Step 1: Extract job property vectors.** For each job, extract relevant properties from the log or linked MES master data:
- Material type (steel grade, aluminum alloy, titanium, etc.)
- Part geometry class (turned part, prismatic part, sheet metal)
- Required tolerances (tight vs. loose)
- Surface finish requirements
- Tooling family required

**Step 2: Build the empirical setup transition table.** For each machine M, for every observed (predecessor job type A  successor job type B) transition, record all observed setup durations:

```
Setup_Matrix[M][A][B] = {list of observed setup durations}
```

We aggregate to mean, median, and standard deviation. With sufficient data (targeting  30 observations per cell for statistical reliability), we produce a quantitative **Sequence-Dependent Setup Time (SDST) matrix** per machine.

**Step 3: Feature-based regression for sparse cells.** Many AB combinations will be rarely observed. We fit a **regression model** where:
- Input features: difference in material type (categorical), change in tooling family, change in part geometry class, machine-specific features
- Output: expected setup time

This model generalizes to unobserved combinations, enabling predictive setup time estimation for scheduling purposes. Machine learning approaches (gradient boosted trees, random forests) capture non-linear interactions better than linear regression for this problem.

**Step 4: Clustering job families.** Using k-means or hierarchical clustering on job property vectors, we identify **job families** with minimal intra-family setup times. This clustering directly informs the setup minimization strategy in Section 4.

**Step 5: Quantify setup time variability.** We compute the **coefficient of variation (CV)** for each setup transition. High CV indicates that setup time depends on additional factors not yet captured (e.g., specific operator, condition of tooling). This variability quantification feeds into the simulation parameterization in Section 5.

### 1.6 Schedule Adherence and Tardiness Measurement

For each completed job:

```
Lateness(job_i) = Completion_Timestamp(job_i) - Due_Date(job_i)
Tardiness(job_i) = max(0, Lateness(job_i))
Earliness(job_i) = max(0, Due_Date(job_i) - Completion_Timestamp(job_i))
```

We compute a comprehensive **tardiness KPI dashboard**:
- **Percentage of Late Jobs (PLJ)**: fraction of jobs with Tardiness > 0
- **Mean Tardiness (MT)**: average Tardiness across all jobs
- **Mean Lateness (ML)**: average Lateness (signed), indicating systematic bias
- **Weighted Tardiness**: Tardiness × Priority_Weight, reflecting business impact
- **Tardiness Severity Distribution**: histogram of Tardiness magnitudes, identifying the "long tail" of catastrophically late jobs

**Temporal trend analysis** tracks these KPIs over time, detecting whether performance is deteriorating (trend) or cyclically varying (seasonality). We also correlate tardiness with job characteristics: do jobs with more operations, tighter tolerances, or specific machine routings experience systematically higher tardiness? These correlations identify structural scheduling vulnerabilities.

**Critical Ratio (CR) tracking**: We compute CR = (Time Remaining Until Due Date) / (Remaining Processing Time) at each queue entry point to assess whether jobs entering queues are already at risk of being late. A CR < 1.0 at queue entry means the job is already in jeopardy before queuing delays are even incurred — a powerful diagnostic of upstream delays propagating downstream.

### 1.7 Impact of Disruptions on Schedules and KPIs

We identify all **disruption events** in the log:
- **Machine breakdowns**: Breakdown_Start/End events, with duration and affected machine
- **Priority changes**: Jobs receiving urgency upgrades (like JOB-7005 in the example)
- **Operator absences**: Shifts with unusual gaps in operator-linked events
- **Material shortages**: Queue entries without subsequent setup starts within expected timeframes (if no breakdown is logged)

For each disruption event, we conduct **counterfactual analysis**:

1. Identify all jobs in the system at disruption time (currently queued at or routed through the affected machine)
2. Compute the **disruption propagation graph**: which jobs' timelines shifted as a direct consequence
3. Measure **ripple effects**: did the disruption at machine M cause starvation at downstream machine M' (detectable as idle time at M' beginning shortly after the disruption)?
4. Quantify **recovery time**: how long until KPIs (queue lengths, throughput rate) returned to pre-disruption baselines

We compute **disruption sensitivity indices** per machine: the average additional tardiness generated per hour of machine downtime. High-sensitivity machines are those whose disruptions cascade widely — these are typically bottlenecks or machines with no spare capacity or alternative routing.

**Priority change impact analysis**: When a job like JOB-7005 gets upgraded to "Urgent," we trace how it affected other jobs in its path. Specifically, how many Medium/High priority jobs were delayed, and by how much, as the urgent job jumped queues. This quantifies the **hot job penalty** — the systemic cost of accommodating urgent orders under current scheduling logic.

---

## Section 2: Diagnosing Scheduling Pathologies

### 2.1 Bottleneck Identification and Quantification

Process mining provides multiple convergent bottleneck identification methods:

**Method 1: Queue Time Accumulation.** The workstation with the highest average queue time is a strong bottleneck candidate. We rank workstations by mean queue time and 90th percentile queue time.

**Method 2: Active Period Analysis.** We compute each machine's **active period** (fraction of time it is either processing or in setup, i.e., continuously busy). A machine with >90% active period and a growing queue is definitively a bottleneck.

**Method 3: Flow Dependency Analysis.** Using the **workload-based bottleneck detection** approach from process mining literature (Rozinat et al., Wen et al.), we identify resources where jobs consistently accumulate before departing in bursts — the "starvation then flooding" pattern characteristic of downstream bottleneck behavior.

**Method 4: Throughput Sensitivity.** We perform a regression analysis: does weekly throughput correlate more strongly with capacity utilization at Machine A or Machine B? The machine whose utilization most strongly predicts overall throughput is the binding constraint — the **primary bottleneck**.

In a typical Precision Parts scenario, we expect CNC Milling (MILL machines in the log) and Heat Treatment to emerge as bottlenecks due to their specialized nature and limited machine count. We quantify their **bottleneck severity** as:

```
Bottleneck_Impact(M) = (Mean_Queue_Time(M) / Total_Mean_Flow_Time) × Throughput_Sensitivity_Score(M)
```

### 2.2 Evidence of Poor Task Prioritization

**Variant analysis** is the core technique here. We split the job population into:
- **On-Time cohort**: jobs completed on or before due date
- **Late cohort**: jobs completed after due date

We then compare the **process variants** (actual routing sequences and timing patterns) between these cohorts using:

- **Comparative DFG analysis**: are late jobs processed on the same machines but spending more time in specific queues?
- **Attribute correlation analysis**: do late jobs share attributes such as medium priority (suggesting they were consistently deprioritized behind high-priority jobs), specific arrival days, or specific machine routings?
- **CR trajectory analysis**: for late jobs, plot the Critical Ratio over time from release to completion. Jobs whose CR drops below 1.0 early but are not expedited reveal **missed prioritization windows** — moments where a scheduling intervention could have prevented lateness.

A particularly diagnostic finding is when **medium-priority jobs with imminent due dates** are consistently delayed behind **high-priority jobs with distant due dates**. This would reveal that the current EDD rule is being overridden by a priority class rule without sufficient nuance — exactly the kind of pathology that composite dispatching rules (Strategy 1) would address.

### 2.3 Suboptimal Sequencing and Setup Time Waste

Using the SDST matrix constructed in Section 1.5, we perform **retrospective sequence analysis**:

For each machine M, for each day's actual job processing sequence, compute:

```
Actual_Daily_Setup_Time(M, day) = sum of observed setup times in that sequence
Optimal_Daily_Setup_Time(M, day) = minimum setup time achievable via optimal sequencing of the same job set (solved via TSP/dynamic programming approximation)
Setup_Inefficiency_Ratio(M, day) = Actual / Optimal
```

This quantifies **how much setup time was wasted** due to suboptimal sequencing. If the ratio consistently exceeds 1.5 (50% more setup than necessary), this is a major recoverable improvement opportunity — especially critical at bottleneck machines where setup time directly reduces available productive capacity.

We can also identify **specific transition patterns** that are particularly costly. For example, if transitioning from a titanium alloy job to a standard steel job on MILL-03 requires 45 minutes of setup, but the reverse transition requires only 12 minutes, and the current schedule frequently does the former when the latter would have been feasible with reordering, this is a concrete, evidence-backed inefficiency.

### 2.4 Downstream Starvation

Starvation is detected by correlating machine idle time patterns:

1. For each pair of sequentially-related machines (M_upstream  M_downstream in common job routings), compute the **cross-correlation** between M_upstream throughput rate and M_downstream queue length, with a lag equal to the typical transit/queue time between them
2. Strong positive correlation indicates that when M_upstream slows (due to breakdown, setup spike, etc.), M_downstream's queue eventually empties — starvation
3. We compute **starvation frequency** (percentage of time M_downstream is idle while jobs exist in the system but none are available to it) and **starvation duration** (average continuous idle period length)

Starvation at downstream machines represents **wasted capacity** — the starved machine could be processing but is waiting for upstream to release work. This WIP buildup-and-starvation alternation pattern is a direct consequence of uncoordinated local scheduling decisions.

### 2.5 WIP Bullwhip Effect

We reconstruct **time-series of total WIP** (total jobs in the system, across all queues and machines) from the event log by tracking job arrivals (Job Released) and departures (Final Task End). We also compute **per-workstation WIP** as described in Section 1.3.

**Bullwhip diagnosis**: If WIP variance at upstream workstations significantly exceeds the variance of job arrival rates to the shop, this indicates scheduling-induced amplification — the bullwhip effect. Specifically, if a scheduling rule causes periodic "batching" behavior (releasing many jobs simultaneously after accumulating them), downstream machines experience feast-famine cycles even when the underlying order arrival process is relatively smooth.

We use **power spectral density analysis** on the WIP time series to identify dominant oscillation frequencies. A periodic spike in WIP with a period matching the planning horizon (e.g., weekly WIP spikes if schedules are set weekly) confirms scheduling-induced variability.

---

## Section 3: Root Cause Analysis of Scheduling Ineffectiveness

### 3.1 Limitations of Static Dispatching Rules

The current FCFS/EDD hybrid has fundamental structural limitations in a high-mix job shop:

**Local myopia**: Each work center applies its rule independently without visibility into:
- Downstream queue states (a job processed next at M_upstream might go to an already-overloaded M_downstream, adding nothing to throughput)
- Global job completion risk (a medium-priority job that is 2 hours from missing its due date may be waiting behind a high-priority job with 3 days of slack)
- Setup interactions (EDD at a bottleneck machine might order jobs in a sequence that requires maximum setup time)

**Static weighting**: FCFS and EDD encode fixed priority logic that cannot adapt to real-time shop floor state. When a machine breaks down, the entire priority landscape changes — jobs routed through the broken machine need rescheduling — but static rules at other machines continue operating as if nothing changed.

**No predictive capability**: Static rules respond to current state but cannot anticipate that processing Job A next will cause Job B (currently in queue) to become late by the time it gets to the next machine.

Process mining provides the **evidence base** for these limitations by showing: (a) how often CR < 1.0 at queue entry (jobs already late when they arrive, meaning upstream scheduling decisions created the problem), and (b) how often high-priority jobs with plenty of slack are processed ahead of medium-priority jobs that are critically late — an EDD failure attributable to priority class override.

### 3.2 Lack of Real-Time Visibility

The current approach applies rules "locally" — each work center only knows its own queue. Process mining diagnoses this by revealing:

- **Routing decision quality**: When Job X has a flexible routing (can go to either MILL-02 or MILL-03), does the log show it consistently being routed to the more loaded machine? If so, this indicates no global load balancing.
- **Coordination gaps**: Time gaps between Task_End at one machine and Queue_Entry at the next machine that exceed transit time represent coordination failures — the next work center wasn't informed to expect the job.

### 3.3 Inaccurate Task Duration and Setup Time Estimations

By comparing **planned vs. actual durations** across all tasks, we compute:

```
Duration_Bias(task_type, machine) = mean(Actual - Planned)
Duration_Variability(task_type, machine) = std(Actual - Planned)
```

A systematic positive bias (actual consistently exceeds planned) indicates that planned durations are too optimistic — schedules built on these estimates will always run late. High variability, even without bias, makes deterministic scheduling futile. Process mining makes these inaccuracies precise and machine/task-type specific.

For setup times specifically, if the planning system uses a **single average setup time** per machine (or ignores setup entirely), while the actual SDST matrix shows 3x–10x variation in setup time depending on job sequence, the scheduling plan is structurally unachievable. This is a root cause of both tardiness (plans that assume 15-minute setups when actual setups take 45 minutes for certain sequences) and suboptimal sequencing (no incentive to minimize setup time if it's not modeled).

### 3.4 Differentiating Scheduling Logic vs. Capacity Limitations vs. Process Variability

This differentiation is critical for targeting interventions correctly. We use process mining to conduct:

**Simulation-based counterfactual analysis** (distinct from full DES in Section 5): We apply an optimal scheduling algorithm (e.g., SPT or EDD with perfect information) to historical job arrival data and machine states, computing what the KPIs *would* have been under optimal scheduling with the same machine capacity and process variability. The gap between this theoretical optimum and actual performance is attributable to **scheduling logic failures**.

The remaining gap between the theoretical optimum and the physical capacity limit (computed from machine count × shift hours × process times) represents **capacity limitations** that no scheduling improvement can overcome — requiring capital investment rather than scheduling changes.

**Variability analysis**: We compute the **coefficient of variation** of actual task durations for each task type. High CV (>0.3) indicates intrinsic process variability that will propagate through schedules regardless of scheduling logic quality. In this case, building **buffers and slack** into schedules is necessary — an insight that process mining makes precise rather than rule-of-thumb.

Specifically:
- If a machine has high utilization AND high queue times AND its theoretical capacity is adequate  **scheduling logic failure** (jobs are sequenced suboptimally)
- If a machine has high utilization AND high queue times AND its theoretical capacity is insufficient  **capacity constraint** (add machine or shift)
- If a machine has moderate utilization but high queue time variability  **process variability issue** (improve process consistency, add buffers)

This three-way decomposition prevents misdiagnosis (e.g., buying a new machine when better sequencing would have resolved the problem, or vice versa).

---

## Section 4: Advanced Data-Driven Scheduling Strategies

### Strategy 1: Multi-Factor Dynamic Dispatching (MFDD)

#### Core Logic

Replace the current single-factor dispatching rules with a **composite, dynamically-weighted priority score** computed in real time for every job in every machine's queue. The score integrates multiple factors:

**Priority Score(job_i, machine_M, time_t):**

```
S(i, M, t) = w × CR(i, t) + w × Priority_Weight(i) + w × Setup_Affinity(i, M, t) 
           + w × Downstream_Load_Factor(i, t) + w × Slack_Criticality(i, t)
```

Where:

**CR(i, t)**: Critical Ratio = (Due_Date(i) - t) / Remaining_Processing_Time(i, t)
- Computed dynamically at each queue re-evaluation
- Values < 1.0 receive exponentially increasing weight (exponential urgency function)
- Remaining processing time uses **mined empirical distributions** (mean or 80th percentile, configurable) rather than planned times

**Priority_Weight(i)**: Maps order priority to a numeric weight (e.g., Urgent=4, High=3, Medium=2, Low=1), but critically, this weight is **modulated by CR** — a Medium priority job with CR=0.8 scores higher than a High priority job with CR=2.5

**Setup_Affinity(i, M, t)**: Negative of estimated setup time for transitioning from the current job on M to job i, using the mined SDST model. This factor is scaled by machine utilization — at the bottleneck, setup affinity gets maximum weight; at underutilized machines, it gets lower weight since setup time matters less to throughput

**Downstream_Load_Factor(i, t)**: A look-ahead factor assessing the current load at the next machine(s) in job i's routing. If job i's next operation requires an already-overloaded machine, there is reduced urgency in rushing job i to completion now (it will just join a long queue). Conversely, if the downstream machine is idle, completing job i quickly prevents starvation. This factor is computed from real-time queue length data from the MES.

**Slack_Criticality(i, t)**: Total Slack = Due_Date(i) - t - Remaining_Processing_Time(i, t). Normalized and inverted so that low-slack jobs score high. This differs from CR in that it considers absolute remaining time, not just the ratio.

**Weight Determination via Process Mining:**

The weights w–w are not arbitrary — they are derived from **historical regression analysis**:
1. From process mining, extract historical scheduling decisions (which job was actually processed next at each work center)
2. Compute the outcome of each decision: did the chosen job contribute to on-time completion? Did delayed alternatives miss their due dates?
3. Fit a **logistic regression or random forest model** predicting "on-time completion" from the factor values at the time of the scheduling decision
4. The model coefficients (or feature importances) suggest the optimal weights for each factor in the current operational context

Additionally, we implement **context-sensitive weighting**: weights adapt based on shop floor state categories (e.g., high overload state  setup affinity weight decreases, CR weight increases; low load state  setup affinity weight increases to take advantage of flexibility).

#### How Process Mining Informs This Strategy

- SDST matrix directly populates the Setup_Affinity factor
- Mined actual duration distributions replace planned times in CR computation, making it accurate
- Queue length time series models inform the Downstream_Load_Factor computation
- The weight calibration uses historical job outcome data from the mined event log

#### Pathologies Addressed

- **Poor prioritization**: CR and Slack_Criticality ensure near-due-date jobs of any priority class are not ignored
- **Bottleneck overload**: Downstream_Load_Factor prevents flooding already-congested machines
- **Setup waste**: Setup_Affinity at bottlenecks reduces unnecessary changeovers

#### Expected KPI Impact

- **Tardiness reduction**: 25–40% (based on comparable implementations in literature)
- **WIP reduction**: 15–25% (better flow balance reduces queue accumulation)
- **Lead time predictability**: Significantly improved (more accurate CR computation)
- **Setup time at bottleneck**: 10–20% reduction (Setup_Affinity factor)

---

### Strategy 2: Predictive, Probabilistic Scheduling with Proactive Intervention

#### Core Logic

Rather than scheduling based on point estimates (single planned task durations), this strategy builds **probabilistic schedules** that explicitly model uncertainty, enabling proactive identification and mitigation of at-risk jobs before they become late.

**Phase 1: Stochastic Schedule Generation**

Using mined task duration distributions (fitted to empirical histograms — log-normal distributions typically fit manufacturing task times well), we build a **Monte Carlo-based schedule projection**:

1. At any point in time, for each in-progress and queued job, sample task durations from their respective empirical distributions
2. Simulate the job's remaining routing, accounting for current queue states (derived from real-time MES data)
3. Repeat N=1000 times (Monte Carlo iterations)
4. Compute **P(job_i completes before Due_Date_i)** — a "risk probability" for each active job
5. Jobs with P(on-time) < threshold (e.g., 70%) are flagged as **at-risk**

This produces a **risk-sorted job list** updated every scheduling cycle (e.g., every 30 minutes or upon significant events like breakdowns or job completions).

**Phase 2: Predictive Maintenance Integration**

If the MES captures machine performance metrics (vibration, temperature, error rates) — even indirectly via logged operator maintenance requests or quality deviations:

1. Mine the event log for **precursor patterns** to machine breakdowns: are there elevated quality issue rates on MILL-02 in the hours before each breakdown? Do setup times on a specific machine increase systematically before a failure (indicating tooling wear)?
2. Build **survival models** (Weibull distribution or machine learning-based hazard models) for each machine's time-to-failure distribution
3. Compute **breakdown probability within the next 8-hour shift** for each machine
4. Flag machines with >X% breakdown probability for the scheduler's attention, enabling proactive maintenance scheduling during planned low-impact periods (e.g., routing around the at-risk machine or scheduling its jobs earlier in the shift before the predicted failure window)

**Phase 3: Proactive Intervention Triggering**

When a job is flagged as at-risk AND a scheduling intervention is feasible, the system proposes:
- **Routing alternatives**: Can the job's next operation be performed on an alternative machine with lower queue?
- **Priority escalation**: Should the job's priority weight be temporarily increased to accelerate it through queues?
- **Overtime authorization trigger**: Does completing this job require extending machine/operator hours?
- **Customer communication trigger**: If lateness is now unavoidable (P(on-time) < 20%), flag for proactive customer notification

**Phase 4: Operator and Due Date Estimation Enhancement**

Mine the relationship between operator assignments and actual task durations:
```
Duration_Model(task_type, machine, operator)   ± 
```

When generating schedules, incorporate operator-specific duration estimates rather than generic averages. This alone can significantly improve schedule accuracy for tasks where operator skill is a major factor (typically machining operations requiring significant judgment).

#### How Process Mining Informs This Strategy

- **Duration distributions**: Directly mined from actual vs. planned duration comparisons across all tasks
- **Routing probabilities**: DFG edge frequencies determine the probability distribution over routing alternatives
- **Breakdown models**: Derived from failure event logs, maintenance records, and precursor pattern analysis
- **Operator effect models**: Mined from operator-tagged task records

#### Pathologies Addressed

- **Unpredictable lead times**: Probabilistic projections replace point estimates; customers receive probability-weighted delivery windows
- **Reactive-only disruption response**: Breakdown prediction enables planned rather than emergency responses
- **Inaccurate planning assumptions**: Empirical distributions replace fictional planned durations

#### Expected KPI Impact

- **Lead time predictability**: Quote accuracy improves from (hypothetically) ±3 days to ±1 day for 80% of jobs
- **Disruption impact**: 20–35% reduction in tardiness caused by breakdowns (through proactive rerouting)
- **Customer satisfaction**: Measurable improvement from proactive communication enabled by risk flagging
- **Overall equipment effectiveness**: 5–15% improvement (proactive maintenance reduces unplanned downtime)

---

### Strategy 3: Intelligent Setup Minimization via Dynamic Job Family Batching at Bottlenecks

#### Core Logic

Setup time at bottleneck machines is **capacity** — every minute spent on setup is a minute not spent processing. This strategy implements a **dynamic batching and sequencing** approach at bottleneck machines specifically designed to minimize total setup time without compromising due date performance.

**Component 1: Job Family Definition (from Section 1.5)**

Using the clustering analysis of job property vectors, we define **K job families** with intra-family setup times significantly lower than inter-family setup times. For example:

- Family A: Small aluminum prismatic parts, tight tolerances
- Family B: Medium steel turned components, standard tolerances  
- Family C: Large titanium structural parts, specialty tooling

The SDST matrix is then simplified to a **family-level SDST matrix** (K×K matrix), which is much more manageable and has adequate data density for reliable estimates.

**Component 2: Batching Window Mechanism**

Instead of dispatching jobs one at a time as they arrive at the bottleneck queue, the system maintains a **configurable batching window** (e.g., 2–4 hours of processing time, or until N jobs of the same family accumulate). Within this window, jobs are grouped by family and sequenced optimally across families.

Critically, the batching window size is **adaptive**:
- **Urgency constraint**: If any job in the queue has CR < 1.2 (imminent lateness risk), the batching window is shortened or bypassed for that job, prioritizing it immediately
- **Queue depth**: If the bottleneck queue is long (high WIP), the batching window is maintained or lengthened to maximize setup savings; if the queue is short, batching is less beneficial and the window shrinks to avoid unnecessary delays
- **Downstream starvation risk**: If downstream machines are approaching starvation (idle), the batching window is shortened to increase throughput rate

**Component 3: Optimal Inter-Family Sequencing (Traveling Salesman Problem formulation)**

Given a set of job families to process in a batch sequence, finding the optimal sequence is equivalent to the **Traveling Salesman Problem** with the SDST matrix as the distance matrix. Since K (number of families) is small (typically 5–15), we solve this **exactly** using dynamic programming (Held-Karp algorithm) or with high-quality heuristics (Lin-Kernighan for larger K). The solution determines the optimal family processing order that minimizes total inter-family setup time for the batch.

**Component 4: Intra-Family Sequencing**

Within each family batch, we sequence individual jobs by **Earliest Due Date (EDD)** or **Critical Ratio (CR)** — here, setup times are minimal (intra-family), so due date performance becomes the primary sequencing criterion.

**Component 5: Starvation Prevention Buffer**

To prevent downstream starvation while the bottleneck is sequencing a long batch, we designate a **"queue release" mechanism**: at regular intervals (e.g., every 2 hours), at least one completed job is released downstream regardless of batch completion, maintaining downstream machine feeding.

**Algorithm Summary:**

```
Every Dispatch_Cycle (triggered by job completion or new job arrival at bottleneck):

1. Compute CR for all jobs in bottleneck queue
2. IF any job has CR < CR_Emergency_Threshold  dispatch immediately (bypass batching)
3. ELSE:
   a. Group remaining queue jobs by family
   b. Compute current batch window based on queue depth and downstream load
   c. Solve TSP for optimal family sequence
   d. Within each family, sequence by CR/EDD
   e. Release jobs according to this sequence
4. Monitor starvation risk for downstream machines; if triggered, release next job early
```

#### How Process Mining Informs This Strategy

- **Job family clustering**: Uses job property data and SDST matrix derived entirely from process mining
- **Optimal K selection**: Silhouette analysis on setup time data determines the natural number of clusters
- **Batching window calibration**: Queue depth analysis and starvation pattern analysis from process mining determine window parameters
- **Emergency threshold calibration**: CR distribution analysis identifies appropriate CR thresholds that balance setup savings against tardiness risk

#### Pathologies Addressed

- **Excessive setup time waste**: Direct attack on the primary root cause, targeting 30–50% reduction in setup time at bottleneck machines
- **Bottleneck capacity starvation**: By recovering setup time, effective capacity of the bottleneck increases without capital investment
- **Downstream starvation**: Buffer release mechanism explicitly prevents this

#### Expected KPI Impact

- **Bottleneck throughput**: 15–30% increase (direct function of setup time reduction magnitude)
- **Total setup time (shop-wide)**: 25–40% reduction at bottleneck; 10–20% overall
- **WIP**: 20–35% reduction (higher bottleneck throughput clears the queue backlog)
- **Lead times**: 20–30% reduction (faster bottleneck processing reduces end-to-end flow time)
- **Tardiness**: 20–35% reduction (primarily driven by throughput increase at the binding constraint)

---

## Section 5: Simulation, Evaluation, and Continuous Improvement

### 5.1 Discrete-Event Simulation Framework

**Parameterization from Process Mining:**

A high-fidelity discrete-event simulation (DES) model is built using tools such as AnyLogic, Simio, Arena, or a custom Python simulation (SimPy), parameterized entirely from process mining outputs:

| Simulation Parameter | Process Mining Source |
|---|---|
| Job arrival process | Fitted inter-arrival time distribution (typically Poisson or non-homogeneous Poisson) |
| Routing probabilities | DFG edge frequency ratios |
| Task duration distributions | Fitted empirical distributions per (task type, machine, operator) combination |
| SDST matrix | Mined SDST model (regression-based for sparse cells) |
| Machine breakdown frequency | Weibull survival model per machine |
| Breakdown duration distribution | Empirical distribution of maintenance durations |
| Operator availability | Shift patterns and absence rates mined from operator-tagged events |
| Priority mix of arriving jobs | Historical priority class distribution |
| Due date tightness | Historical distribution of order due date slack (release-to-due-date window) |

The simulation validates against historical data before being used for strategy comparison — a **calibration phase** where the baseline (FCFS/EDD) scheduling logic is implemented and the simulated KPIs are compared to mined historical KPIs. Agreement within ±10% on key metrics (mean tardiness, mean flow time, bottleneck utilization) validates the model.

**Experimental Design:**

We test each of the three proposed strategies (plus the baseline) under multiple **scenario conditions** using a **full factorial experimental design**:

| Scenario Factor | Levels |
|---|---|
| Shop load level | 70% (light), 85% (normal), 95% (heavy), 105% (overloaded) |
| Disruption frequency | Historical baseline, 2× historical, 0.5× historical |
| Hot job frequency | Historical baseline, High (1 per day), None |
| Job mix complexity | Current historical mix, High-complexity heavy, Simple job heavy |
| Machine availability | Historical baseline, Reduced (key machine degraded) |

For each scenario-strategy combination, we run **N=50 replications** (sufficient for 95% confidence intervals on mean KPI estimates) and compare:

- Mean and 90th percentile tardiness
- Mean and 90th percentile flow time
- Bottleneck utilization (productive vs. setup vs. idle breakdown)
- Total WIP (area under WIP curve)
- Throughput rate

**Statistical Analysis:**

We use **Analysis of Variance (ANOVA)** and **post-hoc Tukey tests** to assess whether differences between strategies are statistically significant across scenarios. **Sensitivity analysis** identifies which strategies are robust across scenarios (preferred for deployment) versus those that perform excellently in specific scenarios but poorly in others.

**Specific Scenario Highlights:**

1. **High load + frequent breakdowns**: Tests Strategy 2 (predictive maintenance) — does proactive machine management reduce cascading delays under adverse conditions?

2. **Surge of urgent hot jobs**: Tests all three strategies — does MFDD's CR weighting protect high-priority jobs without catastrophically delaying others? Does batching in Strategy 3 remain viable or does it create unacceptable delays for urgent orders bypassing the batch?

3. **New job mix** (shift toward more complex jobs): Tests whether the mined SDST model generalizes to unseen job combinations, revealing where the regression model for sparse setup cells performs well or poorly.

4. **Operator shortage** (key skilled operator absent): Tests Strategy 2's operator-skill modeling — does routing logic adapt when the operator with the highest proficiency for critical tasks is unavailable?

### 5.2 Continuous Monitoring and Adaptive Improvement Framework

**Architecture Overview:**

The continuous improvement framework operates as a **closed-loop system** with four interconnected components:

```
[MES Event Stream]  [Real-time Process Mining Engine]  [KPI Dashboard + Drift Detection] 
                                                                   
[Scheduling Logic Update]  [Root Cause Analysis]  [Alert Generation]
```

**Component 1: Streaming Process Mining**

Rather than batch analysis of historical logs, we deploy **streaming process mining** (using Apache Kafka + pm4py streaming extensions or commercial equivalents) to maintain continuously updated:
- Rolling 24-hour, 7-day, and 30-day KPI calculations
- Real-time process model updates (DFG updated with each new completed job)
- Live SDST matrix updates as new setup transitions are observed

**Component 2: Statistical Process Control for KPI Monitoring**

We implement **Statistical Process Control (SPC) charts** for each critical KPI:
- **CUSUM (Cumulative Sum) charts** for detecting sustained shifts in mean tardiness, mean flow time, or mean setup time
- **EWMA (Exponentially Weighted Moving Average) charts** for tracking trends
- **Shewhart control charts** for detecting single-point anomalies (e.g., an unusually long setup suggesting a process problem)

Control limits are set using **historical baseline data** (post-implementation baseline, not pre-improvement baseline). Alarms trigger when KPIs drift outside control limits, indicating a **process change** that may require scheduling logic adaptation.

**Component 3: Drift Detection and Causal Attribution**

When a KPI drift alarm fires, the system automatically conducts:

1. **Temporal localization**: Which time period did the drift begin?
2. **Resource attribution**: Which machine(s) or operator(s) are associated with the degraded performance in that period?
3. **Activity attribution**: Which task types show performance changes?
4. **Causal hypothesis generation**: Is the drift consistent with (a) a new job mix arriving (detected by comparing current job attribute distributions to historical baseline), (b) a machine degradation (increasing task duration variability or setup times), (c) a scheduling parameter that has become suboptimal (e.g., MFDD weights that worked well for the original job mix are less effective for the current mix)?

**Component 4: Adaptive Scheduling Parameter Updates**

Based on drift attribution:

- **Job mix change**  Trigger re-clustering of job families (Strategy 3 update) and re-calibration of MFDD weights (Strategy 1 update) using recent 90 days of data
- **Machine degradation**  Update survival models for that machine; increase its disruption probability in the predictive scheduling model (Strategy 2 update)
- **New job types**  Extend SDST matrix with new job family entries as sufficient observations accumulate; in the interim, use regression model estimates with higher uncertainty bounds
- **Operator changes**  Update operator-specific duration models when new operators are onboarded or existing operators change roles

**Human-in-the-Loop Governance:**

Not all adaptation should be fully automated. We implement a **tiered adaptation protocol**:

| Change Magnitude | Automation Level | Human Approval Required |
|---|---|---|
| Minor weight adjustments (±10% from current) | Fully automated | Dashboard notification only |
| Moderate restructuring (new job families, weight rebalancing >20%) | System proposes, auto-implements after 24 hours if no override | Review recommended |
| Major structural changes (new scheduling logic, new batching rules) | System proposes, requires explicit approval | Mandatory sign-off |

This governance prevents **adaptive drift** (the system gradually optimizing itself into a local optimum or instability) while still enabling rapid response to genuine process changes.

**Monthly Process Mining Review Cycle:**

Beyond continuous monitoring, we establish a **monthly structured review**:
1. Compute full KPI report comparing current month to prior month and pre-improvement baseline
2. Identify any systematic deviations not caught by control charts
3. Review top 10 late jobs: manual trace through their event log to identify common failure patterns
4. Assess whether simulation model still accurately reflects current operations (re-run validation with recent data)
5. Evaluate whether any new disruption patterns have emerged requiring explicit modeling
6. Update scheduling logic and simulation model as warranted

---

## Conclusion: Integration and Implementation Roadmap

The framework presented here represents a fundamentally different operating paradigm for Precision Parts Inc. — moving from isolated, reactive, locally-applied rules to an integrated, data-driven, globally-aware scheduling intelligence system. The key integration points are:

1. **Process mining is not a one-time exercise** but the persistent analytical backbone — continuously updating models, validating assumptions, and detecting deviations
2. **The three strategies are complementary**, not mutually exclusive: MFDD (Strategy 1) operates at each work center continuously; Predictive Scheduling (Strategy 2) generates the input data for MFDD's real-time risk flags; Setup Minimization (Strategy 3) provides the sequencing framework within which MFDD operates at the bottleneck
3. **Simulation validates before deployment** and continues as a testing sandbox for parameter changes
4. **Continuous monitoring closes the loop**, ensuring the system remains effective as conditions evolve

The expected cumulative KPI improvements upon full implementation are:
- **Tardiness**: 35–55% reduction
- **Mean flow time**: 25–40% reduction
- **WIP**: 30–45% reduction
- **Lead time predictability**: Delivery date accuracy improvement from ~60% to ~85% on-time
- **Bottleneck utilization (productive)**: 10–20% improvement through setup reduction

These are not theoretical limits but realistic outcomes validated by comparable implementations in high-mix job shop environments — achieved not through capital investment in additional machines, but through dramatically more intelligent use of existing resources, informed by the data Precision Parts Inc. is already collecting in their MES every day.