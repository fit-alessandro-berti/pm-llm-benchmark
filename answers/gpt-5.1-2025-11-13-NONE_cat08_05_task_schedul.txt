## 1. Analyzing Historical Scheduling Performance and Dynamics with Process Mining

### 1.1. Reconstructing the actual flow of jobs and machine sequences

**Data preparation**

From the MES log, define:

- **Case ID** = Job ID (e.g., `JOB-7001`)
- **Activities**:
  - Job-level: `Job Released`, `Job Completed`, `Priority Change`
  - Task-level: `Queue Entry (Cutting)`, `Setup Start`, `Setup End`, `Task Start (Cutting)`, `Task End (Cutting)`, etc.
  - Resource-level: `Breakdown Start`, `Breakdown End`
- **Resources** = Machines (CUT-01, MILL-03, etc.) and Operators (OP-105)
- **Timestamps** = precise event times

Create two “views”:

1. **Process view (per job)** – order events by timestamp for each job.
2. **Resource view (per machine/operator)** – order events by timestamp for each resource.

**Process mining techniques**

- **Process discovery** (job-centric):
  - Use an algorithm like **Heuristic Miner** or **Inductive Miner** on job event sequences to discover:
    - Typical routings (Cutting  Milling  Heat Treat  Grinding  QC, etc.)
    - Variant patterns (alternative routings, rework loops).
  - Output: a process model (Petri net / BPMN) with frequencies and performance statistics.

- **Resource-centric sequence reconstruction**:
  - For each machine, reconstruct the sequence of jobs:
    - Sort all events for that machine by time.
    - For each job on that machine: identify `Setup Start/End` and `Task Start/End`.
  - This gives **actual execution sequences**, enabling:
    - Sequence-dependent setup analysis
    - Resource utilization analysis
    - Detection of idle gaps, breakdown intervals

### 1.2. Metrics: flow times, lead times, makespan

For each job (case):

- **Job release time**: timestamp of `Job Released`.
- **Job completion time**: last `Task End` or explicit `Job Completed`.
- **Lead time / flow time**: completion  release.
- **Job makespan**: between first `Task Start` and last `Task End` (if you want “active” time).

Use the **Performance Analysis** features of process mining tools (e.g., ProM, Celonis, Disco):

- Plot **distribution of lead times** overall and by:
  - Product family / routing variant
  - Priority class
  - Customer segment
- Compare **planned vs actual durations**:
  - For each job, aggregate planned task times vs. actual task times.
  - Compute ratio and error statistics to assess planning accuracy.

### 1.3. Task waiting times (queue times)

For each task instance:

- **Queue time at machine M**:
  - For a given job J and machine M:
    - `Queue Entry (M)` timestamp
    - `Setup Start` or `Task Start` timestamp (if no setup)
  - Waiting time = `Task/Setup Start`  `Queue Entry`.

Per machine:

- Average / median queue time
- Distribution (percentiles) of queue times
- Split by:
  - Job priority
  - Stage (early vs late operations)
  - Routing variant

Using **bottleneck analysis** in process mining:

- Attach queue time as a performance attribute to edges between activities (e.g., previous operation  `Queue Entry (M)`  `Task Start (M)`).
- Identify machines with high average queue times and long tails (95th/99th percentile).

### 1.4. Resource utilization (productive, idle, setup)

For each machine:

1. Reconstruct time intervals:
   - Setup intervals: `Setup Start`  `Setup End`
   - Processing intervals: `Task Start`  `Task End`
   - Breakdown intervals: `Breakdown Start`  `Breakdown End`
   - Queue is not machine time; machine is idle while jobs are waiting in queue.
2. For each interval between events on that machine, classify:
   - **Busy-Processing**: inside Task Start–End.
   - **Busy-Setup**: inside Setup Start–End.
   - **Down**: inside Breakdown Start–End.
   - **Idle**: gaps not covered by processing/setup/breakdown.

Compute:

- **Utilization** = (Processing time) / (Available time).
- **Effective utilization including setup** = (Processing + Setup) / (Available).
- **Setup fraction** = Setup time / (Processing + Setup).
- **Breakdown fraction**.
- **Idle fraction**.

Do the same operator-centric if operators are critical resources.

### 1.5. Sequence-dependent setup times

For each machine:

1. Build sequences of consecutive jobs:
   - For machine M: {…  JOB-6998  JOB-7001  JOB-7005  …}
2. For each transition (previous job A, next job B):
   - Setup time = `Setup End(B)`  `Setup Start(B)` (if setup required).
   - Record tuple: (Machine M, Prev Job/Product Type A, Job/Product Type B, Setup Time, Operator, shift, etc.).

Aggregate to build a **setup matrix**:

- Define a categorical type per job that drives setup (e.g., material, thickness, part family, fixture type).
- Build a matrix for each machine:

  `SetupTime[M][Type_prev][Type_next] = distribution/statistics of setup time`.

Extract:

- Average and variability (std, 95th percentile).
- Asymmetries (AB vs BA).
- Clusters of families with low setup times between them vs large setups to/from others.

This matrix directly informs any sequence/ batching optimization.

### 1.6. Schedule adherence, tardiness, and delay patterns

For each job:

- **Due date** from the log.
- **Completion time** as above.
- **Tardiness** = max(0, Completion  DueDate).
- **Lateness** = Completion  DueDate (can be negative for early).

Metrics:

- % jobs late
- Mean/median tardiness
- Weighted tardiness (by priority, revenue, or penalty cost).
- Time-series of tardiness over months (process mining + BI dashboard).

Schedule adherence can be extended to:

- **Promised completion** if available in ERP (planned finish) vs actual completion.
- **Operation-level adherence**: planned start/end vs actual (if any planning data is in the MES).

Variant-based analysis:

- Compare **on-time vs late job variants**:
  - Do they differ in routing?
  - Do they differ in waiting times at specific machines?
  - Do late jobs experience more rework loops, priority changes, or machine changes?

### 1.7. Impact of disruptions (breakdowns, hot jobs) on KPIs

**Breakdowns:**

- For each breakdown interval on machine M:
  - Jobs in progress: tasks that were interrupted (Task Start earlier, Task End after breakdown).
  - Jobs in queue: `Queue Entry (M)` before breakdown, Task/Setup Start after breakdown.
- Compute:
  - Additional waiting time at M attributable to breakdown windows (compare to baseline average).
  - Increment in tardiness for those affected jobs vs a matched control group.
- Use **conformance checking**:
  - Expected processing pattern vs actual pattern during breakdown periods, highlighting deviations.

**Priority changes / hot jobs:**

- For jobs whose **priority changed to “High/Urgent”**:
  - Measure how quickly they are picked up at each workstation after priority change.
  - Examine whether they cause:
    - Preemption (jobs being interrupted).
    - Re-sequencing (later jobs being skipped).
  - Compare:
    - Average tardiness of urgent jobs vs normal of similar due dates.
    - Additional delay inflicted on other jobs (e.g., increase in waiting times for non-urgent jobs after hot-job arrival).

Process mining resource-view:

- Visualize resource “timeline” with overlays:
  - Colored by job priority.
  - Highlight periods following breakdowns and urgent order arrivals.
- This shows visually where the plan blows up and how long the system takes to recover.

---

## 2. Diagnosing Scheduling Pathologies Using Process Mining

### 2.1. Bottleneck identification and quantification

Use:

- **Resource performance analysis**:
  - Machines with:
    - High utilization (processing + setup > 85–90%).
    - Long average queue times.
    - Long job waiting time before processing.
- **Process bottleneck analysis** (e.g., Celonis Bottleneck Explorer):
  - Identify lifecycle segments (Queue  Setup  Process) with highest average waiting times per variant.

Quantify impact:

- Measure job lead times and tardiness for jobs that visit the bottleneck resource vs those that do not.
- For jobs that use the bottleneck:
  - % of total lead time spent waiting at this resource.
  - Contribution of this resource’s delays to overall tardiness.

Output: “MILL-03 accounts for 45% of average total waiting time; jobs visiting MILL-03 have 2.1× higher tardiness vs others.”

### 2.2. Poor task prioritization

Evidence via:

- **Priority-stratified performance**:
  - Compare queue times and tardiness for:
    - High vs Medium vs Low priority jobs at each machine.
  - Check if high-priority jobs **actually get shorter waits** as intended. If not, prioritization is ineffective.

- **Due-date proximity analysis**:
  - For jobs with imminent due dates (e.g., due in <X hours) at the time they entered the queue:
    - Measure their position in the queue and actual waiting time.
  - If jobs close to due date often sit behind jobs with more slack, current rules (FCFS/EDD limited) are failing.

Use process mining variant analysis:

- Compare **on-time high-priority jobs vs late high-priority jobs**:
  - Which machines are causing delays?
  - Are late high-priority jobs waiting behind lower-priority ones?

### 2.3. Suboptimal sequencing and excessive setup times

Using the sequence-dependent setup matrix:

- Identify sequences with very high setup times that occur frequently but are avoidable:
  - Example: frequent alternation between two part families A and C where AC and CA setups are large.
- Analyze **setup time per unit of processing**:
  - For each machine: Setup time / Processing time over windows.
  - High ratios indicate poor sequencing (too much switching).

Look for patterns:

- Variants where:

  - Jobs are sequenced randomly by FCFS, not grouped by family.
  - Compare with periods where, by chance, similar jobs were grouped and result in lower setup fraction.

Show evidence:

- Plot sequential job types on a bottleneck machine over a month, color-coded by family.
  - Frequent color flipping = high setup overhead.
- Correlate average daily setup time with tardiness and throughput.

### 2.4. Starvation and blocking (upstream/downstream mis-coordination)

Starvation:

- For each machine:
  - Identify **idle periods** when:
    - The machine is available (no breakdown, no setup, no maintenance).
    - There is WIP upstream (jobs that have completed previous operations but not yet arrived at this machine).
- Use process model and timestamps to check:
  - “Downstream machine idle while upstream machines are heavily loaded or blocked.”

Blocking / WIP bulges:

- Calculate **WIP over time**:
  - Number of jobs in “in-process” state between operations.
- By workstation:
  - Number of jobs in queues in front of each machine over time.
- Look for:
  - Periodic WIP waves between certain stations (e.g., from Cutting to Milling), indicating poor pacing or bottleneck shielding.

Use process mining **performance animations**:

- Replay logs over time to visually see WIP accumulation and starvation at different stations.

### 2.5. Bullwhip effect in WIP due to scheduling variability

Focus on variability:

- For each machine:
  - Time series of queue length (e.g., average and variance per hour/day).
- Correlate queue length fluctuations with:
  - Priority changes.
  - Last-minute hot jobs.
  - Breakdowns upstream.

Diagnosis:

- If upstream machines release jobs in bursts (e.g., because they batch excessively), downstream queues oscillate.
- If scheduling rules change frequently (e.g., ad hoc reprioritizations), you may see large swings in WIP and waiting times.

Process mining supports this via:

- **Temporal aggregation**: analyzing per-shift or per-day patterns of start and completion events.
- **Variant comparison**: days with stable WIP vs days with heavy oscillation and linking to scheduling behavior (e.g., introduction of many urgent jobs).

---

## 3. Root Cause Analysis of Scheduling Ineffectiveness

### 3.1. Limits of static dispatching rules in a dynamic environment

Current rules: FCFS + simple EDD locally.

Observed via process mining:

- Jobs with earlier release dates are sometimes overtaken by later jobs with no due-date justification (FCFS applied only within each work center).
- EDD applied locally does not consider:
  - Remaining routing length.
  - Downstream bottlenecks.
  - Setup changes and sequence-dependent costs.

Root cause:

- Rules operate **myopically**:
  - Each machine optimizes its own short-term queue, ignoring system-wide consequences.
- In a high-mix, sequence-dependent environment, local FCFS/EDD often produce:
  - Frequent switches between families.
  - Long setups.
  - Misaligned priorities vs due dates.

### 3.2. Lack of real-time visibility

Evidence:

- Long periods where high-priority or near-due jobs sit in queues despite machine availability elsewhere that could process them (e.g., alternative machines).
- Machine idle while jobs exist that could be processed there (if operators or planners can’t see global WIP and routing alternatives).

Root cause:

- Schedulers and supervisors may not have:
  - Live dashboards showing queue lengths, job slack times, and machine statuses.
  - Tools to perform short-term re-optimization when reality deviates from plan.

Process mining shows:

- Many patterns of “avoidable delay”:
  - Idle time co-existing with urgent WIP, suggesting poor coordination rather than capacity shortage.

### 3.3. Inaccurate task and setup time estimates

Compare **planned vs actual**:

- For each activity type and machine:
  - Compute bias and variability of (Actual  Planned) times.
- If planned times are systematically lower than actual:
  - Schedules are over-optimistic.
  - Due dates and loading decisions are unrealistic.

Root cause:

- Static routing data and standards not updated with actual MES data.
- No feedback loop from historical performance to planning parameters.

Process mining differentiates:

- If even with accurate durations the system would be overloaded (simulated capacity deficit), then **capacity limitation** is root cause.
- If schedule fails primarily because tasks regularly overrun planned times by large margins, then **data quality / estimation error** is root.

### 3.4. Ineffective handling of sequence-dependent setups

Evidence:

- Setup matrix shows big differences between sequences, but actual sequences look random.
- Setup fraction on bottleneck machines is high and variable day to day.

Root cause:

- Current rules ignore family/setup dependency; they treat jobs as independent FCFS items.
- No explicit logic to batch or sequence to minimize setups.

### 3.5. Poor coordination between work centers

Evidence via process mining:

- Lots of rework/ backflows between specific pairs of operations (e.g., QC  Milling rework).
- Misalignment in capacity:
  - Upstream releasing jobs faster than downstream can handle, causing WIP accumulation and blocking.

Root cause:

- Each work center dispatches based on local queues without central coordination.
- No aggregate load leveling or gated release mechanism to control WIP.

### 3.6. Inadequate strategies for disruptions and hot jobs

From disruption analysis:

- After breakdowns, jobs affected show huge waiting spikes and tardiness.
- Recovery times (time to return to normal queue levels) are long.
- Hot jobs:
  - Sometimes handled too aggressively, starving normal jobs and generating instability.
  - Or, not handled aggressively enough, still late despite high priority.

Root cause:

- No formal rule for rescheduling after a breakdown or priority change:
  - Operators improvise with ad hoc decisions.
- No predictive insight:
  - Breakdowns occur and catch the schedule unaware.
  - Urgent jobs inserted without quantifying impact on other orders.

### 3.7. Distinguishing scheduling logic vs capacity vs variability

Use **what-if simulation and analytical comparisons**:

1. Capacity vs load:
   - Estimate effective capacity per machine from MES (actual throughput hours).
   - Compare to demanded load from actual job mix.
   - If even a perfect schedule (simulated) yields high utilization and tardiness, **capacity is insufficient**.

2. Scheduling logic vs variability:
   - Simulate with:
     - Historical variability in process times.
     - Improved dispatching rules.
   - If performance improves significantly using the same variability, root cause is **scheduling logic**.
   - If improvement is marginal, intrinsic variability (e.g., extreme spread in process times or breakdowns) may dominate.

3. Process mining-based conformance checks:
   - Compare actual sequences to an “ideal” scheduling policy (e.g., shortest processing time or least setup change) and measure deviations.
   - High deviation and poor performance suggests bad scheduling decisions; low deviation and still poor performance suggests structural constraints.

---

## 4. Advanced Data-Driven Scheduling Strategies

### Strategy 1 – Enhanced Dynamic Dispatching with Multi-criteria Rules

**Core idea**

Replace simple FCFS/EDD with **dynamic, multi-factor dispatching rules** that incorporate:

- Job slack / urgency
- Priority class
- Remaining processing workload
- Estimated sequence-dependent setup time if selected next
- Downstream bottleneck load

**Logic**

For each machine at each decision point (when it becomes free), compute a **score** for each candidate job in its queue:

- Let job j have:
  - Due date \(D_j\), current time \(t\)
  - Remaining processing time \(RPT_j\) (sum of expected durations of remaining tasks).
  - Priority weight \(w_{prio}(j)\) (e.g., 1 for normal, 2 for high, 3 for urgent).
  - Current machine M and last job type on M is type a; job j is type b.
  - Expected setup time \(S_{a,b}^M\) from the mined setup matrix.
  - Expected processing time on M: \(p_{j,M}\).

Define job slack:

- \(Slack_j = D_j - t - RPT_j\).

Define an **urgency index**:

- \(U_j = \frac{w_{prio}(j)}{max(Slack_j, \epsilon)}\) (smaller slack  higher urgency).

Define a **setup cost index**:

- \(C_{setup}(j) = \frac{S_{a,b}^M}{\bar{p}_M}\) (normalized by average processing time).

Define **downstream risk**:

- Estimate expected waiting at critical downstream bottlenecks from current queue lengths and task durations.
- Jobs routed through severe bottlenecks get higher weight so they start earlier.

Combine into a dispatch score (to minimize):

- \(Score_j = \alpha \cdot U_j^{-1} + \beta \cdot C_{setup}(j) + \gamma \cdot DownstreamRisk_j + \delta \cdot p_{j,M}\).

Machine always picks job with **lowest Score_j** (or highest priority equivalent).

**How process mining informs it**

- Slack and RPT use **data-driven duration estimates** from mined distributions by machine, family, and operator.
- Setup times \(S_{a,b}^M\) from sequence-dependent setup analysis.
- DownstreamRisk_j based on real-time queue lengths and **historical per-machine waiting-time models** from process mining.

**Addresses pathologies**

- **Poor prioritization**: Explicit urgency index ensures high-priority/near-due jobs rise to the top.
- **Excessive setups**: Setup cost factor encourages sequencing jobs that reduce total setup.
- **Downstream starvation**: Downstream risk term reduces release of jobs that will simply clog bottlenecks and encourages feeding starved ones.
- **WIP and lead times**: Balanced consideration of slack and setups should reduce both tardiness and WIP at critical machines.

**Expected impact**

- Lower weighted tardiness (especially for high-priority jobs).
- Improved setup efficiency (less switching).
- Reduced occurrence of urgent jobs sitting in queues behind low-value tasks.
- Slight increase in planning complexity but implementable as MES dispatch logic or a scheduling service.

---

### Strategy 2 – Predictive Scheduling and Proactive Bottleneck Management

**Core idea**

Use predictive models grounded in MES history to:

- Generate realistic, forward-looking schedules.
- Continually predict **future congestion** and lateness risk.
- Trigger proactive interventions (e.g., re-sequencing, expediting, overtime, maintenance timing).

**Components**

1. **Predictive task duration models**
   - For each activity (machine, operation type), train models (e.g., regression / gradient boosting) with features:
     - Job family, material, batch size
     - Machine ID, operator ID, time-of-day/shift
     - Historical load at the time
   - Output: distribution or expected value + variance of processing and setup times.
   - Use process mining to provide the training set and feature engineering.

2. **Predictive breakdown / maintenance models** (if data supports)
   - Analyze breakdown logs:
     - Time between breakdowns
     - Usage-based metrics (hours since last maintenance, cycles)
   - Build survival / hazard models to estimate breakdown risk per machine in the near future.

3. **Forward simulation / scheduling engine**
   - Use current shop state (WIP, machine status) and predicted durations to:
     - Simulate the next X hours/days under a given dispatching/scheduling rule.
   - Compute predicted lateness per job and utilization per machine.

4. **Proactive bottleneck management**
   - If prediction shows:
     - Specific jobs likely to be late  adjust their priority or sequence now.
     - Bottleneck machine approaching overload  consider:
       - Re-assign tasks to alternative machines.
       - Increase overtime/extra shifts.
       - Delay release of non-critical jobs to avoid WIP explosion.

**How process mining informs it**

- All predictive models use **historical event log data** as ground truth.
- Process mining helps identify:
  - Which features matter (e.g., variant analysis shows certain family-operator combos have higher durations).
  - Where breakdowns most impact flow (targeting machines for predictive maintenance).

**Addresses pathologies**

- **Unpredictable lead times**: Predictions are based on actual performance distributions, not ideal standards.
- **Tardiness**: Early detection of lateness risk allows schedule modifications before due date proximity.
- **Disruptions**: If breakdown risk is high, schedule can:
  - Move critical work earlier/elsewhere.
  - Schedule preventative maintenance when fewer urgent jobs are in the pipeline.
- **Resource utilization**: Better load leveling by anticipating future queues, not just reacting.

**Expected impact**

- More reliable delivery promises.
- Reduced last-minute firefighting for urgent jobs.
- Fewer cascading delays after breakdowns.
- Better use of overtime and maintenance windows.

---

### Strategy 3 – Setup Time Optimization via Intelligent Batching and Sequencing

**Core idea**

Explicitly minimize sequence-dependent setup times, especially at bottleneck machines, by:

- Creating dynamic batches or “campaigns” of similar jobs.
- Optimizing sequencing on key machines to reduce costly family changes, subject to due dates and priorities.

**Logic**

1. **Identify setup-critical machines**
   - From sequence-dependent setup analysis, find machines with:
     - High setup fraction.
     - Highly asymmetric or large setup times between families.

2. **Build setup “distance” matrix**
   - From SetupTime[M][Type_prev][Type_next].
   - Define a “distance” or penalty between families A,B on machine M:
     - e.g., normalized expected setup time.

3. **Batching policy**
   - For each setup-critical machine:
     - Maintain **family-based pools** of jobs waiting or soon to arrive.
   - Dispatch strategy:
     - Form *campaigns*:
       - Process a batch of jobs of same or compatible family until:
         - Diminishing returns vs increasing tardiness for other families.
     - Adjust campaign length dynamically using:
       - Current WIP per family.
       - Due date distribution inside each family.
       - Setup penalties to switch.

4. **Sequencing optimization**
   - For the jobs within a look-ahead window (e.g., 8–24 hours) on that machine:
     - Solve a **sequence optimization problem**:
       - Objective: minimize sum of setup penalties + weighted tardiness (or slack violations).
     - Use heuristic algorithms:
       - Greedy, simulated annealing, genetic algorithms, or specialized heuristics for sequence-dependent setup scheduling.
   - Process mining provides:
     - Accurate penalty matrix.
     - Job parameters (family, due date, expected processing time).

5. **Integration with dispatching**
   - Combine with Strategy 1: treat campaigns as bundles with reliability constraints on due dates.
   - Allow urgent jobs to break campaigns but charge them with “cost” in the optimization to control how often it happens.

**How process mining informs it**

- Setup matrix quantifies the real benefit of sequences.
- Historical analysis also reveals:
  - Typical natural batches (where operators already group families).
  - Where manual “tribal knowledge” works and where it does not.

**Addresses pathologies**

- **Excessive sequence-dependent setups**: Directly targeted.
- **Bottlenecks**: Setup reduction boosts effective capacity at critical machines, reducing WIP and waiting times.
- **Variability**: More stable sequences reduce unpredictability at those stations.

**Expected impact**

- Lower total setup time (especially at bottlenecks).
- Higher throughput and reduced average lead times.
- Better alignment of on-time performance for families frequently processed at constrained resources.

---

## 5. Simulation, Evaluation, and Continuous Improvement

### 5.1. Discrete-event simulation for strategy evaluation

Build a **discrete-event simulation (DES) model** of the shop, parameterized by process-mined data:

- **Entities**: Jobs with:
  - Release times, routing (sequence of operations), due dates, priority.
- **Resources**:
  - Machines, operators, with:
    - Calendar (shifts).
    - Breakdown processes (from breakdown logs).
- **Process times**:
  - Distributions per operation type, machine, family, operator from MES.
- **Sequence-dependent setup model**:
  - SetupTime[M][A][B] distributions.

Embed **dispatching logic**:

- Baseline: current FCFS/EDD.
- Strategy 1: dynamic multi-criteria rule.
- Strategy 3: batching/sequence optimizer at key machines.
- Strategy 2: used both as:
  - Improved duration distributions.
  - Proactive adjustments (e.g., predictive rescheduling events triggered in the simulation).

**Scenarios to test**

1. **Nominal load**:
   - The average historical arrival rate and job mix.

2. **High load**:
   - Increased job release rate (e.g., +20–30%) to stress the system.

3. **High variability**:
   - Larger variance in processing times.

4. **Disruption-rich**:
   - Higher breakdown frequency or severe breakdowns at bottlenecks.
   - Frequent hot jobs with tight due dates.

5. **Mix changes**:
   - Different product mix (more complex or more setup-intensive families).

For each scenario, compare:

- KPI distributions:
  - Mean and percentile of tardiness and lead times.
  - WIP levels and queue lengths by machine.
  - Utilization and setup fraction at key machines.
- Robustness:
  - How sensitive is performance to changes in load or breakdown rates?

Use simulation results to:

- Choose the most robust/beneficial combination of strategies (e.g., Strategy 1 + Strategy 3).
- Tune parameters (weights , , ,  in dispatch score, maximum campaign lengths, etc.).

### 5.2. Continuous monitoring and adaptation with ongoing process mining

Implement a **closed-loop system**:

1. **Operational monitoring**
   - Daily or near-real-time process mining dashboards showing:
     - Tardiness and on-time delivery.
     - WIP and queue times by machine.
     - Setup times and changeover frequency.
     - Schedule adherence (per job, per station).

2. **Drift detection**
   - Compare current period metrics against historical baseline:
     - Use statistical tests/control charts (e.g., CUSUM, EWMA) for:
       - Lead times
       - Setup times
       - Utilization
   - Detect:
     - Gradual drift (e.g., new product mix, aging machinery).
     - Sudden changes (e.g., new shift, new operator team).

3. **Model recalibration**
   - Periodically (e.g., monthly/quarterly):
     - Retrain predictive task duration models with the latest MES data.
     - Update setup matrices for new families or changed setups.
     - Re-estimate breakdown patterns if maintenance policies or equipment change.

4. **Scheduling policy adjustment**
   - Based on monitoring insights:
     - Adjust weights in dispatch rules.
     - Re-tune batching thresholds and campaign sizes.
     - Modify rules for hot-job admission (e.g., limit number of concurrent hot jobs at bottlenecks).

5. **Operational feedback loop**
   - Provide daily performance feedback to supervisors:
     - “Yesterday’s main contributors to tardiness were: MILL-03 queue times, excessive changeovers between families A and C, and a breakdown on HEAT-01.”
   - Encourage shop floor suggestions; compare operator practices with measured outcomes via process mining.

6. **Governance**
   - Establish KPI targets (e.g., % on-time, WIP limits, max setup fraction).
   - Review periodically whether the scheduling strategy still meets targets.
   - When targets fail consistently and structural changes are suspected (new product line, new customers), run new simulation studies with updated models.

---

### Summary

Using detailed MES logs and process mining, Precision Parts Inc. can:

- Precisely quantify current performance, bottlenecks, and scheduling-induced inefficiencies.
- Diagnose root causes separating:
  - Poor scheduling rules,
  - Capacity constraints, and
  - Intrinsic variability/disruptions.
- Design advanced, data-driven scheduling:
  - Multi-criteria dynamic dispatching (Strategy 1),
  - Predictive scheduling and proactive bottleneck management (Strategy 2),
  - Setup-optimized batching and sequencing at key machines (Strategy 3).
- Validate and tune these strategies via discrete-event simulation before deployment.
- Implement a continuous improvement loop where ongoing process mining keeps the scheduling logic aligned with the evolving reality of the shop.

This combination transforms the MES event log from a passive record into the basis of an adaptive, data-driven scheduling system focused on reducing tardiness, stabilizing lead times, lowering WIP, and improving utilization of critical resources.