---

## 1. Analyzing Historical Scheduling Performance and Dynamics (via Process Mining)

### 1.1 Reconstructing the Actual Process and Machine Sequences

**Data preparation**

From the MES event logs:

- **Case ID = Job** (JOB-7001, etc.)
- **Activities**: Job Released, Queue Entry (Machine), Setup Start/End, Task Start/End, Breakdown Start/End, Priority Change, etc.
- **Resources**: Machine IDs, Operators.

Build two key views:

1. **Job-centric event log** (case = job):
   - Sequence of activities: Release  Queue Entry (Cutting)  Setup Start/End  Task Start/End  Queue Entry (next workstation)  …  Final Inspection / Shipping.
   - Used for lead time, flow time, routing, and variant analysis.

2. **Resource-centric event log** (case = machine):
   - Sequence of tasks on each machine: Setup/Process for JOB-1  Setup/Process for JOB-2  Idle  Breakdown  …
   - Used for utilization, sequence-dependent setups, bottleneck analysis.

**Process mining techniques**

- **Process discovery**:
  - Use algorithms like **Heuristic Miner** or **Inductive Miner** on the job-centric log.
  - Outputs: a process model (Petri net / BPMN) showing typical routings, parallelism (e.g., operations that can be done in parallel), rework loops.
- **Performance annotation**:
  - Enrich the discovered model with **time stamps** and **durations**: waiting times, processing times, setup times, inter-operation delays.
- **Resource mining**:
  - Build **resource timelines**: Gantt charts mined from event logs for each machine/operator.

This reconstructs the *actual* flow vs. the theoretical routing from the ERP / routing sheets.

---

### 1.2 Key Metrics and How to Derive Them

#### a) Job flow times, lead times, makespan distributions

For each job (case):

- **Release time** = timestamp of “Job Released”.
- **Completion time** = last relevant event (e.g., “Task End (Quality Inspection)” or “Job Completed/Closed”).
- **Flow time / lead time** = completion – release.
- **Shop floor time** (if some jobs are released earlier but don’t enter shop until later) = completion – first “Queue Entry” event.
- **Job makespan in the shop** = max(task end time) – min(task start time) on shop-floor activities.

Process mining use:

- Use the job-centric process model and compute **per-path distributions**:
  - Lead time distribution per routing variant.
  - Lead time conditioned on **priority**, **due date tightness** (due date – release), and **customer**.
- Visuals:
  - Histograms / violin plots of lead times per variant and priority.
  - KPI: % jobs completed within (due date, due date + X days, etc.).

#### b) Task waiting times (queue times) at each machine

For a given operation on a job:

- **Queue time at machine M** = `Task Start (operation on M)` – `Queue Entry (operation on M)`.

Process mining:

- On the discovered process model, annotate each arc **Job Released  Queue Entry (Cutting)**, **Queue Entry (Cutting)  Task Start (Cutting)** with **average/median waiting time** per machine.
- Produce:
  - **Per-machine** queue time distributions.
  - **Time-of-day** and **day-of-week** patterns (e.g., long queues Monday morning).

KPIs:

- Average and 95th percentile waiting times per machine.
- Ratio of waiting time to processing time per machine and per job type.

#### c) Resource utilization (machines & operators)

Using the resource-centric view for each machine:

- From **Setup Start/End** and **Task Start/End**:
  - **Setup intervals**
  - **Processing intervals**
- From “Breakdown Start/End”:
  - **Down intervals**
- From **gaps** between events:
  - **Idle intervals** (machine available but not processing or in setup).

For a time horizon T (e.g., one month):

- **Utilization (processing)** = sum(processing time) / T.
- **Setup proportion** = sum(setup time) / T.
- **Breakdown proportion** = sum(down time) / T.
- **Idle (available but not used)** = T – (processing + setup + breakdown).

Operators:

- Similar logic, using **Operator ID** in events:
  - Determine simultaneous operations (if one operator can oversee multiple machines).
  - Identify operator bottlenecks (e.g., long waits for setup because operator is busy elsewhere).

Process mining tool:

- **Resource performance charts** / heatmaps:
  - Machines vs. utilization.
  - Operators vs. workload.

#### d) Sequence-dependent setup times

For each machine:

1. Identify sequences: for every **Task End** of job i followed by **Setup Start** for job j on the *same machine*, capture:
   - Previous job type (attributes: material, thickness, part family, operation).
   - Next job type.
   - Actual setup duration = `Setup End – Setup Start`.

2. Build a **setup matrix** per machine:

- Rows: Previous job category.
- Columns: Next job category.
- Cells: average / median setup time, variance, frequency.

You can categorize jobs using:

- Operation/part family (from routing or product code).
- Features (material, thickness, tolerance class).

Process mining use:

- This is **sequence mining** / transition performance mining on the machine-centric log.
- Visualize as:
  - Heatmap: cells with high setup times (e.g., switching from stainless to aluminum).
  - Identify asymmetries (A  B setup  B  A).

Outcome:

- Clear quantitative view of which job sequences are “expensive” in setup and which are “cheap”.

#### e) Schedule adherence and tardiness

From log:

- **Due date** stored per job.
- **Completion time** as above.

Metrics:

- **Tardiness** (T_i) = max(0, completion_i – due_date_i).
- **Lateness** (L_i) = completion_i – due_date_i (can be negative if early).
- KPIs:
  - % jobs on time, mean tardiness, 95th percentile tardiness.
  - Weighted tardiness by priority (e.g., hot jobs count more).

Schedule adherence (if planned schedule is available):

- For each operation:
  - Compare actual start/end times to planned start/end.
  - Compute **start deviation** and **finish deviation**.
- If not: infer “implied schedule” via dispatching rules and compare to a **theoretical best practice** schedule in simulation (see later).

Process mining:

- Annotate each job path with its **lateness**.
- Do **variant analysis**:
  - Compare process paths for on-time vs late jobs.
  - Identify which routings, machines, or patterns correlate with tardiness.

#### f) Impact of disruptions (breakdowns, priority changes)

Breakdowns:

- Identify **Breakdown Start/End** events per machine.
- For each breakdown:
  - List jobs **in queue before breakdown**, jobs **interrupted**, and jobs **queued after**.
  - Measure:
    - Additional waiting time for affected jobs vs. typical waiting time.
    - Increase in tardiness for jobs that went through breakdown windows vs. baseline jobs.

Priority changes / hot jobs:

- From “Priority Change” events (e.g., “High (Urgent)”):
  - Mark jobs as “hot” from the timestamp onward.
  - Measure:
    - How quickly they get scheduled after priority upgrade.
    - The **rescheduling ripple**: Which jobs are delayed (queue re-order events) and by how much; increase in their tardiness.

Process mining:

- **Temporal correlation analysis**:
  - Overlay breakdown time windows on resource Gantt charts and job lead time plots.
- **What-if attribution**:
  - For a sample of late jobs, determine whether they were:
    - Affected by breakdown windows.
    - Delayed due to insertion of hot jobs.
    - Delayed purely due to congestion / poor sequencing.

---

## 2. Diagnosing Scheduling Pathologies

Using the metrics above, we can diagnose systemic problems.

### 2.1 Bottlenecks and Throughput Limits

Using resource utilization and queue time:

- Bottleneck machines show:
  - High processing utilization (e.g., > 85–90%).
  - Long average queue times and large queues.
  - “Upstream” resources often waiting for them to clear.

Process mining evidence:

- **Bottleneck analysis**:
  - Compute **average waiting time** before each machine.
  - Annotate process model: transitions entering bottleneck nodes are “thick and red” (high waiting).
- Link bottlenecks to **job tardiness**:
  - For each late job, see which machine contributed the largest portion of its total waiting time.

Outcome:

- A ranked list of bottleneck machines (e.g., HEAT-01, MILL-03) and their impact on throughput and tardiness.

### 2.2 Poor task prioritization

Symptoms:

- High-priority or near-due jobs still experience long waits.
- Jobs with plenty of slack processed ahead of tight jobs.

Evidence:

- For each job, compute **slack at time of queue entry**:
  - Slack = due_date – (current_time + estimated remaining processing time).
- For each job at queue entry, record **its position** in the actual processing sequence.

Process mining:

- **Queue snapshot analysis**:
  - When a machine finishes a job and selects the next one, compare:
    - Actual selected job vs. the job with minimal slack or highest priority.
- Summarize:
  - % of decisions where machine processes a lower-priority / higher-slack job while a more urgent job is waiting.
- Variant analysis:
  - Compare flows of **late hot jobs** vs. **on-time hot jobs** to see whether they got early priority in queues.

### 2.3 Suboptimal sequencing and excessive setup time

Symptoms:

- High proportion of time in setup.
- Frequent long setup transitions between incompatible job families.

Evidence:

- From the sequence-dependent setup matrix:
  - Identify high setup transitions that occur frequently (e.g., “material A  material B”).
- On bottleneck machines, quantify:
  - Average **setup time per unit of processing time**.
  - Compare to best sequences:
    - Simulate “ideal” grouping (e.g., process all jobs of family F together) and compare total setup time vs. actual.

Process mining:

- **Trace ordering analysis** on machine-centric logs:
  - Find recurring patterns of alternating families (e.g., A  B  A  B), which maximize setup.
- Provide examples where:
  - On a given day, jobs have been interleaved in a way that adds hours of avoidable setup.

### 2.4 Starvation of downstream resources

Symptoms:

- Some machines have frequent idle periods while their downstream machines occasionally run out of work.
- Conversely, some stages are overwhelmed with WIP while upstream is idle.

Evidence:

- For each machine pair (upstream  downstream in routing):
  - Correlate the upstream output (task completions) with downstream queue levels.
- Look for patterns:
  - **Starvation**: downstream machine idle time coincides with no completed jobs from upstream.
  - **Blocking**: upstream machine idle or low throughput because downstream is overloaded or due to WIP caps.

Process mining:

- Use **token-based replay** or **alignment-based performance analysis** in the discovered process model to trace:
  - Where tokens accumulate (WIP hotspots).
  - Where tokens frequently “dry up” (starvation).
- Time-series heatmaps of queue lengths per machine.

### 2.5 Bullwhip effect in WIP due to scheduling variability

Symptoms:

- Large fluctuations of WIP between stations over time.
- Periods of excessive WIP buildup followed by sudden drops.

Evidence:

- Time series of **queue length** per machine.
- Cross-correlation:
  - Sudden bursts of releases or hot jobs, causing upstream machines to overproduce and flood downstream queues.

Process mining:

- Compute **WIP profile over time**:
  - Number of jobs between operation X and Y at time t.
- Identify:
  - Peaks and troughs; relate peaks to disruptions (e.g., clearing backlog after downtime or after pushing a batch for a large order).
- Variant analysis:
  - Compare WIP trajectories for periods with few disruptions vs. many.

---

## 3. Root Cause Analysis of Scheduling Ineffectiveness

### 3.1 Limitations of existing static dispatching rules

Current rules: FCFS / EDD applied locally.

Issues:

- They ignore:
  - Sequence-dependent setup times.
  - Downstream congestion and bottlenecks.
  - Real-time breakdown info and priority changes.
- EDD alone can be poor when:
  - Processing times differ widely.
  - Due dates are tight and variable.
- FCFS leads to:
  - High average flow times when job processing times are highly variable.

Process mining evidence:

- Compare actual queue decisions to **idealized dynamic priority** (e.g., ATC rule) using logged data.
- Show that many late jobs had high slack consumption but were still not prioritized.

### 3.2 Lack of real-time visibility

If the MES only records events but doesn’t feed a real-time decision engine:

- Machines make local decisions without knowledge of:
  - Queue lengths elsewhere.
  - Updated due dates / customer commitments.
  - Current bottleneck status.

Process mining evidence:

- Decisions taken in queue selection are not correlated with global state:
  - E.g., jobs routed to an already congested machine while alternative equivalent machines are idle.
- Multi-machine station analysis:
  - Show load imbalance: one milling center overloaded while others underused.

### 3.3 Inaccurate duration / setup estimates

If planning uses coarse or outdated standard times:

- Scheduled load vs. actual load mismatch.
- Frequent schedule slippage and unreliable due date promises.

Evidence:

- Compare **planned vs. actual** across:
  - Part families, operators, shifts.
- Compute bias and variance:
  - Systematic underestimation for certain combinations (e.g., complex geometries, new operators).
- Use process mining to identify:
  - Which conditions lead to long tails in processing or setup times.

### 3.4 Ineffective handling of sequence-dependent setups

If the system ignores sequence:

- Jobs released in arbitrary order.
- Work centers process jobs in EDD or FCFS without considering setup.

Evidence:

- High percentage of transitions between “dissimilar” job families.
- On bottleneck machines:
  - Setup/wait ratio high despite potential to batch similar jobs.
- Process mining:
  - Identify days or weeks where natural job mix would have allowed much lower setup times if sequenced better.

### 3.5 Poor coordination between work centers

Symptoms:

- Local optima decisions:
  - Upstream machines work to keep themselves busy even if downstream is full.
- No mechanism to:
  - Throttle releases.
  - Sequence jobs to support bottleneck or shared resource.

Evidence:

- Use process model to show:
  - Clearly that certain work centers overproduce WIP while others are starved.
- Analysis of **job handover times**:
  - Large, variable delays between completion on one machine and queue entry on the next (hand-off or transport issues).

### 3.6 Inadequate disruption response

Static rules do not adapt to:

- Machine breakdowns.
- Hot jobs insertion.
- Sudden capacity changes (absence of operator).

Evidence:

- During breakdown windows:
  - No systematic rerouting to alternative machines even when available.
- Hot jobs:
  - Often delayed by ongoing long-processing jobs because there is no preemption or dynamic re-prioritization.

### 3.7 Distinguishing scheduling logic issues vs capacity limits or intrinsic variability

Process mining can help separate:

1. **Capacity issues**:
   - Even under ideal policy (simulated), tardiness remains high due to insufficient capacity.
   - Evidence: high utilization (>90%) across critical machines and little idle even under good sequencing scenarios.

2. **Scheduling issues**:
   - Simulations with same capacity but better sequencing and dispatching show much lower tardiness and WIP.

Approach:

- Build a **baseline simulation** using mined distributions and actual routing, and:
  - Run with current logic.
  - Then run with improved logic (e.g., ATC, batching).
- If performance improves significantly without changing resource capacities:
  - Root cause is primarily scheduling policy.
- If improvement is small:
  - The system is capacity constrained or highly variable; scheduling alone can’t fix it.

---

## 4. Advanced Data-Driven Scheduling Strategies

I’ll outline three distinct strategies, each explicitly using process mining outputs.

### Strategy 1: Dynamic Multi-Criteria Dispatching with Setup Awareness

**Core logic**

Design a dynamic priority index for each job waiting at each machine that considers:

- Job due date and slack.
- Job priority (normal vs hot).
- Remaining processing time to completion.
- Estimated setup time from the last job on that machine.
- Downstream congestion.

A typical form: an **Adjusted Apparent Tardiness Cost (ATC-like)** rule:

For each job j in the queue of machine m at time t:

Priority score P(j) =

- High base for urgent/high-priority jobs:
  - Factor f1(priority, slack)
- Discounted by processing time p_j (favor shorter jobs to reduce average flow time).
- Penalized for setup time s(prev_job, j):
  - Factor f2(setup).
- Adjusted for downstream bottleneck impact:
  - Factor f3(downstream queue / bottleneck status).

Example sketch:

P(j) = w1 * exp( max(0, (d_j  t  r_j) / k1))   // due date tightness (d_j: due date, r_j: remaining processing time)
     + w2 * priority_weight(j)                    // high for hot jobs
      w3 * estimated_setup(prev_job, j)         // large penalty if big setup required
      w4 * downstream_congestion_penalty(j)     // avoid sending jobs into overloaded downstream stations

Where:

- w1–w4 are weights learned/calibrated from historical performance.
- k1 is a normalization factor based on historical slack distribution.

Process mining inputs:

- **Estimated setup(prev_job, j)** from the sequence-dependent setup matrix.
- **Remaining processing time r_j**:
  - Sum of median durations for remaining operations, by routing variant, from mined time distributions.
- **Downstream congestion penalty**:
  - From recent queue statistics at next critical machine(s).

How it addresses pathologies:

- Reduces **tardiness**:
  - Explicitly prioritizes jobs with low slack and hot jobs, while still considering setup and processing times.
- Reduces **total setup time**:
  - Favors sequences that minimize setup at constrained machines (high w3).
- Avoids **starvation and bullwhip**:
  - Penalizes sending work into already congested downstream queues (w4), smoothing WIP.
- Improves **resource utilization**:
  - Keeps bottleneck machines busy on jobs that truly drive due date performance.

Expected impact:

- Reduced mean and especially 95th percentile tardiness.
- Lower ratio of setup time to processing time, particularly on bottlenecks.
- More stable WIP profiles.

Implementation notes:

- Implement as a **real-time dispatching engine** connected to MES:
  - Every time a machine becomes free, compute P(j) for all jobs in its queue and select max P(j).
- Start on key bottleneck machines, then extend.

---

### Strategy 2: Predictive Scheduling and Proactive Bottleneck Management

**Core logic**

Use predictive models from historical logs to:

1. **Predict task durations and setup times** more accurately than static standards.
2. **Predict breakdown risk** / effective availability.
3. Generate a **finite-capacity schedule** that anticipates bottlenecks and disruptions.

Components:

1. **Duration prediction models**:

- Train regression / machine learning models for:
  - Task processing time: f(job features, machine, operator, shift, material, complexity).
  - Setup time: f(previous job family, next job family, machine, operator).
- Use logs labeled with actual durations.

2. **Predictive maintenance input (if possible)**:

- Model breakdowns per machine:
  - Time between failures (TBF) and time to repair (TTR) distributions.
  - Optionally, condition on operating hours, vibration data, etc. (if available).
- Output: predicted availability calendar (e.g., probability of downtime by hour).

3. **Predictive finite-capacity scheduling**:

- Using the predicted durations and availability calendars:
  - Generate a rolling schedule (e.g., for next 1–3 days) across critical resources:
    - Use a **constraint-based scheduler** or **metaheuristic** (e.g., tabu search, genetic algorithm).
    - Objective: minimize weighted tardiness + setup + WIP.

4. **Proactive bottleneck management**:

- Use process mining to identify the current bottleneck machines.
- Use the predictive schedule to:
  - Pull jobs forward or postpone to keep bottlenecks continuously but not over- loaded.
  - Time-release jobs from upstream to avoid building WIP ahead of non-bottlenecks.

Process mining inputs:

- Historical distributions of times and routing variants.
- Breakdown patterns and their impact on queues and tardiness.
- Setup dependency matrix per machine.

How it addresses pathologies:

- **Lead time / due date reliability**:
  - Using realistic duration distributions and availability, promise dates become more reliable.
  - Schedules are less likely to be infeasible.
- **Breakdown impact**:
  - When a machine with high breakdown risk is critical, schedule with buffers or alternative routings in mind.
- **Poor planning vs capacity**:
  - Helps reveal whether current due dates are unrealistic given capacity and actual times.

Expected impact:

- Reduced schedule slippage and rescheduling frequency.
- Tighter **confidence intervals** around promised completion dates.
- Proactive mitigation of congestion: fewer panic reactions to bottlenecks.

Implementation notes:

- This is a **rolling horizon** approach:
  - Recompute schedule every shift/day using latest data and reassign pending tasks where feasible.
- Integrate with Strategy 1:
  - The finite schedule provides target windows; dispatching rules manage real-time deviations.

---

### Strategy 3: Setup Time Optimization via Intelligent Batching and Sequence Planning

**Core logic**

Focus specifically on minimizing sequence-dependent setup times on key machines (e.g., specific CNC mills, heat treatment ovens), without unduly harming due dates.

Main elements:

1. **Identify dominant setup drivers**:

- From the sequence matrix, cluster jobs into **families**:
  - Material group, thickness, part family, similar tooling requirements.
- For each bottleneck machine, identify:
  - “Friendly” sequences: low setup time.
  - “Hostile” sequences: high setup time.

2. **Batch and sequence planning at bottlenecks**:

At each bottleneck machine:

- Establish **time buckets** or **batch windows** (e.g., half-day or shift).
- For each window:
  - Select a **primary family** to run, based on:
    - Sum of workload from that family due within a certain horizon.
    - Minimized predicted tardiness if they are processed together.
- Within that family:
  - Sequence jobs EDD or by ATC to satisfy due dates.
- Between families:
  - Sequence families to minimize total setup (solving a traveling salesman-like problem using the setup matrix as distance).

3. **Release control and upstream alignment**:

- Coordinate upstream machines to:
  - Release or prioritize jobs aligned with upcoming bottleneck family windows.
  - For example, if heat treatment will run stainless steel batch at 14:00–18:00:
    - Ensure upstream operations for those jobs finish before 14:00.
    - Avoid pushing incompatible jobs that would “break” the batch.

Process mining inputs:

- Setup matrix per machine.
- Workload and due date distribution by job family.
- Historical evidence of lateness by family and machine.

How it addresses pathologies:

- **Excessive setup times**:
  - Directly targets the largest setup transitions to reduce them.
- **Bottleneck utilization**:
  - Increases effective processing time vs setup time at bottlenecks.
- **WIP smoothness**:
  - Better alignment of upstream flow to bottlenecks reduces last-minute rush jobs that force family switches.

Expected impact:

- Significant reduction in setup time at targeted machines (10–30% is realistic).
- Higher effective capacity at bottlenecks  improved throughput and tardiness.
- Fewer “oscillating sequences” like A–B–A–B.

Implementation notes:

- Start with **1–2 key bottleneck machines**.
- Implement a **simple family-based schedule**:
  - For example, afternoon = family X, morning = family Y, with some flexibility for emergencies.
- Combine with Strategy 1:
  - Within a family window, use dynamic dispatching for due-date and priority control.

---

## 5. Simulation, Evaluation, and Continuous Improvement

### 5.1 Discrete-Event Simulation for Strategy Evaluation

Use a discrete-event simulator (DES) of the shop, calibrated with process-mined data.

**Model parameterization (from process mining)**:

- **Routing**:
  - From discovered process model: all typical operation sequences, including rework loops.
- **Processing, setup, and transport times**:
  - From mined distributions (per machine, job family, operator, etc.).
  - Use empirical distributions or parametric fits.
- **Setup time dependencies**:
  - Implement the sequence-dependent setup matrix per machine.
- **Breakdowns**:
  - Use TBF and TTR distributions mined from breakdown events.
- **Job arrivals and due date tightness**:
  - From historical release patterns and due date rules.

**Scenarios to test**:

1. **Baseline**:
   - Current dispatching rules (FCFS/EDD) with current release policies.

2. **Strategy 1 only**:
   - Dynamic multi-criteria dispatching on all or selected machines.

3. **Strategy 1 + Strategy 3**:
   - Dispatching + family-based batching at bottlenecks.

4. **Strategy 2 (predictive scheduling)**:
   - Finite-capacity, predictive schedule with simple local dispatching.

5. **Combined strategy**:
   - Strategy 2 for planning + Strategy 1 for execution + Strategy 3 for setup-intensive machines.

Stress scenarios:

- **High load**:
  - Arrival rates increased by 10–30%.
- **Frequent disruptions**:
  - Increased breakdown frequency and/or longer TTR.
- **High hot-job frequency**:
  - More urgent jobs inserted randomly with tight due dates.

KPIs to compare:

- Mean and percentile **tardiness** (overall and by priority).
- Average **WIP** and WIP profile variance.
- Average and peak **queue lengths** at key machines.
- **Utilization** breakdown: processing vs setup vs idle vs breakdown.
- **Lead time** distributions by product family.

Decision criteria:

- Identify strategies that:
  - Reduce high-percentile tardiness and WIP substantially.
  - Do not require unrealistic operational changes.
  - Behave robustly under disruptions.

---

### 5.2 Continuous Monitoring and Adaptation Using Process Mining

After deploying a chosen strategy:

1. **Real-time dashboards** (fed by MES, updated at least every few minutes):

- Queue lengths and waiting times at each machine.
- Machine utilization, setup time ratios.
- Hot job performance.
- Predicted vs actual task durations for current jobs.

2. **Periodic process mining (e.g., weekly/monthly)**:

- Re-run discovery and performance analysis to:
  - Check if routing patterns changed (new products or process variants).
  - Update duration and setup distributions.
  - Update setup matrices and job family clustering.

3. **Drift detection**:

- Compare recent period metrics to historical baselines:
  - If processing times for a certain family have increased significantly:
    - Trigger review of standard time and schedule parameters.
  - If setup times between certain families drift upward:
    - Investigate tooling, standard work, or training changes.
- Use **conformance checking**:
  - Ensure actual process still aligns with the assumed model; deviations might imply new bottlenecks or misrouting.

4. **Adaptive parameter tuning**:

- For Strategy 1 weights (w1–w4):
  - Periodically re-optimize using:
    - Off-line simulation with updated data.
    - Or reinforcement learning based on realized performance.
- For Strategy 2 predictive models:
  - Retrain models periodically with latest logged data to maintain predictive accuracy.

5. **Control loops**:

- Establish regular review cycles (e.g., monthly S&OP / operations review) where:
  - Process mining reports summarize:
    - Tardiness trends, WIP profiles, bottleneck shifts, setup ratios.
  - Scheduling strategy parameters are adjusted based on evidence.

---

### Summary

By mining the MES logs, Precision Parts Inc. can:

- Precisely quantify where time is lost (waiting vs setup vs processing) and how disruptions and choices propagate.
- Identify true bottlenecks, poor priority decisions, and harmful sequencing patterns.
- Design advanced scheduling strategies that:
  - Use multi-criteria, setup-aware dynamic dispatching (Strategy 1).
  - Build realistic, predictive schedules with proactive bottleneck management (Strategy 2).
  - Systematically reduce sequence-dependent setup times via intelligent batching (Strategy 3).

Combining these strategies, validated via discrete-event simulation and continuously refined with ongoing process mining, provides a robust pathway to lower tardiness, reduce WIP, stabilize lead times, and improve utilization of critical resources in a complex job shop environment.