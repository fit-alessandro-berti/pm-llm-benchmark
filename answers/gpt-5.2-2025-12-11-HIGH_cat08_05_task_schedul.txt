## 1) Analyzing historical scheduling performance and shop-floor dynamics (from MES logs)

### 1.1 Build an analyzable event log (job + resource perspectives)
Before “mining,” I would standardize the MES data into a **lifecycle log** with consistent semantics:

- **Case notion:** `Case ID = Job ID` (JOB-7001, …).
- **Activity granularity:** split into *Queue Entry*, *Setup Start/End*, *Process Start/End*, *Inspection*, *Move*, *Rework*, *Hold*, etc.
- **Timestamps:** ensure at least `start_timestamp` and `end_timestamp` for each setup and processing interval (derive if only events exist).
- **Resources:** `machine_id`, `operator_id` with calendars (shifts, breaks).
- **Attributes:** due date, priority, product family/material, routing/operation number, planned time, etc.
- **Disruption objects:** breakdown intervals per machine; priority-change events per job.

Because a job shop has many variants and multiple interacting objects (jobs, machines, operators), I would also create an **object-centric log (OCPM)** view:
- Object types: **Job**, **Operation**, **Machine**, **Operator**
- Relations: an operation *belongs to* a job; an operation *uses* a machine/operator; breakdown *affects* a machine.

This avoids forcing everything into a single-case view when you need machine-centric sequencing and capacity analysis.

---

### 1.2 Reconstruct actual job routings and machine sequences
**Job-flow reconstruction (job-centric):**
- Apply **process discovery** (e.g., *Inductive Miner* / *Heuristics Miner*) on job lifecycle events to learn:
  - typical operation orders (Cutting  Milling  …),
  - common deviations (skips, rework loops, detours),
  - concurrency/overlaps (if any).
- Use **variant analysis** to quantify how many distinct routings exist and which variants correlate with lateness.

**Machine sequence reconstruction (resource-centric):**
- For each machine (e.g., MILL-03), sort all **processing start times** to obtain the *actual sequence of jobs*.
- Insert setup intervals (from Setup Start/End) between consecutive jobs to build a true machine timeline:
  - productive processing,
  - setup,
  - idle/starved,
  - down (breakdown).

This “resource view” is crucial for sequence-dependent setups and bottleneck behavior.

---

### 1.3 Metrics and process mining techniques to quantify performance

#### (a) Job flow times, lead times, makespan distributions
Compute from logs:
- **Job lead time** = `completion_time(job)  release_time(job)`
- **Flow time by operation** = `end(op)  queue_entry(op)` (or from first queue entry to last finish)
- **Makespan** for a period (week/month) = `max(job completion)  min(job release)` for jobs released in that period.

Process-mining support:
- **Performance overlays** on discovered models: average/median/95th percentile durations per path and per activity.
- **Distribution analysis** (not just averages): heavy tails often explain “unpredictable lead time.”

Outputs I’d expect:
- Lead time histograms by product family and by routing variant.
- “Where the time goes” decomposition: queue vs setup vs processing vs rework.

---

#### (b) Task waiting times (queue times) by machine/work center
For each operation instance:
- **Queue time** = `setup_start  queue_entry` (or `process_start  queue_entry` if no setup logged)
- **Preemption/holds**: detect gaps between setup/process segments if jobs are interrupted.

Process mining techniques:
- **Bottleneck analysis / waiting-time heatmaps** by machine, shift, day-of-week.
- **Queueing profile**: arrival rate vs service rate over time windows.

Key deliverables:
- Pareto chart of machines contributing most to total waiting time.
- Time-series of queue time percentiles (p50/p90) to expose volatility.

---

#### (c) Resource utilization (productive, idle, setup, down)
For each machine, build an interval ledger and compute:
- **Processing utilization** = processing_time / available_time
- **Setup utilization** = setup_time / available_time
- **Down fraction** = breakdown_time / available_time
- **Idle/starved** = available_time  (processing + setup + down)

This becomes an OEE-like breakdown (without quality unless scrap/rework is logged):
- Availability impact (breakdowns)
- Performance impact (slow cycles vs planned)
- Setup loss

Process mining features:
- **Resource profiles**: utilization by shift/operator/product family.
- **Handover/assignment analysis**: operator-machine-job coupling that drives variability.

---

#### (d) Sequence-dependent setup time mining (core requirement here)
Goal: quantify how setup time depends on **(previous job  next job)** and other attributes.

Steps:
1. For each machine, create ordered pairs of consecutive processed jobs:
   - `(prev_job, next_job, machine, prev_attributes, next_attributes)`
2. Compute **setup_duration** from logs:
   - `setup_end  setup_start`
3. Build explanatory features:
   - product family change (same/different),
   - material change,
   - toolset/fixture change indicator,
   - dimension range change,
   - operator, shift,
   - time since last maintenance, etc.

Models/analyses:
- **Setup matrix**: average/median setup time for transitions between families (Family AB, AC, …).
- **Regression / ML model** to predict setup time:
  - e.g., gradient-boosted trees for nonlinear effects,
  - mixed-effects models to separate machine vs operator effects.
- **Statistical significance** tests: quantify which attribute changes drive most setup penalty.

Deliverables:
- “Changeover penalty” table per machine (top 20 worst transitions).
- Expected setup time estimator usable directly in scheduling (see strategies).

---

#### (e) Schedule adherence and tardiness (due-date performance)
From logs:
- **Tardiness** = `max(0, completion_time  due_date)`
- **Lateness** = `completion_time  due_date` (negative allowed)
- **Weighted tardiness** = tardiness × priority_weight
- **On-time delivery rate** by priority class, family, customer, routing variant.

If planned schedules exist (even crude), add:
- **Conformance checking** between planned start/finish vs actual:
  - deviation distributions by machine and operation type,
  - frequency of resequencing.

Process mining tools:
- **Milestone conformance** (e.g., due-date risk at each operation completion).
- **On-time vs late** cohort comparison (variant + performance comparison).

---

#### (f) Impact of disruptions (breakdowns, hot jobs, priority changes)
Breakdowns:
- Join breakdown intervals to machine timelines; compute:
  - lost capacity hours,
  - queue growth rate during/after breakdown,
  - tardiness impact on jobs waiting for that machine.
- Use **interrupted service detection** if jobs pause mid-operation.

Hot jobs / priority changes:
- Detect **priority-change events** and measure:
  - number of preemptions caused,
  - downstream ripple: extra waiting introduced for displaced jobs,
  - net benefit: whether hot job actually meets due date and at what cost.

Analytical approaches:
- **Event study**: KPI comparison in windows (e.g., 24h before vs 24h after a breakdown).
- **Counterfactual simulation**: “what if the breakdown hadn’t occurred” using mined distributions.

---

## 2) Diagnosing scheduling pathologies (what’s going wrong, with evidence)

### 2.1 Bottlenecks and their throughput/WIP impact
Evidence to extract:
- Machines with **consistently high effective utilization** (processing+setup near practical limit, e.g., >85–90%).
- Machines where **queue time dominates** total operation time.
- Machines that appear on the **critical path** of most late jobs.

Process mining methods:
- **Bottleneck analysis**: rank resources by contributed waiting time and by “queue amplification” (WIP buildup).
- **Criticality analysis**: fraction of late jobs that waited at machine X more than Y hours.

Expected pathology in job shops:
- A small number of specialized machines (e.g., heat treat, grinding, a specific CNC) act as “drums,” but local rules elsewhere overload them unpredictably.

---

### 2.2 Poor prioritization (local EDD/FCFS failing globally)
Evidence patterns:
- High-priority/near-due jobs spending long periods in queues behind lower-urgency work.
- Frequent “priority inversions” (a hot job enters queue but starts much later than feasible).

Process mining:
- **Resource contention periods**: identify times when many jobs compete for a bottleneck and see the chosen sequence.
- **Cohort analysis**: compare on-time vs late jobs:
  - differences in average queue time at bottleneck machines,
  - differences in number of preemptions or route deviations.

---

### 2.3 Suboptimal sequencing causing excessive setup time
Evidence:
- High setup fraction on bottleneck machines (e.g., setup >25–40% of occupied time).
- Frequent alternation between families/materials/tools (ABAB), inflating changeovers.

Process mining:
- **Transition analysis** on machine sequences using the setup matrix:
  - compute realized setup vs “best-case” setup (if jobs had been grouped by family),
  - quantify avoidable setup hours per week.

---

### 2.4 Starvation/blocking and poor inter-work-center coordination
Evidence:
- Downstream machines idle while upstream queues are high (misaligned release/dispatch).
- Jobs arriving in bursts leading to feast/famine behavior.

Process mining:
- **Time-synchronized machine-state analysis**:
  - correlate idle time on machine Y with queue length upstream.
- **Cumulative Flow Diagram (CFD)** derived from queue entry/start/end events to visualize WIP waves.

---

### 2.5 WIP bullwhip and lead-time volatility from variability + myopic dispatching
Evidence:
- Large swings in WIP between stations over days/weeks.
- Wide lead time distributions even for similar jobs.

Process mining:
- **Throughput and WIP time series** + variability decomposition:
  - processing time variance,
  - setup variance,
  - breakdown-driven variance,
  - dispatching-driven variance (resequencing, preemption).

---

## 3) Root cause analysis: why the current scheduling approach is ineffective

### 3.1 Likely root causes (and how logs confirm/refute them)
1. **Static local dispatching rules are myopic**
   - FCFS/EDD at each work center ignores downstream bottlenecks and setup transitions.
   - Log evidence: jobs optimized locally create downstream congestion; long waits concentrate at a few machines.

2. **No real-time global state awareness**
   - Dispatching does not account for current WIP, machine health, or future bottleneck load.
   - Log evidence: repeated release of work into already-congested bottlenecks (CFD widening at specific stages).

3. **Inaccurate duration and setup estimates**
   - Planned vs actual duration gaps; operator/machine/product effects not modeled.
   - Log evidence: systematic bias (e.g., grinding always 20% longer on night shift, or certain operators faster/slower).

4. **Sequence-dependent setup ignored**
   - Dispatching chooses next job without accounting for transition cost.
   - Log evidence: many high-penalty transitions; setup time spikes follow frequent family switching.

5. **Disruption response is reactive and destabilizing**
   - Breakdowns and hot jobs trigger ad-hoc resequencing that propagates delays.
   - Log evidence: after breakdowns, queue times and tardiness surge for 1–3 days; hot jobs cause measurable displacement cost.

---

### 3.2 Distinguishing “bad scheduling” vs “insufficient capacity / inherent variability”
Process mining helps separate these:

- **Capacity limitation signature**
  - bottleneck utilization (processing+setup) persistently near practical max,
  - even well-sequenced periods still late,
  - simulation with “ideal scheduling” still misses due dates unless capacity increases.
- **Scheduling logic limitation signature**
  - significant avoidable setup hours,
  - priority inversions,
  - high idle time on non-bottlenecks while bottlenecks overloaded,
  - large gains in what-if sequencing scenarios without adding capacity.

Methodologically:
- Use **conformance/performance comparison** between:
  - actual realized sequences,
  - “optimized” sequences (setup-minimized or tardiness-minimized) computed offline,
  - then quantify the theoretical improvement gap.

---

## 4) Advanced data-driven scheduling strategies (3+)

### Strategy 1 — Dynamic multi-criteria dispatching with setup- and bottleneck-awareness (real-time scoring)
**Core logic**
At each machine decision point, choose the next job by maximizing a score that blends:
- due-date risk (slack),
- priority,
- remaining work content,
- predicted downstream bottleneck congestion,
- **predicted sequence-dependent setup time** for (current job on machine  candidate job),
- optional WIP control term.

A practical form is an **ATC/ATCS-style** rule (Apparent Tardiness Cost with Setups), extended with downstream load:
- Prefer jobs with high tardiness risk *unless* setup penalty is extreme, and avoid feeding already overloaded downstream resources.

**How process mining informs it**
- Setup-time prediction model from Section 1.3(d).
- Empirical distributions of processing times by machine/family/operator.
- Bottleneck identification: weight downstream congestion higher when the machine feeds the bottleneck.

**Pathologies addressed**
- Reduces priority inversions (explicit tardiness risk).
- Cuts avoidable setup (explicit setup cost).
- Improves flow (avoids sending work into downstream overload).

**Expected KPI impact**
- Lower weighted tardiness and fewer extreme late jobs (tail reduction).
- Reduced average setup time on bottleneck machines.
- Improved lead-time predictability (less resequencing chaos).

---

### Strategy 2 — Predictive, rolling-horizon scheduling (“digital twin” + frequent rescheduling)
**Core logic**
Move from purely dispatching to a **rolling-horizon scheduler**:
- Every X minutes or on events (breakdown start/end, hot job arrival, operation completion), re-optimize the next horizon (e.g., 1–3 days) using:
  - predicted processing/setup times (probabilistic),
  - machine calendars,
  - current WIP positions,
  - due dates and priority costs,
  - constraints (routing precedence, machine eligibility).

Optimization approaches that work in job shops:
- **Constraint Programming (CP-SAT)** or **MILP** for smaller horizons,
- metaheuristics (tabu search, genetic algorithms) for faster near-optimal solutions,
- robust/stochastic variants to hedge against uncertainty.

**How process mining informs it**
- Calibrated time distributions and correlations (e.g., milling time depends on material + operator).
- Breakdown models (MTBF/MTTR per machine) mined from breakdown logs.
- Rework probabilities / inspection fail loops if present.

**Pathologies addressed**
- Handles dynamic disruptions systematically (not ad-hoc).
- Improves promise dates and internal due-date setting (more realistic duration modeling).
- Reduces instability by controlled rescheduling (e.g., freeze window for near-term operations).

**Expected KPI impact**
- Higher on-time delivery, especially under disruptions.
- Better utilization balance: fewer starving periods on flexible resources.
- More accurate customer lead-time quotes (prediction intervals, not single dates).

---

### Strategy 3 — Setup-time optimization at bottlenecks via family batching + sequence optimization
**Core logic**
For identified bottleneck machines (e.g., GRIND-01, HEAT-TRT-01):
- Maintain a candidate set of waiting jobs (and optionally soon-to-arrive jobs within a look-ahead window).
- Compute a sequence that minimizes total setup time using the mined **transition setup matrix** while respecting due dates/priority constraints.

This is essentially a **sequence optimization** problem (TSP-like with precedence and due-date penalties). Practical policies:
- **Campaigning/batching by family/tooling** with limits:
  - maximum batch size,
  - maximum allowed waiting (to avoid starving urgent jobs),
  - “escape clauses” for hot jobs.
- Hybrid objective: minimize (setup time + weighted tardiness).

**How process mining informs it**
- Transition penalties learned from historical sequences.
- Identification of “setup families” that truly share tooling (derived empirically rather than assumed).

**Pathologies addressed**
- Directly attacks avoidable setups (often a major bottleneck time sink).
- Stabilizes bottleneck output, reducing upstream WIP waves and downstream starvation.

**Expected KPI impact**
- Reduced setup hours on the bottleneck (often converts directly into throughput).
- Lower WIP (less time wasted in setups means faster queue depletion).
- Improved lead time and reduced lateness variance.

---

### (Optional but often high-impact) Strategy 4 — WIP control with Drum-Buffer-Rope / CONWIP tied to the true bottleneck
**Core logic**
- Schedule the bottleneck (“drum”) explicitly (using Strategy 3).
- Set a protective time buffer before it.
- Control job release (“rope”) so WIP stays within a target (CONWIP).

**Why it matters**
Even excellent dispatching fails if release is uncontrolled; WIP explodes and lead time becomes unbounded.

---

## 5) Simulation, evaluation, and continuous improvement

### 5.1 Discrete-event simulation (DES) parameterized from process mining
Build a DES “digital twin” where parameters come from mined log statistics:

**Inputs mined from logs**
- Routing distributions / variant probabilities (or explicit routing rules by product type).
- Processing-time distributions by (machine, operation, family, operator/shift).
- Setup-time model: transition matrix or predictive model.
- Breakdown processes: MTBF/MTTR per machine (and possibly condition-dependent).
- Arrival patterns: release rates by day/shift; hot job arrival frequency.
- Rework/inspection loops probabilities and times (if present).

**Validation**
- **Event log replay / calibration:** the simulation should reproduce historical distributions of WIP, lead time, utilization, and tardiness under the baseline dispatching rules.

**What to compare**
Baseline vs Strategy 1 vs Strategy 2 vs Strategy 3 (and combinations), using:
- On-time delivery %, weighted tardiness, 95th percentile tardiness
- Average/median/p95 lead time
- Average WIP and WIP variability
- Bottleneck utilization split: processing vs setup vs down
- Schedule stability metric (how often jobs are resequenced)

**Stress-test scenarios**
- High load (e.g., +15–30% releases)
- Breakdown surge (MTBF reduced)
- More hot jobs (higher priority-change frequency)
- Mix shift (more complex jobs)
- Operator shortage (reduced staffing calendars)
- Increased setup variability (worn tooling / learning curve changes)

Use **Monte Carlo** replications to capture randomness and produce confidence intervals.

---

### 5.2 Continuous monitoring + adaptation using ongoing process mining
Implement a closed-loop framework:

**1) Operational dashboards (near real-time)**
- Lead time and tardiness by family/priority/customer
- Queue time heatmaps by machine
- Setup time trends and transition-frequency charts
- Bottleneck state (queue length, projected workload, down risk)

**2) Automated conformance + drift detection**
- Detect when:
  - setup times shift (new tooling, new product families),
  - processing times drift (operator changes, wear),
  - routing variants change (engineering revisions),
  - breakdown rates worsen (maintenance issues).
Techniques: statistical process control on KPIs, plus **process drift detection** on directly-follows graphs / variant frequencies.

**3) Model retraining + policy tuning**
- Periodically retrain:
  - setup prediction model,
  - processing time predictors,
  - breakdown risk model (if used).
- Re-optimize dispatching weights (Strategy 1) using recent data (e.g., monthly).

**4) Governance**
- Use “freeze windows” and controlled overrides to keep schedule stability.
- Track override reasons (expedite, quality hold, customer change) as explicit events—so the mining remains truthful.

---

### Bottom line
Process mining turns the MES logs into (1) a factual reconstruction of job and machine behavior, (2) quantified evidence of bottlenecks, priority inversions, and setup-driven losses, and (3) calibrated predictive models. Those outputs directly enable advanced scheduling that is **setup-aware, bottleneck-aware, disruption-aware, and continuously learning**, which is what a high-mix job shop needs to reduce tardiness, WIP, and lead-time volatility without blindly adding capacity.