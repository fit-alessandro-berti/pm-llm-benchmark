# Comprehensive Process Mining Analysis for TechSolve Solutions' Resource Assignment Optimization

## Executive Summary

TechSolve Solutions faces significant operational challenges stemming from inefficient resource assignment practices within their IT service desk. This analysis presents a systematic, data-driven approach using process mining to diagnose the root causes of SLA breaches, excessive ticket reassignments, and skill mismatches. The proposed methodology will transform the event log data into actionable insights, enabling TechSolve to implement optimized resource assignment strategies that leverage agent skills effectively while balancing workload distribution.

---

## 1. Analyzing Resource Behavior and Assignment Patterns

### 1.1 Individual Agent and Tier Performance Metrics

To establish a comprehensive understanding of resource performance, I would extract and analyze the following metrics from the event log:

#### **Workload Distribution Metrics**

| Metric | Calculation Method | Purpose |
|--------|-------------------|---------|
| **Tickets Handled per Agent/Day** | Count(Case IDs) grouped by Resource, Date | Identify workload imbalances |
| **Active Work Time Ratio** | Sum(Work End - Work Start) / Total Shift Duration | Measure actual utilization |
| **Queue Time per Agent** | Time between Assignment and Work Start | Reveal capacity constraints |
| **Concurrent Ticket Load** | Count of tickets in "Work Start" state at any timestamp | Identify overloaded agents |

#### **Processing Time Analysis**

For each agent and tier, I would calculate:

```
Processing Time = Work End Timestamp - Work Start Timestamp
Handling Time = Resolution Timestamp - Assignment Timestamp
Touch Time = Sum of all work periods across tiers for a single ticket
```

**Tier-Specific Processing Time Distribution:**
- **L1 Average Processing Time**: Segmented by ticket category and priority
- **L2 Average Processing Time**: Segmented by required skill and complexity
- **L3 Average Processing Time**: For escalated complex issues

The event log allows us to compute these by filtering activities like "Work L1 Start/End," "Work L2 Start/End," and grouping by Agent ID.

#### **First-Call Resolution (FCR) Rate for L1**

```
FCR Rate = (Tickets resolved at L1 without escalation) / (Total L1 tickets) × 100

Where "resolved at L1" is identified by:
- Case contains "Work L1 End" followed by "Ticket Resolved" 
- Case does NOT contain "Escalate L2" or "Escalate L3"
```

I would segment FCR by:
- **Ticket Category**: Identifying which categories L1 can handle effectively
- **Ticket Priority**: Understanding if complexity correlates with priority
- **Individual L1 Agent**: Revealing training gaps or skill variations
- **Time of Day/Day of Week**: Detecting fatigue or staffing-related patterns

#### **Skill-Based Performance Analysis**

Using the "Required Skill" and "Agent Skills" attributes:

```
Skill Match Rate = (Assignments where Required Skill  Agent Skills) / (Total Assignments) × 100
```

For each agent, I would create a **skill utilization matrix**:

| Agent ID | Skill | Times Used | Avg Resolution Time | Success Rate |
|----------|-------|------------|--------------------|--------------| 
| B12 | App-CRM | 145 | 42 min | 94% |
| B12 | DB-SQL | 89 | 67 min | 91% |
| B12 | Basic-Troubleshoot | 23 | 15 min | 100% |

This reveals whether agents are being deployed optimally against their documented capabilities.

### 1.2 Process Mining Techniques for Assignment Pattern Discovery

#### **Resource Interaction Analysis**

Using the handover-of-work concept, I would construct a **resource interaction matrix** showing:
- Frequency of ticket transfers between specific agents
- Average delay during each handover
- Typical reasons for handovers (noted in the "Notes" field)

**Handover Analysis Query Logic:**
```
For each Case ID:
  Extract sequence of (Resource, Activity) pairs
  Identify transitions where Resource changes
  Calculate: Time_to_handover = Next_Activity_Timestamp - Previous_Activity_Timestamp
  Categorize: Same_tier vs Cross_tier handover
```

**Expected Output - Handover Frequency Matrix:**

| From  / To  | L1 Pool | L2 Pool | L3 Pool | Back to L1 |
|---------------|---------|---------|---------|------------|
| **L1 Pool** | 12% | 65% | 8% | - |
| **L2 Pool** | 3% | 25% | 15% | 5% |
| **L3 Pool** | 1% | 8% | 12% | 2% |

This matrix reveals patterns such as:
- High L2-to-L2 reassignment rate (25%) indicating skill mismatches at L2
- Unexpected back-escalations from L2 to L1 (5%) suggesting over-escalation

#### **Social Network Analysis (SNA)**

Constructing a social network based on ticket handovers:

**Network Construction:**
- **Nodes**: Individual agents or agent pools (L1, L2, L3)
- **Edges**: Directed connections representing ticket transfers
- **Edge Weights**: Frequency of transfers, average associated delay

**Key SNA Metrics:**

| Metric | Interpretation for TechSolve |
|--------|------------------------------|
| **Betweenness Centrality** | Agents who act as "gatekeepers" – potential bottlenecks |
| **In-Degree Centrality** | Agents receiving many tickets – overload candidates |
| **Out-Degree Centrality** | Agents frequently passing tickets – escalation-heavy agents |
| **Clustering Coefficient** | Informal sub-teams or silos in ticket handling |

**Application Example:**
If Agent B12 shows high betweenness centrality with high in-degree and high out-degree, this indicates B12 is a critical routing point but also a source of reassignments – potentially due to incorrect initial routing to B12 for non-matching skills.

#### **Role Discovery (Organizational Mining)**

Using unsupervised clustering on agent activity patterns:

**Input Features per Agent:**
- Distribution of activity types performed
- Distribution of ticket categories handled
- Average processing times
- Escalation/reassignment behavior

**Expected Output:**
The algorithm may discover roles that differ from the formal L1/L2/L3 structure:

| Discovered Role | Formal Tier | Characteristics |
|-----------------|-------------|-----------------|
| "Quick Resolver" | L1 | High FCR, short processing times |
| "Escalation Router" | L1 | Low FCR, primarily logs and escalates |
| "Generalist Specialist" | L2 | Handles broad categories, moderate times |
| "Deep Specialist" | L2/L3 | Handles specific skills, longer but thorough |
| "Firefighter" | L3 | Handles P1/P2, often reassigned tickets |

**Comparing Actual vs. Intended Assignment Logic:**

The intended logic (round-robin within tiers + manual escalation) would produce:
- Even ticket distribution within each tier
- Escalations based on clear skill requirements
- Sequential tier progression (L1L2L3)

Process mining reveals deviations:
- **Actual pattern**: Certain agents receive disproportionate volume (popularity bias)
- **Actual pattern**: Lateral reassignments within tiers occur frequently
- **Actual pattern**: Skip-escalations (L1L3 direct) for certain categories
- **Actual pattern**: Reverse escalations (L2L1) for misrouted tickets

### 1.3 Skill Utilization Analysis

#### **Skill Demand vs. Capacity Analysis**

Creating a time-series view of skill demand:

```
For each time window (e.g., hourly):
  Skill_Demand[skill] = Count of tickets requiring 'skill' in queue or active
  Skill_Capacity[skill] = Count of available agents with 'skill'
  Skill_Gap[skill] = Skill_Demand - Skill_Capacity
```

**Visualization: Skill Gap Heatmap**

| Hour | Networking-Firewall | App-CRM | Database-SQL | Security-IAM |
|------|---------------------|---------|--------------|--------------|
| 09:00 | +3 (shortage) | -1 | +1 | 0 |
| 10:00 | +5 (shortage) | +2 | -2 | +1 |
| 14:00 | +2 | +4 (shortage) | +1 | -1 |

This reveals peak demand periods for specific skills.

#### **Specialist Utilization Efficiency**

Calculating the percentage of specialist time spent on appropriately skilled tasks:

```
Specialist_Efficiency[agent] = 
  Sum(Time on tasks matching specialist skills) / 
  Sum(Total task time) × 100
```

**Example Findings:**

| Agent | Tier | Primary Skills | % Time on Skill-Matched Tasks | % Time on Basic Tasks |
|-------|------|----------------|-------------------------------|----------------------|
| B12 | L2 | App-CRM, DB-SQL | 68% | 32% |
| B08 | L2 | Networking-Firewall | 72% | 28% |
| C03 | L3 | Security-IAM | 55% | 45% |

Finding: L2/L3 specialists spending 28-45% of time on tasks that could be handled by L1 indicates significant resource waste and supports management's suspicion.

#### **Skill Mismatch Detection**

Identifying assignments where required skills don't match agent skills:

```
Mismatch_Rate = Count(Required_Skill  Agent_Skills) / Total_Assignments × 100

Mismatch_Impact = Avg(Processing_Time | Mismatch) - Avg(Processing_Time | Match)
```

Segmenting by downstream impact:
- **Mismatch  Reassignment**: Tickets where skill mismatch led to reassignment
- **Mismatch  Extended Time**: Tickets resolved despite mismatch but with longer duration
- **Mismatch  SLA Breach**: Direct correlation to SLA failures

---

## 2. Identifying Resource-Related Bottlenecks and Issues

### 2.1 Skill-Based Bottleneck Identification

#### **Bottleneck Detection Methodology**

Using queue analysis and flow time decomposition:

```
For each Required_Skill:
  Avg_Queue_Time = Mean(Work_Start - Assign_Timestamp) for tickets requiring skill
  Avg_Processing_Time = Mean(Work_End - Work_Start)
  Throughput = Tickets_Resolved / Time_Period
  Work_In_Progress = Avg concurrent tickets awaiting this skill
  
  Bottleneck_Score = (Queue_Time × WIP) / Throughput
```

**Expected Findings:**

| Required Skill | Avg Queue Time | Agent Count | Bottleneck Score | Assessment |
|----------------|----------------|-------------|------------------|------------|
| Networking-Firewall | 2.3 hours | 3 | 156 | **Critical Bottleneck** |
| App-CRM | 1.1 hours | 5 | 72 | Moderate constraint |
| Database-SQL | 45 min | 4 | 38 | Adequate |
| Security-IAM | 3.5 hours | 2 | 189 | **Critical Bottleneck** |
| Basic-Troubleshoot | 12 min | 15 | 8 | Well-resourced |

**Quantified Impact:**
- Tickets requiring "Networking-Firewall" experience average **2.3 hours queue delay** vs. process average of 45 minutes
- Security-IAM tickets wait **3.5 hours** due to only 2 agents possessing this skill
- Estimated **23% of P2 SLA breaches** are directly attributable to skill-based bottlenecks

### 2.2 Reassignment and Escalation Delay Analysis

#### **Reassignment Pattern Categorization**

Classifying reassignments by type and cause:

| Reassignment Type | Description | Detection Logic |
|-------------------|-------------|-----------------|
| **Skill Mismatch** | Assigned agent lacks required skill | Agent_Skills  Required_Skill =  |
| **Workload Balance** | Agent overloaded | Agent queue > threshold at assignment time |
| **Complexity Escalation** | Ticket more complex than anticipated | Same tier reassignment with updated Required_Skill |
| **Availability** | Agent unavailable | Assignment during agent off-hours or PTO |
| **Error Correction** | Wrong initial categorization | Required_Skill changes during case |

**Reassignment Impact Quantification:**

```
Delay_Per_Reassignment = Avg(New_Assignment_Time - Reassign_Activity_Time) + 
                         Avg(New_Work_Start - New_Assignment_Time)
```

**Expected Findings:**

| Reassignment Type | Frequency | Avg Delay Added | % Leading to SLA Breach |
|-------------------|-----------|-----------------|------------------------|
| Skill Mismatch | 34% | 1.8 hours | 42% |
| Workload Balance | 28% | 0.9 hours | 18% |
| Complexity Escalation | 22% | 2.4 hours | 38% |
| Availability | 12% | 1.2 hours | 25% |
| Error Correction | 4% | 3.1 hours | 56% |

**Critical Insight:** Skill mismatch reassignments (34% of all reassignments) add an average of 1.8 hours delay, with 42% of these tickets eventually breaching SLA.

#### **Escalation Appropriateness Analysis**

Evaluating whether escalations were necessary:

```
Unnecessary_Escalation = Cases where:
  - L1 escalated to L2
  - L2 resolved within 15 minutes using only Basic-Troubleshoot activities
  - Similar tickets (same category, priority) were resolved by L1 agents

Escalation_Necessity_Rate = 
  (Escalations that were appropriate) / (Total escalations) × 100
```

**Expected Finding:** 
- **28% of L1L2 escalations** appear unnecessary based on L2 resolution patterns
- These tickets could have been resolved by L1 with better training or documentation

### 2.3 Agent Performance Variance Analysis

#### **Identifying Overloaded vs. Underutilized Agents**

Using workload distribution analysis:

```
For each Agent:
  Daily_Ticket_Volume = Count of tickets worked per day
  Daily_Work_Hours = Sum of (Work_End - Work_Start) per day
  Utilization_Rate = Daily_Work_Hours / Scheduled_Hours
  
  Overload_Threshold = Mean + 1.5 × Std_Dev
  Underutilization_Threshold = Mean - 1.5 × Std_Dev
```

**Agent Workload Distribution (Example):**

| Agent Category | Agent Count | Avg Daily Tickets | Utilization Rate | Issues |
|----------------|-------------|-------------------|------------------|--------|
| **Overloaded** | 8 | 42 | 112% | Overtime, quality risks |
| **Balanced** | 25 | 28 | 85% | Optimal |
| **Underutilized** | 12 | 14 | 54% | Capacity waste |

**Statistical Measure:**
- **Gini Coefficient for Workload Distribution**: 0.38 (indicating moderate inequality)
- Target should be < 0.20 for balanced distribution

#### **Performance Outlier Detection**

Identifying agents with significant performance deviations:

| Metric | Top Performers (10%) | Average | Bottom Performers (10%) |
|--------|---------------------|---------|------------------------|
| Avg Resolution Time | 18 min | 34 min | 67 min |
| FCR Rate (L1) | 78% | 52% | 23% |
| Reassignment Rate | 5% | 18% | 41% |
| Customer Satisfaction | 4.6/5 | 4.1/5 | 3.4/5 |

### 2.4 SLA Breach Correlation Analysis

#### **Resource-SLA Correlation Matrix**

Calculating correlation between resource-related factors and SLA breaches:

```
For each Case:
  SLA_Breach = 1 if Resolution_Time > SLA_Target else 0
  
  Correlate with:
  - Number_of_Reassignments
  - Skill_Match (binary)
  - Initial_Tier_Correct (L1 for simple, L2/L3 for complex)
  - Agent_Workload_at_Assignment
  - Time_in_Queue
```

**Correlation Results:**

| Factor | Correlation with SLA Breach | Statistical Significance |
|--------|----------------------------|-------------------------|
| Number of Reassignments | 0.72 | p < 0.001 |
| Initial Skill Mismatch | 0.58 | p < 0.001 |
| Agent Overload at Assignment | 0.45 | p < 0.001 |
| Incorrect Initial Tier | 0.41 | p < 0.01 |
| Queue Time at Assignment | 0.38 | p < 0.01 |

**Key Quantified Impacts:**

- **67% of P2 SLA breaches** occur in cases with 2+ reassignments
- **Tickets with skill mismatch** are **3.2x more likely** to breach SLA
- **P3 tickets assigned to overloaded agents** have **45% SLA breach rate** vs. **12% average**

---

## 3. Root Cause Analysis for Assignment Inefficiencies

### 3.1 Systematic Root Cause Framework

Using an Ishikawa (fishbone) diagram approach, structured around process mining findings:

```
                            SLA Breaches & Reassignments
                                        |
    
    |               |               |               |               |
Assignment      Skill           Ticket          Process         Technology
  Rules       Management     Categorization      Design          Systems
    |               |               |               |               |
- Round-robin   - Incomplete   - Ambiguous      - Too many     - No real-time
  ignores        skill          categories       tiers           workload
  skills         profiles                                        visibility
                               - Requried       - Unclear
- No workload  - Outdated       skill not       escalation     - Manual
  balancing      skills         captured        criteria         dispatcher
                 data          early                             decisions
- Static                                       - L1 not
  assignment   - No            - User errors    empowered      - No skill-
  rules          proficiency    in web portal   sufficiently    matching
                 levels                                          engine
```

### 3.2 Detailed Root Cause Analysis

#### **Root Cause 1: Deficient Assignment Rules**

**Evidence from Process Mining:**
- Round-robin analysis shows equal distribution attempts regardless of ticket requirements
- Skill match rates of only 62% indicate assignment decisions don't consider agent capabilities
- No correlation between agent current workload and assignment probability

**Process Mining Detection Method:**
```
Assignment_Decision_Analysis:
  For each "Assign L1/L2" activity:
    Extract: Time_of_day, Agent_selected, Queue_size, Required_skill
    Compare: Agent_skills vs Required_skill
    Check: Agent_workload at assignment moment
    
  Finding: 78% of assignments show no skill-consideration pattern
```

#### **Root Cause 2: Inaccurate Skill Profiles**

**Evidence from Process Mining:**
- Agents successfully resolve tickets for skills not in their documented profile
- Agents fail/reassign tickets for skills in their documented profile
- Skill profile vs. actual performance discrepancy analysis:

```
For each Agent:
  Documented_Skills = Skills in Agent_Skills field
  Demonstrated_Skills = Skills of successfully resolved tickets (no reassignment)
  
  Profile_Accuracy = |Documented  Demonstrated| / |Documented  Demonstrated|
```

**Finding:** Average Profile Accuracy = 71%, indicating 29% of skill data is outdated or incorrect

#### **Root Cause 3: Poor Initial Categorization**

**Evidence from Process Mining:**
- 23% of tickets have Required_Skill changed during their lifecycle
- Keyword analysis of ticket descriptions vs. final category shows misclassification patterns
- Tickets from "Web Portal" channel have 35% higher reassignment rate than "Phone" channel (where agents can probe for details)

**Detection Method - Category Stability Analysis:**
```
For each Case:
  Initial_Required_Skill = First recorded Required_Skill
  Final_Required_Skill = Required_Skill at resolution
  
  Category_Stability = Cases where Initial == Final / Total Cases
  
Finding: Category_Stability = 77% (23% instability)
```

#### **Root Cause 4: Lack of Real-Time Visibility**

**Evidence from Process Mining:**
- Queue time analysis shows no correlation between queue length and assignment rate
- Assignments occur even when agents have 5+ tickets in progress
- Peak demand periods show increased SLA breaches despite available underutilized agents

**Detection Method:**
```
For each Assignment:
  Calculate: Agent_Active_Tickets at assignment time
  Calculate: Other_Agents_Active_Tickets
  
  If Agent_Active_Tickets >> Mean(Other_Agents_Active_Tickets):
    Flag as "Suboptimal_Assignment"
    
Finding: 34% of assignments are suboptimal given available alternatives
```

#### **Root Cause 5: L1 Training/Empowerment Gaps**

**Evidence from Process Mining:**
- High variance in FCR rates among L1 agents (23% to 78%)
- Similar tickets handled differently by different L1 agents (some resolve, others escalate)
- Top FCR performers handle same ticket types as low FCR performers

**Detection Method - Resolution Pattern Variance:**
```
For each Ticket_Category, Priority combination:
  Calculate: L1_Resolution_Rate per Agent
  Calculate: Standard_Deviation of rates
  
  If High_StdDev: Training/Knowledge gap indicated
  
Finding: StdDev for App-CRM/P3 tickets = 0.31 (very high variance)
```

### 3.3 Variant Analysis for Assignment Pattern Discovery

#### **Comparing Smooth vs. Problematic Cases**

**Case Classification:**
- **Smooth Cases**: Resolved with 0-1 assignments, no reassignments, within SLA
- **Problematic Cases**: 3+ assignments, 2+ reassignments, or SLA breach

**Variant Analysis Process:**

| Attribute | Smooth Cases (n=4,200) | Problematic Cases (n=1,800) | Delta |
|-----------|----------------------|---------------------------|-------|
| Avg Initial Skill Match | 89% | 43% | **-46%** |
| Avg Initial Agent Workload | 3.2 tickets | 6.8 tickets | **+3.6** |
| % Correct Initial Category | 91% | 58% | **-33%** |
| % Phone Channel | 45% | 28% | -17% |
| Avg L1 Time Before Escalation | 22 min | 8 min | **-14 min** |
| % During Peak Hours | 52% | 71% | **+19%** |

**Key Insights:**
1. **Skill match at initial assignment** is the strongest differentiator
2. **Agent workload at assignment** significantly impacts outcome
3. **Premature escalation** (only 8 min at L1) correlates with problems
4. **Peak hour assignments** are more problematic (capacity issues)

#### **Decision Mining for Escalation Decisions**

Building a decision tree to understand escalation patterns:

**Input Features:**
- Ticket_Category
- Ticket_Priority  
- L1_Agent_Skills
- L1_Processing_Time
- Ticket_Description_Keywords

**Target Variable:**
- Escalation_Appropriate (1 = L2/L3 resolved with specialized skill, 0 = Could have been L1)

**Decision Tree Output (Simplified):**

```
IF Ticket_Category = "Network" AND Priority = "P1/P2":
     92% appropriate escalation
    
ELIF Ticket_Category = "Software-App" AND L1_Time < 10 min:
     67% inappropriate escalation (premature)
    
ELIF Ticket_Category = "Access Management" AND Description contains "password reset":
     89% inappropriate escalation (L1 capable)
    
ELIF Required_Skill = "Database-SQL" AND Priority = "P3/P4":
     78% appropriate escalation
```

**Actionable Insight:** 31% of escalations are premature or unnecessary, driven by:
- L1 agents not attempting resolution for certain categories
- Lack of knowledge base utilization
- Risk-averse behavior (escalate to avoid mistakes)

---

## 4. Developing Data-Driven Resource Assignment Strategies

### 4.1 Strategy 1: Intelligent Skill-Based Routing with Proficiency Weighting

#### **Issue Addressed:**
- Skill mismatch causing 34% of reassignments
- Specialists spending time on mismatched tasks
- Current round-robin ignoring skill requirements

#### **Strategy Description:**

Implement a **skill-matching algorithm** that:
1. Matches ticket Required_Skill to available agents with that skill
2. Weights agents by proficiency level (derived from historical performance)
3. Falls back to nearest-skill match if no exact match available

**Algorithm Logic:**

```python
def assign_ticket(ticket, available_agents):
    required_skill = ticket.required_skill
    
    # Filter agents with required skill
    matching_agents = [a for a in available_agents 
                       if required_skill in a.skills]
    
    if matching_agents:
        # Score by proficiency (historical resolution rate × speed)
        for agent in matching_agents:
            agent.score = (agent.resolution_rate[required_skill] * 0.6 + 
                          (1/agent.avg_time[required_skill]) * 0.4)
        
        # Select top scorer with availability
        return select_best(matching_agents, consider_workload=True)
    
    else:
        # Fallback: Find agent with most similar skill
        similarity_scores = calculate_skill_similarity(required_skill, 
                                                       available_agents)
        return select_best(similarity_scores, consider_workload=True)
```

**Proficiency Score Calculation:**
```
Proficiency[agent, skill] = 
    0.4 × Resolution_Rate[agent, skill] +
    0.3 × (1 / Normalized_Avg_Time[agent, skill]) +
    0.2 × (1 - Reassignment_Rate[agent, skill]) +
    0.1 × Customer_Satisfaction[agent, skill]
```

#### **Process Mining Insights Leveraged:**
- Historical resolution rates per agent-skill combination
- Processing time distributions per agent-skill
- Reassignment patterns revealing actual vs. documented skill proficiency

#### **Data Required:**
- Agent skill profiles (enhanced with proficiency levels)
- Historical performance metrics per agent-skill combination
- Real-time agent availability status
- Ticket required skill (accurately captured)

#### **Expected Benefits:**
| Metric | Current State | Expected After | Improvement |
|--------|--------------|----------------|-------------|
| Skill Match Rate | 62% | 91% | +47% |
| Reassignments (Skill Mismatch) | 34% of total | 8% | -76% |
| Avg Resolution Time | 2.4 hours | 1.6 hours | -33% |
| Specialist Efficiency | 68% | 89% | +31% |

### 4.2 Strategy 2: Dynamic Workload-Aware Assignment Algorithm

#### **Issue Addressed:**
- Uneven workload distribution (Gini = 0.38)
- Overloaded agents causing delays
- 34% of assignments suboptimal given available alternatives

#### **Strategy Description:**

Implement a **real-time workload balancing system** that:
1. Tracks current work-in-progress for each agent
2. Considers queue depth and estimated completion times
3. Distributes tickets to optimize overall throughput

**Algorithm Logic:**

```python
def calculate_agent_availability_score(agent, ticket):
    # Current workload factors
    active_tickets = agent.current_active_tickets
    queue_depth = agent.queue_depth
    avg_completion_time = agent.avg_completion_time[ticket.category]
    
    # Estimated time to start working on new ticket
    estimated_queue_time = queue_depth * avg_completion_time
    
    # Capacity factor (based on historical sustainable load)
    capacity_ratio = active_tickets / agent.optimal_capacity
    
    # Workload score (lower is better - more available)
    workload_score = (
        0.4 × capacity_ratio +
        0.3 × normalized(estimated_queue_time) +
        0.2 × normalized(active_tickets) +
        0.1 × (1 if agent.on_break else 0)
    )
    
    return 1 - workload_score  # Availability score (higher is better)

def assign_ticket(ticket, available_agents):
    # Get skill-matched agents
    skilled_agents = filter_by_skill(available_agents, ticket.required_skill)
    
    # Score by combination of proficiency and availability
    for agent in skilled_agents:
        agent.final_score = (
            0.5 × agent.proficiency_score[ticket.required_skill] +
            0.5 × calculate_agent_availability_score(agent, ticket)
        )
    
    # Apply priority weighting (P1/P2 tickets go to more available agents)
    if ticket.priority in ['P1', 'P2']:
        for agent in skilled_agents:
            agent.final_score *= (1 + calculate_agent_availability_score(agent, ticket))
    
    return max(skilled_agents, key=lambda a: a.final_score)
```

#### **Process Mining Insights Leveraged:**
- Historical agent capacity patterns (optimal concurrent ticket load)
- Processing time distributions for queue estimation
- Peak demand period identification for proactive load management

#### **Data Required:**
- Real-time agent status (active tickets, queue depth)
- Historical processing time distributions
- Agent shift schedules and break patterns
- Ticket priority for weighted routing

#### **Expected Benefits:**
| Metric | Current State | Expected After | Improvement |
|--------|--------------|----------------|-------------|
| Workload Gini Coefficient | 0.38 | 0.15 | -61% |
| Overloaded Agent Count | 8 | 2 | -75% |
| Avg Queue Time | 48 min | 22 min | -54% |
| P2 SLA Compliance | 73% | 89% | +22% |

### 4.3 Strategy 3: Predictive Ticket Classification and Skill Anticipation

#### **Issue Addressed:**
- 23% of tickets have incorrect initial categorization
- Required skill often changes during case lifecycle
- Premature or inappropriate escalations

#### **Strategy Description:**

Implement a **machine learning-based ticket classifier** that:
1. Analyzes ticket description text to predict category and required skill
2. Estimates ticket complexity for tier assignment
3. Provides confidence scores for human override when needed

**ML Model Architecture:**

```
Input Features:
 Text Features (from ticket description)
    TF-IDF vectors
    Named entity recognition (system names, error codes)
    Sentiment indicators (frustration level  complexity proxy)
 Structured Features
    Reported channel (phone/email/portal)
    User history (past ticket categories)
    User department (affects typical issues)
    Time of report (certain issues peak at certain times)

Output Predictions:
 Category (multi-class)
 Required_Skill (multi-class)
 Predicted_Complexity (L1/L2/L3 appropriate)
 Confidence_Scores (for each prediction)
```

**Training Data from Process Mining:**
```
For each historical case:
  Input: Initial_Description, Channel, User_Info, Time
  Output: Final_Category, Final_Required_Skill, Resolution_Tier
  
  (Using FINAL values captures the "correct" categorization 
   after human corrections during ticket lifecycle)
```

**Integration with Assignment:**

```python
def classify_and_route(ticket):
    # ML prediction
    predictions = ml_model.predict(ticket)
    
    category = predictions.category
    required_skill = predictions.required_skill
    complexity = predictions.complexity
    confidence = predictions.confidence
    
    # High confidence: auto-route
    if confidence > 0.85:
        ticket.category = category
        ticket.required_skill = required_skill
        target_tier = complexity
    
    # Medium confidence: route but flag for review
    elif confidence > 0.65:
        ticket.category = category
        ticket.required_skill = required_skill
        ticket.flag_for_review = True
        target_tier = complexity
    
    # Low confidence: human classification
    else:
        route_to_dispatcher_for_classification(ticket)
        return
    
    # Apply skill-based and workload-aware assignment
    assign_to_tier(ticket, target_tier)
```

#### **Process Mining Insights Leveraged:**
- Historical ticket descriptions and final (corrected) categories
- Patterns of category changes during ticket lifecycle
- Correlation between description keywords and resolution paths
- Complexity indicators from resolution patterns

#### **Data Required:**
- Historical ticket descriptions (text data)
- Category and skill change logs
- Resolution tier for each ticket
- User/department information

#### **Expected Benefits:**
| Metric | Current State | Expected After | Improvement |
|--------|--------------|----------------|-------------|
| Initial Category Accuracy | 77% | 93% | +21% |
| Category Changes per Ticket | 0.28 | 0.08 | -71% |
| Inappropriate Escalation Rate | 31% | 12% | -61% |
| Avg Time to Correct Assignment | 1.2 hours | 5 min | -93% |

### 4.4 Strategy 4: Enhanced Escalation Criteria with L1 Empowerment

#### **Issue Addressed:**
- 28% of L1L2 escalations appear unnecessary
- High variance in L1 FCR rates (23%-78%)
- Specialists handling tasks L1 could resolve

#### **Strategy Description:**

Implement **data-driven escalation guidelines** and **L1 capability expansion**:

1. **Escalation Decision Support System:**
   - Clear criteria based on ticket attributes
   - Recommended resolution paths from similar historical tickets
   - Required L1 time before escalation (prevent premature escalation)

2. **L1 Resolution Knowledge Base:**
   - Curated from successful L1 resolutions
   - Searchable by symptoms and categories
   - Confidence-scored suggestions

**Escalation Criteria (Data-Derived):**

| Condition | Action | Rationale |
|-----------|--------|-----------|
| Category = "Password Reset" | L1 MUST attempt | 94% L1 success rate historically |
| Priority = P1 | Immediate L2 escalation allowed | Time-critical |
| Category = "Network" + Description contains "firewall rule" | Direct L2 routing | 89% require L2 specialist |
| L1 time < 10 min + No resolution attempt | Block escalation | Premature escalation pattern |
| Similar ticket resolved by L1 in past 30 days | Suggest L1 resolution path | Learnable pattern |

**Process Mining Detection of Escalation Patterns:**

```
Identify_Unnecessary_Escalations:
  For each L1L2 escalation:
    If L2_Resolution_Time < 15 minutes AND
       L2_Activities contain only basic troubleshooting AND
       Similar_Tickets resolved by L1:
          Mark as "Potentially Unnecessary"
          Add to L1 Training Queue
```

#### **Process Mining Insights Leveraged:**
- L1 resolution patterns for each category
- Characteristics of tickets successfully resolved by L1 vs. requiring L2
- Agent-level FCR variations identifying training opportunities
- Time-to-escalation patterns revealing premature escalations

#### **Data Required:**
- Historical escalation outcomes
- L1 and L2 resolution patterns by category
- Similar ticket matching criteria
- Agent skill and training records

#### **Expected Benefits:**
| Metric | Current State | Expected After | Improvement |
|--------|--------------|----------------|-------------|
| L1 FCR Rate | 52% | 68% | +31% |
| Unnecessary Escalations | 28% | 10% | -64% |
| L2 Time on Basic Tasks | 28% | 12% | -57% |
| Avg Resolution Time (L1-resolvable) | 1.8 hours | 0.6 hours | -67% |

### 4.5 Strategy 5: Dynamic Resource Reallocation Based on Demand Patterns

#### **Issue Addressed:**
- Peak demand periods cause SLA breaches
- Skill bottlenecks during specific timeframes
- Static tier assignments don't adapt to demand

#### **Strategy Description:**

Implement **demand-driven dynamic agent allocation**:

1. **Demand Forecasting:** Predict ticket volume and skill requirements by hour/day
2. **Flexible Agent Pools:** Allow L1 agents with additional skills to temporarily support L2
3. **Proactive Scheduling:** Adjust shift patterns based on predicted demand

**Dynamic Allocation Algorithm:**

```python
def rebalance_resources(current_time, forecast_window=2_hours):
    # Get demand forecast
    predicted_demand = forecast_model.predict(
        time=current_time, 
        window=forecast_window
    )
    
    # Calculate skill gaps
    for skill in skill_list:
        predicted_volume = predicted_demand[skill]
        current_capacity = sum(1 for a in available_agents if skill in a.skills)
        
        skill_gap[skill] = predicted_volume - (current_capacity * throughput_rate[skill])
    
    # Identify reallocation opportunities
    for skill in sorted(skill_gap, key=lambda s: skill_gap[s], reverse=True):
        if skill_gap[skill] > 0:  # Shortage
            # Find agents with secondary skills who are underutilized
            donors = find_underutilized_agents_with_secondary_skill(skill)
            for donor in donors:
                if donor.primary_skill_demand < threshold:
                    reallocate(donor, to_skill=skill, duration=forecast_window)
                    skill_gap[skill] -= 1
                    if skill_gap[skill] <= 0:
                        break

def forecast_model(historical_data, features):
    # Time series features
    hour_of_day = current_time.hour
    day_of_week = current_time.weekday()
    
    # Seasonal patterns
    week_of_month = current_time.day // 7
    
    # Use historical patterns + recent trend
    return predict_volume(features)
```

**Cross-Training Implementation:**
- Identify L1 agents with aptitude for specific L2 skills
- Provide targeted training for high-demand L2 skills
- Create "flex" agent pool capable of L1 and limited L2 work

#### **Process Mining Insights Leveraged:**
- Hourly/daily demand patterns by skill
- Seasonal and cyclical trends
- Queue buildup patterns indicating capacity constraints
- Agent performance on cross-tier tasks

#### **Data Required:**
- Historical ticket volume time series
- Skill requirement distributions over time
- Agent cross-training capabilities
- Real-time queue metrics

#### **Expected Benefits:**
| Metric | Current State | Expected After | Improvement |
|--------|--------------|----------------|-------------|
| Peak Hour SLA Compliance | 61% | 82% | +34% |
| Skill Bottleneck Duration | 3.2 hours/day avg | 0.8 hours/day | -75% |
| Agent Utilization Variance | 35% coefficient of variation | 18% | -49% |
| Response Time (Peak Hours) | 45 min | 22 min | -51% |

---

## 5. Simulation, Implementation, and Monitoring

### 5.1 Business Process Simulation for Strategy Evaluation

#### **Simulation Model Construction**

Using the mined process models and resource characteristics to build a discrete-event simulation:

**Model Components:**

```
1. PROCESS MODEL (from discovery):
   - Activity sequence patterns
   - Transition probabilities
   - Case routing logic
   
2. RESOURCE MODEL (from organizational mining):
   - Agent pools by tier/skill
   - Processing time distributions per agent-skill-category
   - Availability schedules
   
3. ARRIVAL MODEL (from case analysis):
   - Ticket arrival rate (time-varying Poisson process)
   - Ticket attribute distributions (category, priority, skill)
   
4. ASSIGNMENT MODEL (configurable):
   - Current: Round-robin + manual escalation
   - Proposed: Skill-based + workload-aware + predictive
```

**Simulation Scenarios:**

| Scenario | Assignment Strategy | Description |
|----------|---------------------|-------------|
| Baseline | Current (round-robin) | Calibrated to match actual performance |
| Strategy 1 | Skill-based routing | Add proficiency-weighted skill matching |
| Strategy 2 | + Workload balancing | Add real-time workload consideration |
| Strategy 3 | + Predictive classification | Add ML-based initial routing |
| Combined | All strategies | Full implementation |

**Calibration and Validation:**
```
Calibration_Metrics:
  - Match simulated throughput to actual throughput (±5%)
  - Match simulated processing time distributions
  - Match simulated SLA rates to actual rates (±3%)
  
Validation:
  - Use 70% of historical data for calibration
  - Validate on remaining 30%
  - Verify prediction accuracy before scenario testing
```

#### **Expected Simulation Outputs:**

| Metric | Baseline | Strategy 1 | Strategy 2 | Strategy 3 | Combined |
|--------|----------|------------|------------|------------|----------|
| Avg Resolution Time | 2.4 hr | 1.9 hr | 1.7 hr | 1.8 hr | 1.4 hr |
| P2 SLA Compliance | 73% | 82% | 86% | 80% | 92% |
| Reassignment Rate | 18% | 9% | 8% | 11% | 5% |
| Workload Gini | 0.38 | 0.32 | 0.16 | 0.35 | 0.14 |
| Specialist Efficiency | 68% | 85% | 87% | 72% | 91% |

**Sensitivity Analysis:**
- Test strategy performance under demand surges (120%, 150% of normal)
- Test with reduced staff (simulate absences)
- Test with varying ticket complexity distributions

### 5.2 Implementation Roadmap

#### **Phase 1: Foundation (Weeks 1-4)**
- Deploy real-time agent status tracking
- Implement enhanced data capture for skill utilization
- Update agent skill profiles based on historical performance analysis
- Establish baseline metrics

#### **Phase 2: Skill-Based Routing (Weeks 5-8)**
- Implement skill-matching algorithm
- Deploy proficiency scoring system
- Train dispatchers on new system
- Pilot with subset of ticket categories
- Monitor and tune matching logic

#### **Phase 3: Workload Balancing (Weeks 9-12)**
- Deploy real-time workload dashboards
- Implement workload-aware assignment
- Integrate with skill-based routing
- Full rollout with monitoring

#### **Phase 4: Predictive Classification (Weeks 13-18)**
- Train ML model on historical data
- Deploy classification API
- Implement confidence-based routing
- Continuous model improvement cycle

#### **Phase 5: Dynamic Reallocation (Weeks 19-24)**
- Deploy demand forecasting
- Implement flex agent pool
- Cross-training program launch
- Full integration and optimization

### 5.3 Continuous Monitoring Framework

#### **Process Mining Dashboard Architecture**

```

                    RESOURCE PERFORMANCE DASHBOARD                    

                                                                      
           
   SLA COMPLIANCE      REASSIGNMENT      SKILL UTILIZATION     
                           RATE                               
     P1: 98%                            Match: 91%         
     P2: 89%           5.2%           Specialist          
     P3: 94%          Target: <8%       Efficiency: 89%     
     P4: 97%                                                
           
                                                                      
     
             WORKLOAD DISTRIBUTION (LIVE)                          
    Agent Pool    Active  Queue  Utilization                      
    L1 Pool (15)    42      8       84%                
    L2 Pool (12)    28      5       82%                
    L3 Pool (5)     11      2       78%                
     
                                                                      
      
   SKILL GAPS                AGENT PERFORMANCE VARIANCE         
   (Current Hour)                                                
                       FCR Rate     [===========|===]            
   Network-FW: +2                    min   avg   max             
   App-CRM: -1         Resolution   [====|========]              
   Security: +3                      fast   avg   slow           
   DB-SQL: 0                                                     
      
                                                                      

```

#### **Key Resource-Related KPIs**

| KPI Category | Specific Metric | Target | Alert Threshold |
|--------------|-----------------|--------|-----------------|
| **Assignment Quality** | Skill Match Rate | >90% | <85% |
| | First-Assignment Resolution | >75% | <65% |
| | Avg Assignments per Ticket | <1.3 | >1.5 |
| **Workload Balance** | Gini Coefficient | <0.20 | >0.30 |
| | Overloaded Agent Count | 0 | >3 |
| | Utilization Range | <20% spread | >35% spread |
| **Skill Utilization** | Specialist on Matched Tasks | >85% | <75% |
| | Skill Gap Duration | <30 min/day | >1 hr/day |
| **Escalation Health** | L1 FCR Rate | >65% | <55% |
| | Unnecessary Escalation Rate | <12% | >20% |
| | Avg L1 Time Before Escalation | >15 min | <8 min |
| **SLA Impact** | P2 Compliance | >88% | <80% |
| | P3 Compliance | >92% | <85% |
| | Reassignment-caused Breaches | <10% of breaches | >25% |

#### **Process Views for Continuous Analysis**

**1. Resource Interaction Network (Weekly Update)**
- Monitor for emerging handover hotspots
- Track changes in team collaboration patterns
- Identify new bottleneck agents

**2. Performance Trend Analysis (Daily)**
```
Track rolling averages:
- 7-day SLA compliance trend
- 7-day reassignment rate trend
- Agent performance trajectories
```

**3. Conformance Checking (Continuous)**
```
Compare actual assignment patterns to designed logic:
- Skill matching conformance
- Workload balancing conformance
- Escalation criteria adherence

Flag deviations > 5% from expected patterns
```

**4. Variant Analysis (Weekly)**
- Track distribution of smooth vs. problematic cases
- Identify new patterns in problematic cases
- Adjust strategies based on emerging patterns

#### **Alerting and Intervention Framework**

```
Alert Level 1 (Informational):
  - Metric deviation 5-10% from target
  - Action: Dashboard highlight, weekly review

Alert Level 2 (Warning):
  - Metric deviation 10-20% from target
  - Action: Daily review, investigate root cause

Alert Level 3 (Critical):
  - Metric deviation >20% from target
  - Action: Immediate investigation, potential rollback

Example Alert Rules:
  IF Skill_Match_Rate < 85% for 2 consecutive hours:
     Alert Level 2: "Skill matching degraded - verify skill profile updates"
    
  IF Reassignment_Rate > 15% for 1 day:
     Alert Level 2: "Reassignment spike - analyze recent assignment decisions"
    
  IF P2_SLA_Compliance < 75% for 4 hours:
     Alert Level 3: "P2 SLA Critical - immediate capacity review required"
```

---

## Conclusion

This comprehensive process mining analysis provides TechSolve Solutions with a systematic, data-driven approach to diagnosing and resolving their resource assignment challenges. By implementing the proposed strategies—skill-based routing, workload-aware assignment, predictive classification, enhanced escalation criteria, and dynamic resource reallocation—TechSolve can expect significant improvements:

**Projected Overall Impact:**

| Challenge Area | Current State | Expected Future State | Improvement |
|---------------|--------------|----------------------|-------------|
| P2 SLA Compliance | 73% | 92% | +26% |
| Reassignment Rate | 18% | 5% | -72% |
| L1 FCR Rate | 52% | 68% | +31% |
| Specialist Efficiency | 68% | 91% | +34% |
| Workload Balance (Gini) | 0.38 | 0.14 | -63% |
| Avg Resolution Time | 2.4 hours | 1.4 hours | -42% |

The success of these strategies depends on:
1. **Data quality**: Accurate skill profiles and ticket categorization
2. **Technology investment**: Real-time tracking and ML infrastructure
3. **Change management**: Training agents and dispatchers on new processes
4. **Continuous improvement**: Ongoing process mining monitoring and optimization

By treating resource assignment as a data-driven optimization problem rather than a static rule set, TechSolve can transform their service desk into a more efficient, responsive, and strategically aligned operation.