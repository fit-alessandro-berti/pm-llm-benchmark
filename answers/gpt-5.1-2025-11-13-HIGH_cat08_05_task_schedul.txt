## 1. Analyzing Historical Scheduling Performance and Dynamics

### 1.1. Event-log preparation and reconstruction of the actual flow

From the MES event logs, we construct several views:

- **Case notion:**  
  - Primary: `Case ID = Job ID` (JOB-7001, etc.).  
  - Secondary: `Job ID + Operation Index` for operation-level analyses (Cutting #1, Milling #2, etc.).

- **Activities (events):**
  - Job-level: `Job Released`, `Job Completed`, `Priority Change`, `Order Canceled`.
  - Operation-level: `Queue Entry (Machine)`, `Setup Start/End`, `Task Start/End`, `Rework`, `Inspection`, etc.
  - Resource-level: `Breakdown Start/End`, `Preventive Maintenance`, `Shift Start/End`.

- **Resources and attributes:**
  - `Resource`: Machine ID; also Operator ID where relevant.
  - Job attributes: order priority, due date, part family, material, quantity, processing route, etc.
  - Planned vs actual durations.

Using process mining tools (e.g., ProM / PM4Py / Celonis):

- **Job-level process model:**  
  - Use *Inductive Miner* or *Heuristic Miner* on job-level events (release  sequence of operations  completion).  
  - This reveals actual routings, alternative paths, loops (e.g., rework), optional steps (e.g., extra inspection) and frequency of each path.

- **Machine-level execution sequences:**  
  - Filter log per machine (e.g., CUT-01) and sort events by timestamp.  
  - You get an explicit timeline:  
    `… Task End(JOB-6998)  Setup Start(JOB-7001)  Setup End  Task Start(JOB-7001)  …`  
  - This is crucial for sequence-dependent setup analysis and utilization.

- **Timestamp-based performance overlays:**  
  - On the discovered model, overlay throughput times, waiting times, etc., using “performance mining” plugins.

This reconstruction gives a **true, as-executed** picture of flows and resource behavior, not what was planned.

---

### 1.2. Key metrics via process mining

#### 1.2.1. Job flow times, lead times, makespan

For each job (case):

- **Flow time / lead time:**  
  - `Lead Time(job) = timestamp(Job Completed) – timestamp(Job Released)`.  
  - If completion events are not explicit, approximate completion as last operation’s `Task End`.

- **Value-add time vs waiting time:**  
  - Sum actual `Task Duration (Actual)` + `Setup Duration (Actual)` across all operations of that job.  
  - `Waiting Time(job) = Lead Time – (Sum Processing + Sum Setup)`.

- **Makespan:**  
  - Per production day/period: `max completion time – min start time`.  
  - Per job family or large batch (if jobs belong to same customer order).

Process mining tools provide:

- Histograms and distributions of lead times, per:
  - Product family / routing variant,
  - Customer priority,
  - Due date tightness (due date – release date).

- Performance comparison:
  - On-time jobs vs late jobs: show difference in lead-time distribution and where delays accumulate.

---

#### 1.2.2. Task waiting (queue) times

From operation-level events for each job:

- **Waiting time at machine m for job j:**
  - `Queue Wait(m, j) = timestamp(Setup Start or Task Start) – timestamp(Queue Entry at m)`.

Aggregate per machine/work center:

- Average and percentile waiting times.
- Distribution per priority level, job family.
- Time-series of mean queue times per shift/day.

On the process map, use **performance overlays** to color each machine node by average waiting time and each edge by average transit delay between machines.

---

#### 1.2.3. Machine and operator utilization

For each machine m:

1. Sort all events (setup start/end, task start/end, breakdown, maintenance) by time.
2. Build a **state timeline**:
   - Busy-processing: between `Task Start` and `Task End`.
   - Busy-setup: between `Setup Start` and `Setup End`.
   - Down: between `Breakdown Start` and `Breakdown End`, or scheduled maintenance.
   - Idle (available but not processing): time not in the above states during scheduled working hours.

Compute:

- `Utilization_processing(m) = Processing_time / Available_time`
- `Utilization_setup(m) = Setup_time / Available_time`
- `Utilization_total(m) = (Processing + Setup) / Available_time`
- `Downtime_ratio(m) = Down_time / Available_time`
- `Idle_ratio(m) = 1 – Utilization_total – Downtime_ratio`

Do similar for operators where allocation is explicit.

Use:

- **Resource utilization charts**: per machine, shift, product family.
- **Resource Gantt charts / timelines**: visually show long setups, idle gaps, breakdowns.

---

#### 1.2.4. Sequence-dependent setup times

For each machine m:

1. Sort events by time.  
2. Consider every pair of consecutive processing jobs (i, j) on that machine.  
3. Identify the setup interval between them:
   - Setup from ij = `Setup End(job j) – Setup Start(job j)` where job j is next after job i.
4. Map job-level to **family-level** (or setup group) using attributes:
   - E.g., material, thickness range, part family, heat treatment group.

Build a **sequence-dependent setup matrix** per machine:

- For each pair of families (F_prev, F_next):
  - `S(F_prev, F_next) = average setup duration`  
    and distribution (mean, variance, percentiles).
- Identify clusters of families where transitions are cheap vs expensive.

Process mining support:

- Use **transition performance analysis**: treat “(Job of F_prev)  Setup(F_next)” as a transition; overlay average setup times.
- Use clustering to infer families if not predefined: group jobs so that intra-group setups are short and inter-group setups long.

This matrix will later drive sequencing and batching strategies.

---

#### 1.2.5. Schedule adherence, lateness, tardiness

From `Order Due Date` in the log:

For each job:

- Completion time `C_j` (job completion).
- Due date `D_j`.
- **Lateness:** `L_j = C_j – D_j` (can be positive or negative).
- **Tardiness:** `T_j = max(0, L_j)`.
- **Earliness:** `E_j = max(0, D_j – C_j)`.

KPIs:

- % jobs on-time (`C_j  D_j`).
- Average tardiness, median tardiness, 95th percentile.
- Weighted tardiness (e.g., weighted by order priority or revenue).

Process mining:

- Color variants by average tardiness; compare:
  - Variant (routing) A vs B on tardiness.
  - High-priority vs normal-priority job flows.
- Dotted charts: show completion times relative to due dates (markers above/below a due-date baseline).

---

#### 1.2.6. Impact of disruptions (breakdowns, hot jobs)

**Breakdowns:**

- Identify breakdown intervals per machine: [Breakdown Start, Breakdown End].
- Tag each job operation that:
  - Was in queue on that machine when breakdown started,
  - Or arrived during downtime,
  - Or was processed immediately before/after breakdown.

Compare KPIs for operations “affected by breakdown” vs “unaffected”:

- Additional waiting time.
- Resulting job tardiness.
- Spill-over effects on downstream machines (later start, queue spikes).

**Hot jobs / priority changes:**

- For jobs with `Priority Change` or priority = High:
  - Compare their waiting/processing times vs normal jobs.
  - Check if they caused surge in tardiness for nearby jobs processed on the same machines.

Process mining actions:

- **Variant analysis**:
  - Variant “with breakdown exposure” vs “without” for same routing.  
  - Variant “urgent priority” vs “normal” – see whether urgents truly flow faster.
- **Temporal analysis**:
  - Dotted charts to see spikes in WIP and waiting times around disruption timestamps.

This quantifies how much of tardiness and variability is attributable to disruptions vs normal variability and scheduling.

---

## 2. Diagnosing Scheduling Pathologies

Using the above metrics and process mining visualizations, we can identify specific inefficiencies.

### 2.1. Bottleneck resources

Evidence via process mining:

- Very high **processing utilization** (e.g., > 85–90%) on specific machines (e.g., certain CNCs, heat treatment).
- Long and persistent queues: high average waiting time and large WIP in front of those machines.
- Performance overlay: those nodes are red (long delays) on the process map.
- Critical path analysis: in many jobs, the longest single-operation waiting time is at the same machine.

Quantification:

- Measure contribution of each machine to total job lead time:
  - For each job, decompose lead time into waiting per machine; aggregate across jobs.
- Ranking: “Top 3 machines contributing to total waiting time”  systemic bottlenecks.

---

### 2.2. Poor prioritization / high-priority jobs not protected

Indicators:

- High-priority jobs frequently **late** or with similar mean waiting times as low-priority jobs.
- On a machine’s timeline, low-priority jobs often overtake (start earlier than) high-priority jobs that are already in the queue.

Process-mining-based evidence:

- **Overtaking analysis** on queues:
  - For each queue episode (set of jobs waiting at time t), compare actual start order vs ideal priority order.
  - Count priority violations (low-priority processed before waiting higher-priority job).
- Compare “average waiting time by priority class”:
  - If high-priority waiting time is not significantly lower (or even higher)  poor prioritization.

---

### 2.3. Suboptimal sequencing inflating setup times

Evidence:

- For key machines, **setup_time / (setup + processing)** is high, e.g., > 20–30%.
- Sequence-dependent setup matrix shows:
  - Many realized transitions are from “expensive pairing” families, although alternative sequences with much smaller total setups would be possible given the same job mix.

Process-mining analyses:

- For sample days on a bottleneck machine:
  - Compute total observed setup time.
  - Using the same set of jobs and release times, compute **heuristic “good” sequences** (e.g., group same families or nearest-neighbor by setup time).
  - Compare actual vs heuristic total setup; large gaps reveal improvement potential.
- Timeline visualization:
  - Many short alternating families (A–B–A–B) vs long “campaigns” (A–A–A–B–B) suggests unnecessary changeovers.

---

### 2.4. Starvation of downstream resources

Evidence:

- Downstream machines show periods of **idle** time while upstream predecessors have long queues and high utilization.
- Jobs spend long time waiting upstream even though downstream capacity is available.

Process-mining tools:

- Overlay upstream and downstream utilization/queue-time trends across time.
- Correlate:
  - When upstream is backed up, does downstream often idle due to unbalanced release or inefficient routing?

Example pathology:

- Critical cutting machine is overloaded; milling machines are sometimes idle:
  - Jobs queue for cutting; once cut, they rush into milling causing bursts; between bursts, milling may starve. This indicates poor **release and synchronization** logic.

---

### 2.5. Bullwhip in WIP levels due to scheduling variability

Evidence:

- Time-series of WIP at each stage shows:
  - Large oscillations in WIP at certain machines relative to stable or smoother arrival patterns.
- A lot of jobs are pushed quickly through early operations then sit waiting in front of bottlenecks.

Process-mining approach:

- Compute WIP(t) for each work center:
  - Count jobs in “in system but not yet completed this operation” state at each time.
- Visualize WIP heatmaps across time and stations; show amplification of variability downstream.

This often traces back to local dispatch rules and uncontrolled **order release**, not underlying demand variability.

---

### 2.6. Resource contention and contention periods

- Identify **contention windows** where many jobs simultaneously need the same bottleneck machine(s).
- Compare the subsequent tardiness of jobs that passed through these windows vs those processed in calmer periods.

Process mining:

- Bottleneck analyzer / resource contention graphs:
  - Show when multiple parallel routes converge on one machine.
- Variant analysis:
  - Jobs that hit peak contention windows vs off-peak.

---

## 3. Root Cause Analysis of Scheduling Ineffectiveness

### 3.1. Limitations of current static dispatching rules

Current: FCFS + simple EDD applied locally.

Root problems:

- **Myopic:** Each work center optimizes locally without regard to downstream queues or global due-date risk.
- **Setup-blind:** FCFS/EDD ignore sequence-dependent setups entirely, causing:
  - Excessive changeovers,
  - Fragmentation of ‘natural’ families.
- **Disruption-blind:** Rules are not re-optimized after breakdowns or hot-job arrivals.

Process mining evidence:

- Frequent priority violations and high tardiness for urgent jobs.
- High proportion of time in setup on bottleneck machines.
- Similar scheduling patterns and pathologies across long periods  suggests static, unchanged rules.

---

### 3.2. Lack of real-time shop floor visibility

Symptoms:

- Jobs are released based on due dates and rough capacity estimates, not actual live WIP, queue lengths, machine states.
- Operators follow local rules even if the next machine is fully congested or idle.

Evidence from logs:

- Jobs released very early relative to due dates then sit in intermediate queues for days.
- Upstream machines keep feeding a bottleneck despite long existing queue (no WIP cap).

Process mining:

- Compare release timing with actual start time of bottleneck operations:
  - Large mismatch indicates over-early release and lack of coordination.

---

### 3.3. Inaccurate durations / setup estimates in planning

Compare planned vs actual:

- For each operation type and machine:
  - Compute error statistics: `Actual – Planned`, percentage error, bias.
- Often find:
  - Systematic underestimation of actual times for certain operations,
  - Very high variability.

Implications:

- MRP or planning assumptions are too optimistic  due dates and load estimates are unreliable.
- Schedules based on these numbers are infeasible even with perfect rules.

Process mining helps by providing **empirical distributions** and bias estimates, highlighting where planning data must be updated.

---

### 3.4. Ineffective handling of sequence-dependent setups

Root issues:

- No explicit setup modeling in the scheduling rules.
- Job attributes (family, material) not used in sequencing decisions.
- Operators may favor “just do next” rather than grouping jobs.

Evidence:

- Setup matrices show very high cost transitions occur frequently.
- Many “lonely” jobs of certain families scheduled in between long sequences of other families.

---

### 3.5. Poor cross-work-center coordination and load balancing

- Parallel machines exist (e.g., multiple mills), but some heavily loaded while others underused.
- Because dispatching is local and may favor certain machines by habit, convenience, or perceived performance.

Process mining:

- Resource load profiling: utilization comparison across parallel machines.  
  - If MILL-02 at 90% and MILL-03 at 50% for similar job types  poor routing and dispatching.
- Handover-of-work network analysis:
  - Shows which machines are most often chosen for which operation types.

---

### 3.6. Inadequate response to breakdowns and urgent jobs

- After breakdowns, there is no global re-optimization or rerouting; jobs simply wait.
- Hot jobs are injected but often cause chaotic local decisions, leading to large collateral tardiness.

Evidence:

- Dotted charts: sharp spikes in waiting times for many jobs after breakdowns or urgent order arrivals.
- No “planned rerouting” events (e.g., jobs moved to parallel machines) even when alternatives exist.

---

### 3.7. Distinguishing scheduling problems vs capacity / variability limits

Process mining helps separate:

- **Scheduling-related issues:**
  - Long waiting times even when utilization is moderate (<80%).
  - Frequent priority violations.
  - Much higher observed waiting than what basic queuing theory would predict given utilization and variability.

- **Capacity-related issues:**
  - Sustained utilization ~90–95% on a bottleneck even during long periods.
  - Waiting times close to theoretical limits for that utilization (given observed service-time variability).
  - Large fraction of lateness even when scheduling appears “reasonable”.

Concrete steps:

1. **Fit duration distributions** per machine/operation from event logs.
2. Use queuing approximations (e.g., M/G/1 with Kingman’s formula) to compute minimal achievable waiting times under random arrivals.
3. Compare with observed waiting times:
   - If observed  theoretical  scheduling / control is main issue.
   - If observed  theoretical  genuine capacity limitation.

Also:

- Compare performance during breakdown-free intervals vs overall.
- If performance acceptable without breakdowns but poor overall, then reliability is key constraint.

---

## 4. Advanced Data-Driven Scheduling Strategies

Below are three advanced strategies grounded in the mined data.

---

### Strategy 1 – Dynamic, Multi-Factor Dispatching (ATCS-like with setup and lookahead)

**Core idea:**  
Replace simple FCFS/EDD with a dynamic priority index per job at each machine that accounts for:

- Due-date urgency,
- Order priority,
- Remaining processing time,
- Sequence-dependent setup impact,
- Downstream bottleneck load,
- Job “age” to avoid starvation.

A practical form is an **enhanced Apparent Tardiness Cost with Setups (ATCS)**.

#### 4.1.1. Priority index definition

For each job j waiting at machine m at time t:

Let:

- `s_j` = estimated processing time of current operation on m (from mined durations).
- `R_j` = estimated remaining total processing time for the job after this operation (from routing and averages).
- `D_j` = due date; `t` = current time.
- `Slack_j = (D_j – t – R_j)` (can be negative).
- `P_j` = priority class weight (e.g., 1 for normal, 2 for urgent).
- `Setup(prev, j)` = expected setup time from current job family to j’s family on machine m (from setup matrix).
- `Downstream_risk_j` = function of expected load at the next bottleneck machine(s) along j’s route (computed from current queue lengths and average service times).
- `Age_j` = time since job release or time in system.

Define a **priority score** (higher means earlier):

\[
\text{Score}_{m,j} = P_j \cdot \exp \left( - \alpha \frac{\text{Slack}_j}{\bar{s}} \right) \cdot \exp \left( -\beta \frac{\text{Setup(prev, j)}}{\bar{\text{setup}}} \right) \cdot \exp \left( -\gamma \cdot \text{Downstream\_risk}_j \right) + \delta \cdot f(\text{Age}_j)
\]

- Parameters , , ,  calibrated using historical data:
  - Run optimization on historical days: simulate dispatching with different weights, choose those minimizing tardiness or weighted cost.
  - Or use reinforcement learning in simulation.

This structure:

- Increases score when slack is low (urgent jobs),
- Penalizes jobs that require very long setups from current family,
- Penalizes those that will likely congest downstream bottlenecks,
- Provides a small boost for older jobs to avoid starvation.

#### 4.1.2. Use of process mining data

- **Slack, remaining processing time:** from mined average durations per operation and routing per job type.
- **Setup(prev,j):** from mined sequence-dependent setup matrix.
- **Downstream_risk:** computed from real-time queue lengths (from MES events) plus mined processing times.
- **Priority & job age:** directly from log / order data.

#### 4.1.3. How it addresses pathologies

- **Tardiness & high-priority jobs:** Due-date and priority explicitly in score  urgent jobs jump ahead when risk of lateness is high.
- **Setup inefficiency:** Setup term nudges schedule toward grouping jobs with similar setups, especially on bottlenecks.
- **Downstream starvation & WIP bullwhip:** Downstream risk term prevents pushing lots of work into already congested bottlenecks.
- **WIP reduction:** Combine with simple WIP caps (CONWIP or bottleneck-based release). Jobs enter early steps only when there is capacity downstream in their likely path.

Expected impact:

- Lower average and variance of tardiness.
- Less time lost in setups at bottlenecks.
- Reduced WIP and smoother flow.

---

### Strategy 2 – Predictive Scheduling & Rolling-Horizon Re-Optimization

**Core idea:**  
Use mined historical data and machine learning to predict:

- Operation processing times (job & machine specific),
- Setup times (sequence specific),
- Breakdown risk patterns.

Then, run a **rolling-horizon scheduler** (digital twin) that regularly re-optimizes the short-term schedule using these predictions.

#### 4.2.1. Predictive models

From the event log:

- Build models for **processing time**:
  - Inputs/features: job family, material, thickness, quantity, machine ID, operator, time of day, queue length.
  - Target: `Task Duration (Actual)`.
  - Methods: Gradient boosted trees / random forest / regression, depending on complexity.

- Build models for **setup time**:
  - Features: family_prev, family_curr, machine, operator, whether tool change or fixture change required.
  - Target: `Setup Duration (Actual)`.

- Build **breakdown risk** models:
  - From breakdown logs: model time-between-failures per machine (Weibull, etc).
  - Optionally conditional on usage (e.g., hours since last maintenance) and load.

Process mining provides the cleaned data sets, derived features (families, operations, state durations) and routing structure.

#### 4.2.2. Rolling-horizon optimization

Every  minutes (e.g., 15–30):

1. **State snapshot**:
   - Which jobs are in system and where,
   - Which operations are ongoing and their predicted remaining times,
   - Up-to-date predictions of processing/setup times for queued and upcoming jobs,
   - Machine availability and breakdown risk for the next horizon (e.g., use expected downtime slots).

2. **Scheduling problem formulation** (for a limited time window, e.g., next 8–12 hours):
   - Objective: Minimize total weighted tardiness + setup penalties, maintain WIP limits.
   - Constraints:
     - Precedence constraints between operations of a job.
     - Machine and operator availability, maximum parallel work.
     - Non-preemptive tasks.

3. **Solution methods**:
   - Shifting bottleneck heuristic,
   - Metaheuristics (tabu search, simulated annealing, GA),
   - MILP for smaller horizon (e.g., bottleneck machines only).

4. **Execution**:
   - Implement only near-term decisions (e.g., next few hours, or next few machine allocations).
   - At next horizon, re-evaluate with updated data and predictions.

#### 4.2.3. Due-date quotation and risk assessment

When new orders arrive:

- Simulate their path using predictive models and current load.
- Estimate completion time distribution and tardiness risk.
- Suggest due date that satisfies target on-time probability (e.g., 90%).
- If risk too high, negotiate later due dates or adjust capacity (overtime).

#### 4.2.4. How it addresses pathologies

- **Unpredictable lead times:** Prediction-based simulation at order entry improves due-date reliability.
- **High tardiness:** Scheduler anticipates future bottlenecks and proactively sequences to avoid them.
- **Breakdowns:** Incorporates breakdown risk into planning, reducing exposure (e.g., avoid loading heavily just before high-risk periods).
- **Inaccurate estimates:** Prediction models replace crude planned times with empirically calibrated expectations.

Expected impact:

- Significantly better alignment between promised and actual completion times.
- Lower tardiness for a given load level.
- Better mitigation of disruptions through proactive rescheduling.

---

### Strategy 3 – Setup Time Optimization via Family Batching and Sequence Design

**Core idea:**  
Focus specifically on minimizing sequence-dependent setup times at bottleneck and high-setup machines through intelligent batching and sequencing of job families.

#### 4.3.1. Family identification and setup matrix

Using process mining:

- Define or infer **setup families**:
  - Using domain knowledge: material, thickness, fixture type, machine program family.
  - Using clustering: group jobs that exhibit mutually low setup times between each other.

- Build a **family-level setup matrix S(F_prev, F_next)** per key machine from historical data.

#### 4.3.2. Local sequencing optimization at bottlenecks

For each scheduling horizon (e.g., daily plan) at a bottleneck machine:

1. Determine jobs **available or expected** to be available within the horizon (release/ready times from upstream & log-based predictions).
2. Formulate a scheduling problem:

   - Decision: sequence of jobs on this machine.
   - Objective: Weighted combination of:
     - Minimize total sequence-dependent setup time,
     - Minimize due-date related penalties (late jobs get higher penalty).
   - Constraints:
     - Job ready time,
     - Non-preemptive, sequence constraints.

3. Use heuristics:
   - **Modified NEH / insertion heuristics**:
     - Start with earliest due-date list, iteratively reorder to reduce setup time.
   - **Greedy nearest-neighbor on setup matrix**:
     - From current family, pick next job from family minimizing `Setup + tardiness_penalty`.
   - **Tabu search** or **GRASP** for further improvement on high-impact bottlenecks.

4. Implement sequence decisions as **soft constraints**:
   - Daily plan gives a recommended sequence.
   - Within that plan, dynamic rules (Strategy 1) can still adjust for urgent jobs, but keep groupings where possible.

#### 4.3.3. Intelligent batching

For families with frequent jobs:

- Create **min-batch / max-batch** rules:
  - Min-batch: only switch away after processing at least n jobs of the same family or certain accumulated volume.
  - Max-delay: any job cannot wait more than X time just to join a batch.

- Example:
  - “At HEAT-01, run carbon steel batch once every 4 hours; accumulate jobs up to that point, but no job waits more than 1 day purely for batching.”

Process mining helps identify:

- Historically effective batch sizes and campaign lengths (where setup/processing ratio looked optimal).
- Tolerance of customers/due dates to batching delays.

#### 4.3.4. How it addresses pathologies

- **High setup times on bottlenecks:** Directly attacks the largest component of non-value-adding time.
- **WIP in front of bottlenecks:** By smoother campaigns and better grouping, throughput increases for same capacity, reducing upstream queues.
- **Tardiness due to fragmented sequencing:** Less time “lost” in repeated setups frees capacity for jobs closer to due dates.

Expected impact:

- Significant reduction in setup time share at targeted machines (e.g., 20–40% reduction).
- Higher effective throughput at bottleneck without investment.
- Reduction in variability of waiting times for jobs processed within campaigns.

---

## 5. Simulation, Evaluation, and Continuous Improvement

### 5.1. Discrete-event simulation for evaluation

Build a **digital twin** of the job shop using DES, parameterized with process-mined data.

#### 5.1.1. Model inputs from process mining

- **Process structure:**
  - Routing probabilities: from discovered process model (Inductive Miner).
  - Operation sequences per job type and rework probabilities.

- **Processing times:**
  - Distributions per (operation type, machine, job family), from actual durations.
  - Correlations where relevant (e.g., longer cutting  longer grinding).

- **Setup times:**
  - Sequence-dependent setup matrices per machine, using distributions (not just mean).

- **Arrivals and due dates:**
  - Job arrival process (time-varying): from historical order release patterns.
  - Due-date assignment logic: as in reality, or a new policy you want to test.

- **Breakdowns and maintenance:**
  - MTBF/MTTR per machine from breakdown logs.
  - Preventive maintenance schedule.

- **Operators and shifts:**
  - Calendars, skill constraints, if relevant.

- **Scheduling rules:**
  - Baseline: FCFS/EDD.
  - Strategy 1: dynamic dispatching index.
  - Strategy 2: rolling-horizon scheduling decisions.
  - Strategy 3: batching & campaign rules.

#### 5.1.2. Scenarios to test

- **Baseline scenario:**
  - Reproduce last year’s load and mix; verify simulation matches historical KPI distributions (calibration).

- **Stress scenarios:**
  - Increased load: +10%, +20%, +30% orders.
  - Increased variability: more variation in processing times.
  - Higher disruption rates: 50% more breakdown frequency.

- **Product mix changes:**
  - More jobs of high-setup families.

- **Priority mix changes:**
  - Higher proportion of urgent jobs.

For each scenario and strategy:

- Run multiple replications (different random seeds) to obtain statistically reliable results.

#### 5.1.3. KPIs to compare

- **Due-date performance:**
  - % on-time, average tardiness, 95th percentile tardiness, weighted tardiness.
- **Lead times and WIP:**
  - Mean and variance of job lead time, WIP levels per work center.
- **Resource utilization:**
  - Overall, processing-only, setup-only.
- **Setup metrics:**
  - Total setup hours at key machines.
- **Robustness:**
  - How KPIs degrade under stress scenarios.

Use the simulation to:

- Select the best scheduling strategy (or combination).
- Tune parameters (weights in the priority function, batching thresholds, WIP caps).

---

### 5.2. Continuous monitoring and adaptive improvement

After deploying an improved strategy, use ongoing process mining on live event data as a **control loop**.

#### 5.2.1. Real-time / near-real-time monitoring

- Stream MES events into a process-mining-enabled dashboard (with small delay).
- Monitor key KPIs by shift/day:
  - Tardiness rate,
  - Lead times,
  - WIP per work center,
  - Setup time ratios at bottlenecks,
  - Breakdown time vs plan.

Use **control charts** (EWMA/Shewhart) to detect when:

- Average lead time or tardiness drifts above thresholds.
- Setup ratios grow (suggesting new product mixes or degraded adherence to sequence rules).
- Resource utilization patterns shift.

#### 5.2.2. Drift detection in process and performance

Regular intervals (e.g., monthly):

- Re-run **process discovery**:
  - Check if new routing variants appeared (new product types, process changes).
  - Use conformance checking against the reference model to detect deviations.
- Recompute performance overlays:
  - Identify if bottlenecks have shifted to different machines.
  - Detect new “hot spots” of waiting time.

#### 5.2.3. Model and policy updating

- Retrain predictive models of processing and setup time with the latest data.
- Update setup matrices, family definitions, and batch rules as product mix evolves.
- Re-optimize dispatching weights (, , ,  in Strategy 1) using recent logs and simulation.

If KPIs degrade despite good scheduling adherence:

- Signal need for structural capacity changes (extra shifts, new machines) or maintenance improvements.

#### 5.2.4. Feedback to planning and S&OP

- Use mined and simulated capacity envelopes to:
  - Set realistic due-date promises and load acceptance levels.
  - Align long-term capacity decisions (e.g., buying an additional mill vs extending shifts) with observed bottlenecks and utilization.

---

### Summary

Using detailed MES event logs and process mining, Precision Parts Inc. can:

1. Precisely map and measure the current as-is process, including flow times, waiting, utilization, setups, and disruption impacts.
2. Diagnose pathologies: true bottlenecks, poor prioritization, wasteful sequencing, WIP bullwhip, and inadequate disruption handling.
3. Distinguish between problems solvable by smarter scheduling and those requiring structural capacity or reliability changes.
4. Implement advanced, data-driven scheduling strategies:
   - Multi-factor dynamic dispatching that accounts for due dates, setups, and downstream load,
   - Predictive, rolling-horizon scheduling using ML-based duration and breakdown predictions,
   - Setup optimization via family batching and optimized sequencing at bottlenecks.
5. Validate and refine these strategies through discrete-event simulation, then continuously monitor and adapt them using ongoing process mining.

This approach moves the shop from reactive, local rule-based dispatching to an integrated, predictive, and adaptive scheduling framework grounded in its own data.