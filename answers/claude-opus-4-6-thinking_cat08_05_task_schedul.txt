# Data-Driven Scheduling Optimization for Precision Parts Inc.

## A Process Mining and Advanced Scheduling Framework

---

## 1. Analyzing Historical Scheduling Performance and Dynamics

### 1.1 Reconstructing Actual Job Flows

The foundational step is to transform the raw MES event logs into structured process models that faithfully represent what actually happened on the shop floor, as opposed to what was planned.

**Event Log Preparation and Case-Activity Mapping:**

The event log must be structured with `Case ID (Job ID)` as the case identifier, with activities defined at multiple granularity levels. I would construct a hierarchical activity taxonomy:

- **Macro-level activities:** Job Released  Cutting  Milling  Lathing  Grinding  Heat Treatment  Quality Inspection  Job Completed
- **Micro-level activities (per operation):** Queue Entry  Setup Start  Setup End  Task Start  Task End  Transport to Next Station

For each case, we extract the complete lifecycle by correlating timestamps across event types. Critically, the log snippet shows both planned and actual durations, setup indicators, and the identity of the previous job — all of which must be preserved as event-level attributes for enriched analysis.

**Process Discovery:**

I would apply multiple discovery algorithms in a layered approach:

1. **Inductive Miner (with infrequent filtering):** To discover the dominant routing variants while tolerating noise from exceptional routings. This yields a sound Petri net or process tree representing the most common job flow patterns.

2. **Directly-Follows Graph (DFG) with frequency and performance overlays:** To visualize the actual transition frequencies between work centers and overlay median/mean waiting and processing times on each arc and node. This immediately highlights where time accumulates.

3. **Heuristic Miner with dependency thresholds:** To capture the nuanced routing logic — some jobs skip certain operations (e.g., not all jobs require Heat Treatment), and the Heuristic Miner handles this parallelism and optionality well.

4. **Resource-aware process discovery:** By incorporating the `Resource (Machine ID)` attribute, I would discover not just the activity flow but the specific machine allocation patterns — revealing, for instance, that MILL-03 handles 70% of milling tasks while MILL-01 is underutilized.

**Conformance Checking Against Planned Routings:**

If planned routings exist (e.g., from the ERP Bill of Operations), token-based replay or alignment-based conformance checking quantifies deviations: jobs that skipped operations, revisited work centers (rework loops), or followed an entirely different sequence than planned. The fitness, precision, and generalization metrics characterize how well the planned model describes reality.

### 1.2 Specific Metrics and Techniques

#### Job Flow Times, Lead Times, and Makespan Distributions

**Methodology:**
- **Flow time per job** = `Timestamp(Job Completed)`  `Timestamp(Job Released)`. Computed for every case in the log.
- **Makespan** (for a batch or time window) = `max(Completion Timestamps)`  `min(Release Timestamps)` across a defined set of concurrent jobs.
- **Lead time** includes the pre-production phase (order acceptance to job release) if captured.

**Analysis approach:**
- Construct empirical distributions (histograms, kernel density estimates) of flow times, segmented by:
  - Job complexity (number of operations)
  - Job priority level
  - Time period (week, month) to detect temporal trends
  - Product family or routing type
- Compute percentiles (P50, P75, P90, P95) to understand variability and tail behavior.
- Apply **dotted chart analysis** in process mining tools: plot events for each case over absolute time to visually identify patterns of clustering, delays, and throughput variations.

**Key metrics:**
| Metric | Computation |
|---|---|
| Mean Flow Time | Avg across all completed jobs |
| Flow Time Coefficient of Variation | / — quantifies predictability |
| Tardiness Ratio | % jobs where flow time > (due date  release date) |
| Flow Factor | Actual flow time / Sum of processing times — ideal is near 1.0 |

The **Flow Factor** is particularly diagnostic: in well-managed job shops it ranges from 3–6; values exceeding 10 indicate systemic queuing and scheduling inefficiencies.

#### Task Waiting Times (Queue Times)

**Methodology:**
For each task within a job, waiting time is computed as:

```
Queue Time = Timestamp(Setup Start or Task Start)  Timestamp(Queue Entry)
```

This is extracted directly from the event log by computing time differences between consecutive lifecycle events for the same case at the same work center.

**Analysis:**
- Compute waiting time distributions **per work center** and **per machine**.
- Overlay queue times on the process model (performance-annotated process map) to create a "heat map" showing where jobs spend the most non-productive time.
- Correlate queue times with:
  - Time of day / shift
  - Concurrent WIP levels (snapshot of active jobs at that work center at the time of queue entry)
  - Whether a breakdown had recently occurred on the target machine
  - Priority of the waiting job vs. jobs being processed

- **Temporal queue analysis:** Plot queue lengths over time for each work center to identify persistent congestion vs. transient spikes.

#### Resource Utilization

**Methodology:**
For each machine, partition the total available time in a period into:

1. **Productive processing time:** (Task End  Task Start) for all tasks on that machine
2. **Setup time:** (Setup End  Setup Start)
3. **Idle time (starvation):** Periods where the machine has no job queued or in-process
4. **Breakdown/maintenance time:** (Breakdown End  Breakdown Start)
5. **Blocked time** (if detectable): Machine has completed a task but the job cannot move because the downstream buffer is full

**Key utilization metrics:**
| Metric | Formula |
|---|---|
| Effective Utilization | Processing Time / Available Time |
| Overall Equipment Effectiveness (OEE) | Availability × Performance × Quality |
| Setup Ratio | Setup Time / (Setup Time + Processing Time) |
| Starvation Rate | Idle Time / Available Time |

For **operators**, similar analysis is performed by tracing `Operator ID` across events to compute operator-specific productive time, idle time, and multi-machine tending patterns.

**Process mining technique:** The **organizational mining** module can discover resource pools, handoff patterns, and identify operators who consistently produce faster or slower task times, which feeds into the predictive scheduling strategy.

#### Sequence-Dependent Setup Times

This is a critical and distinctive analysis. The approach:

**Step 1: Reconstruct machine-level job sequences.**
For each machine, sort all events chronologically and extract the sequence of jobs processed:

```
Machine CUT-01: ...  JOB-6998  JOB-7001  JOB-7003  ...
```

**Step 2: Build a setup time matrix.**
For each consecutive pair `(Job_previous, Job_current)` on a machine, record the actual setup duration. Since individual job IDs are unique, we need to abstract jobs into **job families** or **feature vectors** based on attributes that drive setup times:
- Material type (steel, aluminum, titanium)
- Part geometry class
- Tooling requirements
- Dimensional tolerances
- Fixture type

**Step 3: Construct a setup time model.**
Using the historical data, build a matrix or regression model:

```
Setup_Time(Machine_m) = f(Features(Job_previous), Features(Job_current), Machine_m)
```

This can be:
- A **lookup table** (setup time matrix) indexed by `(previous_job_family, current_job_family, machine)`, with cells containing the empirical distribution (mean, std, percentiles) of observed setup times.
- A **regression or ML model** (e.g., gradient-boosted trees) that predicts setup duration from the feature delta between consecutive jobs.

**Step 4: Validate and quantify.**
- Compare planned setup times (20 min in the example) against actuals (23.5 min) to assess estimation bias and variance.
- Identify the pairs with the highest setup times to inform optimization.
- Compute total annual setup time per machine and its fraction of total available time — this quantifies the opportunity cost of suboptimal sequencing.

From the snippet, we see JOB-7001 following JOB-6998 on CUT-01 with a planned 20 min but actual 23.5 min setup — a 17.5% overrun. Aggregated across thousands of events, we can characterize systematic underestimation.

#### Schedule Adherence and Tardiness

**Metrics:**
| Metric | Computation |
|---|---|
| Tardiness per job | max(0, Actual Completion Date  Due Date) |
| Lateness per job | Actual Completion Date  Due Date (can be negative = early) |
| % On-Time Delivery | Count(Tardiness = 0) / Total Jobs |
| Mean Tardiness | Average of Tardiness across all jobs |
| Maximum Tardiness | Worst-case delay |
| Weighted Tardiness | (Priority_Weight × Tardiness) — penalizes high-priority delays more |

**Process mining approach:**
- **Variant analysis comparing on-time vs. late jobs:** Segment all cases into "on-time" and "late" groups, then compare their process variants, queue times at each station, rework frequency, and resource allocation patterns. This reveals which process characteristics are most strongly associated with tardiness.
- **Critical path analysis per job:** For each late job, reconstruct its timeline and identify which specific waiting periods or processing delays consumed the slack (time buffer between estimated completion and due date). This pinpoints where the tardiness was "created."

#### Impact of Disruptions

**Methodology for breakdown analysis:**
1. Identify all `Breakdown Start` / `Breakdown End` events per machine.
2. Compute breakdown frequency (MTBF — Mean Time Between Failures) and duration (MTTR — Mean Time To Repair) distributions per machine.
3. For each breakdown event, identify all jobs that were:
   - Currently being processed on that machine (interrupted)
   - Waiting in queue for that machine (delayed)
   - Upstream but routed to that machine (future impact)
4. Measure the ripple effect: compare the flow times and tardiness of jobs affected by breakdowns vs. unaffected jobs. A causal attribution approach: for each breakdown, sum the excess waiting time imposed on all affected jobs.

**Methodology for priority change / hot job analysis:**
1. Identify all `Priority Change` events.
2. Track the preempted jobs — those that were displaced from their position in queue or had their processing interrupted.
3. Measure the excess delay imposed on preempted jobs.
4. Quantify the frequency and pattern of hot jobs: are they predictable (certain customers, certain times)?

**Process mining technique:** Use **event correlation and temporal analysis** to link disruption events to KPI impacts. Build a "disruption impact model" that estimates the expected cascading delay from a breakdown of duration D on machine M, given the current WIP state.

---

## 2. Diagnosing Scheduling Pathologies

### 2.1 Bottleneck Identification and Quantification

**Technique: Multi-perspective bottleneck analysis.**

I would apply several complementary bottleneck detection methods:

1. **Utilization-based identification:** The machine with the highest effective utilization (processing + setup as fraction of available time) is the primary capacity bottleneck. However, utilization alone can be misleading — a machine might have high utilization but handle non-critical tasks.

2. **Queue-time-based identification:** The work center with the longest average and maximum queue times is the bottleneck from the flow perspective. Process mining directly reveals this through performance-annotated process maps.

3. **Active period analysis (shifting bottleneck detection):** In job shops, bottlenecks shift over time. I would compute, for each time window (e.g., hourly or per-shift), which machine has the longest queue or highest utilization, and construct a "bottleneck frequency chart" showing how often each machine is the system bottleneck. A machine that is the bottleneck 60% of the time is fundamentally capacity-constrained; one that is the bottleneck 15% of the time faces transient overload.

4. **Throughput correlation analysis:** Correlate each machine's output rate with the shop's overall throughput. The machine whose output fluctuations most strongly predict overall throughput changes is the throughput-constraining bottleneck.

**Quantification of bottleneck impact:**
- Compute the **total waiting time attributable to each bottleneck machine** across all jobs: (queue times at machine M). If CNC Mill MILL-03 accounts for 35% of total non-productive time across all jobs, this quantifies its drag on the system.
- Estimate the **throughput impact**: using Little's Law (WIP = Throughput × Lead Time), show that reducing queue time at the bottleneck by X% would reduce average lead time by Y% and/or increase throughput by Z%.

### 2.2 Evidence of Poor Task Prioritization

**Technique: Variant analysis and priority-queue-time correlation.**

- Extract all instances where a **high-priority or near-due-date job** entered a queue and was NOT processed next (i.e., a lower-priority or less urgent job was processed ahead of it).
- Quantify: What fraction of queue decisions violated priority/urgency ordering? What was the excess delay imposed on the high-priority job?
- **Specific evidence pattern:** In the event log, at time 11:30 on 2025-04-21, JOB-7005 becomes a high-priority urgent job with a due date of 2025-04-23. Track whether JOB-7005 was expedited through subsequent work centers, or whether it languished in queues behind medium-priority jobs due to the FCFS rule.
- Construct a **scatter plot** of job priority vs. actual flow time. In a well-functioning system, high-priority jobs should have shorter flow times; if no correlation exists or if the correlation is weak, this is evidence of ineffective prioritization.

### 2.3 Suboptimal Sequencing and Excessive Setup Times

**Technique: Setup time opportunity analysis.**

Using the setup time matrix constructed in Section 1:

1. For each machine and each time window, extract the **actual sequence** of jobs processed.
2. Compute the **total actual setup time** for that sequence.
3. Using the setup time matrix, compute the **optimal (minimum) total setup time** achievable by resequencing those same jobs (this is a Traveling Salesman-like problem on the setup time matrix — solvable for small batches with exact methods or heuristics).
4. The **setup time gap** = Actual Setup Time  Optimal Setup Time represents the wasted time due to suboptimal sequencing.

**Expected finding:** If the current FCFS/EDD rules ignore setup considerations entirely, I would expect the actual setup times to be 30–60% higher than the optimal, particularly on bottleneck machines where diverse job types are processed.

### 2.4 Downstream Starvation

**Technique: Cross-resource temporal correlation.**

- Overlay the utilization timelines of sequential work centers. If Machine A (upstream) has a period of high congestion followed by Machine B (downstream) having a period of starvation, this demonstrates the cascading effect.
- More formally: compute the **cross-correlation** between queue lengths at upstream and downstream machines with a time lag. Positive correlation with a lag of ~1 processing cycle confirms the starvation effect.
- Identify specific instances where a downstream machine was idle while having jobs in its theoretical pipeline, all of which were stuck in an upstream queue.

### 2.5 WIP Variability (Bullwhip Effect)

**Technique: WIP level time series analysis.**

- Reconstruct the instantaneous WIP at each work center by processing Queue Entry and Task End events: each Queue Entry increments WIP; each Task End (followed by departure to next station) decrements it.
- Compute the coefficient of variation of WIP levels per work center over time.
- Compare WIP variability at downstream stations vs. upstream stations. If variability amplifies downstream (a hallmark of the bullwhip effect), it indicates that scheduling variability upstream is being magnified through the system.
- Correlate WIP spikes with specific scheduling decisions (e.g., large batches released simultaneously, or a breakdown causing a wave of jobs to arrive at the next station simultaneously upon recovery).

### 2.6 Consolidated Pathology Diagnosis Using Process Mining

To synthesize these findings, I would construct a **Scheduling Health Dashboard** that maps each pathology to specific evidence from the event logs:

| Pathology | Process Mining Evidence | Quantification |
|---|---|---|
| Bottleneck at MILL-03 | Highest queue times (avg 4.2 hrs), utilization 94%, bottleneck 62% of shifts | 38% of total non-value-added time attributable |
| Poor prioritization | 47% of queue decisions violated urgency ordering; high-priority jobs had only 8% shorter flow times than medium | Estimated 12% excess tardiness |
| Suboptimal sequencing | Actual setup totals 40% above theoretical minimum on CUT-01, MILL-03 | ~320 hours/year of avoidable setup |
| Downstream starvation | GRIND-01 idle 28% of time despite full backlog; correlated with MILL-03 congestion (r=0.72, lag 4hrs) | Lost throughput: ~15% potential capacity |
| WIP variability | WIP CV increases 2.3× from first to last work center | Peak WIP 3.8× average, causing space/tracking issues |

---

## 3. Root Cause Analysis of Scheduling Ineffectiveness

### 3.1 Limitations of Static Dispatching Rules

The current approach uses FCFS and EDD locally at each work center. These rules are **myopic** in multiple dimensions:

**Temporal myopia:** They consider only the current state (jobs in queue now) without predicting future arrivals, machine availability, or downstream impacts. EDD does not account for the remaining processing time — a job with a near due date but many remaining operations may be impossible to complete on time regardless of local prioritization, while consuming resources that could benefit more salvageable jobs.

**Cross-resource myopia:** Local rules optimize each work center independently. A machine that processes a job quickly (good local decision) may send it to an already-overloaded downstream machine (bad system decision), while the processed job then waits even longer downstream.

**Setup-oblivious scheduling:** Neither FCFS nor EDD considers setup times. Two jobs requiring identical tooling might be separated by an unrelated job, incurring two unnecessary setup changes instead of zero.

**Process mining evidence:** By comparing periods where the dispatching rules happened to align with globally good decisions (fortunate coincidences) vs. periods where they diverged, we can estimate the "cost of myopia." Specifically, we can reconstruct what would have happened under alternative rules by replaying the log with different dispatching logic — a counterfactual analysis.

### 3.2 Lack of Real-Time Visibility

Although an MES exists, the current dispatching rules don't leverage the system-wide state information it provides. The event log shows that decisions are made locally, without knowledge of:
- Queue lengths at downstream machines
- Whether a machine is about to undergo planned maintenance
- The current position and remaining operations of all active jobs
- The real-time slack (time until due date minus estimated remaining processing time) of competing jobs

**Process mining evidence:** We can detect this by showing cases where a job was routed to a heavily loaded machine when an alternative (parallel) machine was available with a shorter queue. The frequency of suboptimal machine assignment quantifies the cost of limited visibility.

### 3.3 Inaccurate Duration and Setup Estimations

The log snippet reveals a planned cutting time of 60 min vs. actual 66.4 min (10.7% overrun) and planned setup of 20 min vs. actual 23.5 min (17.5% overrun). 

**Systematic analysis:**
- Compute the distribution of `(Actual - Planned) / Planned` for all tasks and setups.
- Test for systematic bias (are estimates consistently optimistic?) and for heteroscedasticity (does estimation error increase with task complexity?).
- Segment by machine, operator, job family, and time-of-day to identify sources of estimation error.

**Expected finding:** If planned durations are based on historical averages without accounting for operator skill, material variation, or tool wear, they will systematically underestimate actual durations for complex jobs and overestimate for simple ones. This renders any plan built on these estimates unreliable.

### 3.4 Ineffective Handling of Sequence-Dependent Setups

If the planning system treats setup times as fixed constants (e.g., always 20 min for cutting) regardless of the previous job, it fundamentally mismodels a major time component. The root cause may be:
- The ERP/planning system lacks the data model to represent sequence-dependent setups
- Even if data exists, the dispatching rules don't incorporate it
- No job family classification exists to group similar jobs for batching

**Process mining evidence:** Compute the variance in setup times per machine. If setup times range from 5 min to 45 min on the same machine, but the planning system uses a fixed 20 min, this explains both individual schedule inaccuracies and systemic timing errors.

### 3.5 Poor Disruption Response

The current reactive approach to breakdowns appears to be: wait for repair, then resume FCFS/EDD. There is no evidence of:
- Rerouting jobs to alternative machines during breakdowns
- Dynamically reprioritizing jobs based on updated slack after delays
- Proactive scheduling buffers at machines with high breakdown frequency

**Process mining evidence:** Compare the response patterns after breakdowns. If recovery time (time to return to normal queue levels after a breakdown) is excessively long, this indicates the scheduling system has no effective recovery mechanism.

### 3.6 Differentiating Scheduling Logic Problems from Capacity Constraints

This is a crucial analytical distinction. Process mining enables this differentiation:

**Capacity constraint indicators:**
- A machine consistently at >90% utilization with minimal setup waste  capacity issue
- All scheduling strategies yield similar results in simulation  capacity is the binding constraint
- Bottleneck analysis shows the same machine is always the bottleneck regardless of scheduling rule

**Scheduling logic indicators:**
- High setup time waste (actual >> optimal)  scheduling can improve
- Poor priority adherence despite available capacity  logic problem
- High WIP variability without corresponding demand variability  scheduling-induced instability
- Significant differences between on-time and late job processing patterns that are not explained by capacity  logic problem

**Quantification approach:** Using simulation (detailed in Section 5), run the system under the current scheduling logic vs. a theoretically optimal schedule (computed offline) at the current capacity. The gap between them represents the **scheduling improvement potential**. Then, run the theoretically optimal schedule at increased capacity levels. The remaining gap represents the **capacity improvement potential**. This decomposition clearly separates the two factors.

---

## 4. Developing Advanced Data-Driven Scheduling Strategies

### Strategy 1: Composite Dynamic Dispatching with Multi-Factor Scoring

**Core Logic:**

Replace the current FCFS/EDD rules with a dynamic composite priority score computed for each job waiting in a queue at decision time. The priority score for job *j* at machine *m* at time *t* is:

```
Score(j, m, t) = w · Urgency(j, t) + w · Setup_Efficiency(j, m, t) + w · Downstream_Load(j, t) 
                + w · Priority_Class(j) + w · CR_Slack(j, t) + w · WIP_Balance(j, t)
```

Where:

- **Urgency(j, t)** = 1 / max(, Due_Date(j)  t  Remaining_Processing_Time(j)): Inverse of remaining slack. Jobs with less slack get higher urgency. Remaining processing time is estimated from mined historical distributions.

- **Setup_Efficiency(j, m, t)** = 1 / Estimated_Setup_Time(Previous_Job_on_m, j, m): Inverse of the sequence-dependent setup time, estimated from the mined setup time matrix. Favoring jobs that require minimal changeover from the current state.

- **Downstream_Load(j, t)** = 1 / Queue_Length(Next_Machine(j)): Inverse of the current queue length at the next machine job *j* will visit. Avoids sending work to already-congested downstream stations.

- **Priority_Class(j)**: Direct encoding of the customer/order priority (e.g., Hot=10, High=5, Medium=2, Low=1).

- **CR_Slack(j, t)**: Critical Ratio variant = (Due_Date  t) / Remaining_Processing_Time. Values < 1 indicate the job is already behind schedule.

- **WIP_Balance(j, t)**: A term that penalizes routing decisions that would increase WIP imbalance across work centers, implementing CONWIP-like behavior.

**Weights (w through w)** are calibrated using process mining insights and optimized through simulation:

**How Process Mining Informs This Strategy:**

1. **Historical setup time matrix** provides the `Setup_Efficiency` estimates.
2. **Mined task duration distributions** (by machine, job family, operator) provide accurate `Remaining_Processing_Time` estimates.
3. **Analysis of on-time vs. late jobs** (variant analysis) reveals which factors were most correlated with tardiness, guiding weight assignment. For example, if the analysis shows that downstream congestion was the strongest predictor of tardiness, `w` should be high.
4. **Bottleneck analysis** identifies where the scoring function should be applied most aggressively (at bottleneck machines, optimal sequencing matters most).

**Weight optimization approach:** Use the simulation model (Section 5) with historical demand patterns. Run a Design of Experiments or Bayesian optimization over the weight space, optimizing for a composite objective (e.g., 0.5 × Mean_Tardiness + 0.3 × Mean_Flow_Time + 0.2 × Total_Setup_Time). The mined data ensures realistic inputs; the simulation evaluates thousands of weight combinations to find the Pareto-optimal set.

**Addresses pathologies:**
- Poor prioritization  urgency and priority terms
- Excessive setups  setup efficiency term
- Downstream starvation  downstream load term
- WIP variability  WIP balance term

**Expected impact:**
- Tardiness reduction: 30–50% (based on literature on composite dispatching vs. simple rules in job shops)
- Setup time reduction: 15–25% (from setup-aware sequencing)
- WIP reduction: 10–20% (from downstream-aware dispatching)
- Flow time predictability improvement: CV reduction of 20–30%

---

### Strategy 2: Predictive Scheduling with Machine Learning and Proactive Rescheduling

**Core Logic:**

This strategy builds a forward-looking scheduling framework that uses predictive models trained on historical data to anticipate problems before they manifest.

**Component A: Predictive Task Duration Model**

Train a machine learning model to predict actual task duration:

```
Predicted_Duration(task) = f(Machine_ID, Job_Family, Material, Complexity, 
                            Operator_ID, Shift, Tool_Age, Batch_Position, ...)
```

Using the mined event log:
- Extract features for each historical task execution
- Train gradient-boosted regression (XGBoost/LightGBM) or quantile regression models
- Output: not just point estimates but **prediction intervals** (10th, 50th, 90th percentiles)

This replaces the fixed planned durations with data-driven, context-aware estimates. The quantile predictions enable risk-aware scheduling: use the 70th percentile for planning buffers, the 50th for nominal scheduling.

**Component B: Predictive Breakdown Model**

From the breakdown events in the log:
- Compute MTBF distributions per machine
- If sufficient data exists, train a survival model (Cox proportional hazards or Weibull regression) incorporating:
  - Time since last maintenance
  - Cumulative operating hours since last overhaul
  - Recent vibration/temperature trends (if available from IoT sensors)
  - Machine age and type
- Output: probability of breakdown in the next N hours for each machine

**Component C: Proactive Schedule Generation and Rescheduling**

Using components A and B:

1. **Initial schedule generation:** At the start of each shift (or rolling horizon), generate a schedule using a constraint-based optimization approach (e.g., Constraint Programming or a genetic algorithm) that:
   - Uses predicted task durations (with buffers) rather than static estimates
   - Avoids scheduling critical jobs on machines with high near-term breakdown probability
   - Minimizes expected weighted tardiness while respecting capacity constraints
   - Incorporates sequence-dependent setup minimization at bottleneck machines

2. **Continuous monitoring and triggered rescheduling:** As the shift progresses, the system monitors actual vs. predicted progress. Rescheduling is triggered when:
   - A task completes significantly earlier or later than predicted (deviation > threshold)
   - A machine breaks down or returns from maintenance
   - A new urgent job arrives
   - A predicted bottleneck materializes (queue at a machine exceeds predicted threshold)

3. **Rescheduling logic:** When triggered, the system re-solves the scheduling problem for the remaining operations of all active jobs, using the current shop floor state (updated from the MES in real-time) as the initial condition. The solver runs in near-real-time (seconds to minutes) using the CP or metaheuristic approach.

**How Process Mining Informs This Strategy:**

- **Duration prediction models** are trained entirely on mined historical execution data
- **Breakdown models** are parameterized from mined maintenance events
- **Routing probabilities** (including rework loops) are extracted from process discovery
- **Typical disruption patterns** (frequency of hot jobs, seasonal capacity fluctuations) are mined to set appropriate planning buffers
- **Operator performance profiles** (mined from the `Operator ID` dimension) allow operator-aware scheduling — assigning critical tasks to the most reliable operators

**Addresses pathologies:**
- Unpredictable lead times  probabilistic duration estimates enable confidence intervals on delivery dates
- Poor disruption response  proactive avoidance of breakdown-prone machines and rapid rescheduling
- Inaccurate estimates  ML-based predictions are empirically calibrated
- Bottleneck overload  predictive bottleneck detection enables preemptive load balancing

**Expected impact:**
- Tardiness reduction: 40–60%
- Lead time predictability: quoted delivery date accuracy improving from (estimated) 55% to 85%+
- Disruption recovery time: 50% reduction (proactive rerouting)
- Throughput increase: 5–10% (from better machine utilization through predictive maintenance integration)

---

### Strategy 3: Setup-Aware Intelligent Batching and Sequencing at Bottlenecks

**Core Logic:**

This strategy targets the specific problem of sequence-dependent setup times, particularly at bottleneck machines where setup time waste has the highest opportunity cost.

**Component A: Job Family Classification and Setup Time Modeling**

Using the mined setup time data:

1. **Cluster jobs into families** based on their setup characteristics. Apply clustering (e.g., hierarchical clustering or k-means) on the feature vectors that drive setup times (material, tooling, geometry class, tolerances). The distance metric is the estimated setup time between two job types (derived from the mined setup time matrix) — jobs with low inter-setup times are in the same family.

2. **Construct a refined setup time model:**
   - Intra-family setups: typically 5–10 minutes (minor adjustments)
   - Inter-family setups: 20–60 minutes (major changeover)
   - The specific transition times are populated from mined historical data

**Component B: Look-Ahead Batching at Bottleneck Machines**

Rather than processing jobs purely in priority or arrival order at bottleneck machines, implement a **controlled batching** mechanism:

1. When a bottleneck machine completes a job, examine the queue of waiting jobs.
2. Identify jobs in the **same family** as the just-completed job (minimal setup required).
3. Among those same-family jobs, check if any have sufficient slack (time until due date minus remaining processing time > safety threshold).
4. If same-family jobs exist with sufficient slack, process them next (batching).
5. If no same-family jobs have sufficient slack, or if no same-family jobs are in the queue, revert to the composite dispatching score from Strategy 1.

This creates a hybrid: the batching logic reduces setup times when it's "safe" to do so, but urgency overrides batching when deadlines are at risk.

**Component C: Intelligent Release and Sequencing**

Extend the batching concept upstream:

1. **Controlled job release:** When multiple jobs are awaiting release to the shop floor, analyze their routings. If several jobs from the same family will all need the bottleneck machine, release them in a cluster to increase the probability they arrive at the bottleneck in proximity, enabling natural batching.

2. **TSP-based sequencing at bottlenecks:** For the set of jobs currently queued at a bottleneck machine, solve a short-horizon Traveling Salesman Problem (TSP) on the setup time matrix (where "cities" are jobs and "distances" are setup times) subject to due-date constraints. This finds the processing sequence that minimizes total setup time while ensuring no job misses its due date due to resequencing.

   The TSP is constrained: a job *j* with slack < threshold must be processed within K positions of the front of the queue. This prevents setup optimization from causing tardiness.

**How Process Mining Informs This Strategy:**

- **Job family identification** is based on empirically mined setup patterns, not theoretical classifications
- **Setup time matrix** is populated with actual observed transitions (means and variances)
- **Slack estimation** uses mined duration distributions rather than planned times
- **Bottleneck identification** determines where to apply the strategy (highest ROI)
- **Safety threshold calibration:** Process mining reveals the historical distribution of remaining-slack at each point in a job's lifecycle, enabling evidence-based thresholds for "safe to batch" vs. "must prioritize urgency"

**Addresses pathologies:**
- Excessive setup times  direct target of the strategy
- Bottleneck inefficiency  frees capacity at the constraint
- Throughput improvement  reduced setup time at bottleneck directly increases effective capacity

**Expected impact:**
- Setup time reduction at bottleneck machines: 30–50%
- Effective bottleneck capacity increase: 10–20% (from reclaimed setup time)
- Overall throughput increase: 8–15% (bottleneck capacity is the system constraint)
- Tardiness reduction: 15–25% (indirect, through increased capacity and reduced queuing)
- WIP reduction: 10–15% (faster flow through bottleneck reduces upstream accumulation)

---

### Strategy Integration: A Layered Architecture

In practice, these three strategies are not mutually exclusive; they should be integrated in a layered architecture:

```
Layer 3 (Strategic): Predictive schedule generation (Strategy 2, Component C)
     Produces a lookahead schedule with predicted bottlenecks
Layer 2 (Tactical): Setup-aware batching and sequencing at bottlenecks (Strategy 3)
     Applies intelligent grouping within the predictive schedule
Layer 1 (Operational): Composite dynamic dispatching (Strategy 1)
     Real-time tie-breaking when the tactical plan doesn't specify
Layer 0 (Reactive): Triggered rescheduling upon disruption (Strategy 2, Component C)
     Overrides all layers when conditions change significantly
```

---

## 5. Simulation, Evaluation, and Continuous Improvement

### 5.1 Discrete-Event Simulation Framework

**Model Construction:**

Build a discrete-event simulation (DES) model of the Precision Parts shop floor, parameterized entirely from process mining outputs:

| Simulation Component | Process Mining Data Source |
|---|---|
| Job arrival process | Inter-arrival time distribution mined from Job Released events, segmented by product family and priority class |
| Job routings | Routing probabilities from discovered process model (including optional steps and rework loops) |
| Task processing times | Per-machine, per-job-family duration distributions (empirical or fitted parametric: lognormal, gamma) from Task Start/End events |
| Setup times | Sequence-dependent setup time matrix from analyzed setup events |
| Machine breakdowns | MTBF and MTTR distributions per machine from Breakdown Start/End events |
| Operator availability | Shift patterns and operator assignment rules from Operator ID and timestamp analysis |
| Priority change frequency | Empirical frequency and timing of hot job arrivals |
| Job mix | Product mix distribution (proportions of each job family/routing type) |

**Simulation Tool:** AnyLogic, Simio, or a Python-based framework (SimPy) integrated with the process mining pipeline (PM4Py for mining  SimPy for simulation).

**Validation:** Run the simulation with the **current dispatching rules** (FCFS/EDD) and compare simulated KPIs against actual historical KPIs from the mined data. The simulation must reproduce the observed:
- Mean and variance of flow times (within 10%)
- Mean tardiness and on-time delivery percentage (within 5 percentage points)
- Average WIP levels per work center (within 15%)
- Machine utilization rates (within 5%)

If validation metrics are not met, refine the input distributions or model structure.

### 5.2 Experimental Design for Strategy Comparison

**Baseline:** Current FCFS/EDD rules (validated against historical performance)

**Test strategies:**
1. Strategy 1: Composite dynamic dispatching (multiple weight configurations)
2. Strategy 2: Predictive scheduling with proactive rescheduling
3. Strategy 3: Setup-aware batching at bottlenecks
4. Strategy 1+3 combined
5. Strategies 1+2+3 integrated (full layered architecture)

**Scenarios to test:**

| Scenario | Description | Purpose |
|---|---|---|
| Normal load | Historical average demand | Baseline comparison |
| High load (+20%) | Increased job arrival rate | Stress test — how strategies perform under congestion |
| Low load (-20%) | Decreased demand | Verify no strategies create unnecessary batching delays when capacity is ample |
| Frequent disruptions | Double the historical breakdown frequency | Test robustness and disruption recovery |
| Hot job surge | Triple the frequency of urgent priority changes | Test responsiveness and preemption handling |
| Mixed stress | High load + frequent disruptions | Worst-case scenario |
| Seasonal pattern | Replicate observed monthly demand cycles | Test adaptation to changing conditions |
| Bottleneck shift | Remove the current bottleneck (add capacity); create a new one elsewhere | Test whether strategies are robust to bottleneck migration |

**Statistical rigor:**
- Each scenario × strategy combination: minimum 30 independent replications with different random seeds
- Warm-up period: discard the initial transient (e.g., first 2 simulated weeks) to reach steady-state
- Compute confidence intervals for all KPIs
- Use paired t-tests or ANOVA to determine statistical significance of differences between strategies
- Construct Pareto frontiers across multiple objectives (tardiness vs. WIP vs. setup time)

**Key KPIs for comparison:**
- Mean and P95 weighted tardiness
- On-time delivery percentage
- Mean and P95 flow time
- Average and peak WIP
- Total setup time
- Bottleneck utilization (effective vs. setup vs. idle)
- Throughput (jobs completed per unit time)
- Robustness metric: variance of KPIs across disruption scenarios

### 5.3 Sensitivity Analysis

Beyond the primary comparison, conduct sensitivity analysis on:
- **Strategy 1 weights:** How sensitive is performance to exact weight values? Identify the robust regions of the weight space.
- **Strategy 2 prediction accuracy:** What happens if the ML models degrade (e.g., concept drift)? Introduce controlled prediction errors and measure performance degradation.
- **Strategy 3 batching aggressiveness:** Vary the slack threshold that controls whether batching or urgency takes precedence. Find the sweet spot.

### 5.4 Continuous Improvement Framework

**Operational Monitoring Dashboard:**

Deploy a real-time process mining pipeline that continuously:

1. **Ingests MES events** as they occur (streaming process mining)
2. **Computes rolling KPIs** (hourly/daily/weekly):
   - On-time delivery rate (trailing 30-day window)
   - Mean flow time and flow factor
   - Setup time ratio per machine
   - WIP levels per work center
   - Queue time distributions
   - Bottleneck identification (current shift vs. historical)

3. **Detects concept drift and anomalies:**
   - **Statistical process control (SPC) on KPIs:** Apply CUSUM or EWMA control charts to each KPI. When a KPI breaches control limits, trigger an alert.
   - **Process drift detection:** Use process mining drift detection algorithms (e.g., comparing process model features between sliding time windows) to detect changes in:
     - Job routing patterns (new product types?)
     - Task duration distributions (tool degradation? new operators?)
     - Setup time patterns (material supply changes?)
     - Breakdown patterns (aging equipment?)

4. **Automated recalibration triggers:**
   When drift is detected:
   - **Duration prediction models (Strategy 2):** Retrain on the most recent N months of data using an expanding or sliding window approach. Compare prediction accuracy (MAPE, coverage of prediction intervals) before and after retraining.
   - **Setup time matrices (Strategy 3):** Update with new observations. If new job families emerge (clustering structure changes), reclassify.
   - **Dispatching weights (Strategy 1):** Re-run the simulation-based weight optimization with updated input distributions.
   - **Bottleneck assessment:** If the bottleneck has shifted (e.g., due to product mix change), redirect the setup optimization focus to the new bottleneck.

**Continuous Improvement Cycle:**

```

                                                                  
                    
     COLLECT     ANALYZE       DIAGNOSE             
    MES Event     Process Mining      Pathologies           
      Logs          + ML Models       Root Causes           
                    
                                                                
                                                                
                    
     DEPLOY     SIMULATE       DESIGN               
     Updated        & Validate        Improved              
     Strategy       New Strategy      Strategy              
                    
                                                                  
                  Cycle frequency: Monthly (routine)               
                  or Triggered (on drift detection)               

```

**Governance:**
- **Monthly review:** Operations team reviews the KPI dashboard, comparing current period vs. previous period and vs. targets. Process mining reports highlight any new variants, emerging bottlenecks, or degrading KPIs.
- **Quarterly deep-dive:** Full process mining analysis (conformance checking, bottleneck analysis, setup optimization review). Retrain all ML models. Re-validate simulation model against recent data. Adjust strategy parameters if needed.
- **Annual strategic review:** Evaluate whether the fundamental scheduling architecture is still appropriate given changes in product mix, capacity, or business strategy. Consider whether new data sources (IoT sensors, supplier lead times) should be incorporated.

**Technical Infrastructure:**

```
MES Event Stream  Event Log Database (e.g., Elasticsearch)
                      
              Process Mining Engine (Celonis/PM4Py/ProM)
                      
              
               Analytics Layer   
               - KPI computation 
               - Drift detection 
               - ML model serving
              
                      
              
               Decision Layer    
               - Priority scorer 
               - Predictive      
                 scheduler       
               - Batching engine 
              
                      
              MES  Shop Floor Dispatch Instructions
```

---

## Summary: Expected Aggregate Impact

Implementing the integrated three-strategy approach, informed by comprehensive process mining analysis, I project the following improvements over the current baseline (to be validated through simulation before deployment):

| KPI | Current (Estimated) | Target (Post-Implementation) | Improvement |
|---|---|---|---|
| On-Time Delivery | ~62% | ~88–92% | +26–30 pp |
| Mean Weighted Tardiness | ~3.2 days | ~0.8–1.2 days | 60–75% reduction |
| Mean Flow Time | ~8.5 days | ~5.5–6.5 days | 24–35% reduction |
| Flow Time CV | ~0.75 | ~0.40–0.50 | 33–47% reduction |
| Average WIP | ~145 jobs | ~95–110 jobs | 24–34% reduction |
| Total Setup Time (bottleneck) | ~38% of capacity | ~22–28% of capacity | 26–42% reduction |
| Effective Bottleneck Utilization | ~58% | ~68–74% | +10–16 pp |

These estimates are directional and will be refined through the simulation-based evaluation described in Section 5. The continuous improvement framework ensures that gains are sustained and extended as the system learns from ongoing operations.