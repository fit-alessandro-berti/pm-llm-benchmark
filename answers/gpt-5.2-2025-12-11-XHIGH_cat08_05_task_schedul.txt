## 1) Analyzing Historical Scheduling Performance and Dynamics (Process Mining on MES Logs)

### 1.1. Event log preparation (make the log “schedulable”)
Your MES already logs most of what we need, but for high-quality analysis we typically normalize into a *lifecycle* event log:

- **Case notion:** `Case ID = Job ID`
- **Activities:** map to operation names (Cutting, Milling, Grinding, Heat Treat, Inspection, …) and also keep explicit states:
  - `Queue Entry (Op)`
  - `Setup Start/End (Op)`
  - `Task Start/End (Op)`
  - Optional: `Move`, `Hold`, `Rework`, `Scrap`
- **Resources:** `Machine ID` and (if relevant) `Operator ID`
- **Business attributes:** priority, due date, customer, material, geometry class, routing family, etc.
- **Disruption events:** `Breakdown Start/End`, `Priority Change`, `Rush Release`, etc.

Key derived timestamps/intervals per operation instance:
- **Queue time** = `Setup Start` (or `Task Start` if no setup)  `Queue Entry`
- **Setup time** = `Setup End`  `Setup Start`
- **Run time** = `Task End`  `Task Start`
- **Touch time** = setup + run
- **Flow time contribution** = queue + touch (plus any holds/moves)

Also create machine timelines by ordering all `Setup/Task` intervals by machine and time (this is critical for sequence-dependent setup mining).

---

### 1.2. Reconstruct the *actual* job flows (process discovery + enhancement)
Job shops create “spaghetti” if you try to force a single end-to-end model. The practical approach:

**A) Discover routing families / variants**
- **Variant mining**: group jobs by *operation sequence* (e.g., CuttingMillingGrindingInspection vs CuttingLatheHeat TreatGrindingInspection).
- Also cluster jobs by *engineering attributes* (material/tooling family), because these strongly influence setups and durations.

**B) Process discovery per family (or per product line / customer segment)**
Use:
- **Directly-Follows Graph (DFG)** with frequency + performance overlays (median waiting/processing on edges).
- **Inductive Miner** (structured Petri net/BPMN) for interpretable models.
- **Heuristics Miner** when noise and optional steps are common.

**C) Performance enhancement**
Overlay on the discovered model:
- Median/mean/p95 waiting times per operation
- Rework loops frequency
- Service level by variant (on-time vs late)

This yields an evidence-based view of:
- What routes actually happen (not what routings say should happen)
- Where time is spent (queue vs setup vs run)
- Which variants are disproportionately late or WIP-heavy

---

### 1.3. Quantify core metrics (with process mining “performance” analytics)

#### (i) Job flow times, lead times, makespan
- **Job lead time**: `Job Complete`  `Job Released`
- **Job flow time decomposition**:
  - Total queue time (sum of all queue intervals)
  - Total setup time
  - Total run time
  - Holds/rework
- Report distributions: mean/median, p90/p95, and *tail behavior* (late jobs usually live in the tail).
- **Makespan** (useful per planning bucket): for each week/day:
  - `max(completion)`  `min(release)` for the set, or per “campaign”/batch.

Process mining tools typically provide throughput histograms; you add segmentation by:
- routing family
- priority class
- customer segment
- “touch labor” intensity
- bottleneck usage (jobs that require GRIND-01 etc.)

#### (ii) Task waiting times (queue times) per work center/machine
Compute per operation instance:
- `queue_time(op, machine)` as defined above.

Then derive:
- Queue time distributions by machine (median, p95)
- **Queue time heatmaps** by hour-of-day / shift / day-of-week
- **Arrival vs start lag**: how long after entering a queue a job actually begins setup/run

In process mining, this is classic *performance analysis*—but for job shops you also want:
- queue time by **job criticality** (slack) at time of queue entry
- queue time by **priority level**
- queue time by **next-step bottleneck** (upstream decisions often starve/overfeed downstream)

#### (iii) Resource utilization (machine/operator): productive, idle, setup, down
From machine timelines, label states:
- **Setup**
- **Run**
- **Idle** (available, no job)
- **Down** (breakdown/maintenance)
- Optionally **Blocked** (cannot unload) or **Starved** (no WIP)

Metrics:
- Utilization = (setup + run) / calendar time
- Setup ratio = setup / (setup + run)
- OEE-like breakdown: Availability (1  downtime), Performance (actual vs planned run), Quality (if scrap/rework is logged)
- Operator loading and multi-machine interference (operator as shared resource)

This shows whether lateness is caused by:
- true capacity shortage (high utilization and persistent queues)
- or poor sequencing/dispatch (high setups, high idle-with-WIP elsewhere)

#### (iv) Sequence-dependent setup times (log-based setup matrix + drivers)
Because setups depend on the *previous job on the same machine*, you mine setups on each machine as follows:

1. For each machine, sort all executed jobs by `Setup Start` (or `Task Start` if setup missing).
2. For each setup event, identify:
   - `prev_job` = job processed immediately before on that machine
   - `next_job` = the job being set up
3. Compute:
   - `setup_duration = Setup End  Setup Start`
   - Pair features: (prev_job attributes, next_job attributes), e.g. material, thickness, tool family, tolerance class, fixture, program, coolant type, etc.

Outputs:
- **Setup transition matrix**: average/median setup time from family Afamily B
- **Within-family vs cross-family** setup distributions
- **Model-based setup predictor**:
  - Regression/GBM to estimate setup time as a function of prev/next attributes
  - Useful when you don’t have clean “family labels” but do have rich job attributes

Also quantify *avoidable setup loss*:
- Compare actual setup total in a period vs a “best feasible” sequence (same jobs, minimal-changeover heuristic) to estimate the opportunity size.

#### (v) Schedule adherence and tardiness
At job level:
- **Lateness** \(L_i\) = \(C_i - d_i\)
- **Tardiness** \(T_i\) = \(\max(C_i - d_i, 0)\)
- **Weighted tardiness**: weight by priority/customer penalty
- **OTD (On-Time Delivery)**: % with \(C_i \le d_i\)

At operation level:
- If you have planned start/end, compute deviations. If not, still compute:
  - *internal promise accuracy*: compare “planned duration” vs actual for each task/setup
  - *schedule stability*: number of resequencing events, priority changes, preemptions

With process mining you can also measure **dispatching-rule conformance**:
- At each machine decision point, reconstruct the queue and check:
  - Did the chosen next job match EDD? FCFS? Priority-first?
  - If not, what was the implied rule (often “who shouted loudest”)?

This is powerful evidence when the shop believes it follows EDD/FCFS but behavior differs.

#### (vi) Impact of disruptions (breakdowns, hot jobs, priority changes)
Treat disruptions as time intervals/events and correlate with KPI deltas.

- For each breakdown on machine M:
  - downtime duration (MTTR) and time-between-failures (MTBF)
  - *lost capacity* = expected run/setup minutes that could have been processed
  - queue growth before/after breakdown (WIP shock)
- For hot jobs / priority changes:
  - measure “queue displacement”: how many jobs were overtaken
  - quantify tardiness increase for displaced jobs
  - measure instability: how often the bottleneck machine’s sequence changes

Analytically:
- Segment jobs into “affected by disruption” vs “not affected” and compare lead time/tardiness distributions.
- Run a difference-in-differences style comparison across similar weeks with/without major downtime.

---

## 2) Diagnosing Scheduling Pathologies (What’s Actually Going Wrong)

The process mining outputs above typically reveal several recurring job-shop pathologies. Here’s how to identify them with evidence.

### 2.1. Bottlenecks and their systemic impact
**Evidence pattern:**
- One or two machines show:
  - very high utilization (setup+run), low idle
  - persistent high queue times upstream of that machine
  - high WIP accumulation in its input buffer
  - jobs that require this machine have disproportionately high tardiness

**Process mining techniques:**
- Bottleneck analysis on the process map: highlight steps with highest waiting contribution.
- Resource performance dashboards: queue-time-by-machine Pareto.
- “Where is time spent?” decomposition for late vs on-time jobs.

**Quantification:**
- If 60–80% of job lead time is waiting and most waiting concentrates at a few machines, the problem is primarily *scheduling/flow control around bottlenecks* (even if capacity is tight).

---

### 2.2. Poor prioritization (local rules defeating global due-date performance)
**Evidence pattern:**
- High-priority jobs spend comparable or even *longer* time waiting than low-priority jobs.
- Jobs close to due date are not consistently accelerated.
- Hot jobs cause large collateral tardiness (expedite spiral).

**Techniques:**
- Compare queue times and start delays by priority and due-date slack at queue entry.
- Reconstruct dispatch decisions: at each “machine free” event, compute EDD/CR choice vs actual choice.
- Variant analysis: late jobs share patterns like “waited twice at Milling queue due to repeated hot-job insertions”.

**Outcome:**
- You’ll often find the shop is *not truly executing* its stated dispatching rules, or the rules are too simplistic and cause thrashing.

---

### 2.3. Suboptimal sequencing inflating total setup time (especially at bottlenecks)
**Evidence pattern:**
- High setup ratio on a bottleneck machine
- Many ABA family switches in short windows
- Setup time spikes correlate strongly with tardiness and WIP growth

**Techniques:**
- Setup transition matrix and frequency of family switches
- Compare actual setup total vs “family-batched EDD within family” heuristic baseline
- Correlation/causal analysis: setup-heavy days vs throughput and tardiness

**Interpretation:**
- Even without adding machines, reducing changeovers can “create capacity” where you need it most.

---

### 2.4. Starvation/Blocking (imbalanced flow and poor coordination across work centers)
**Evidence pattern:**
- Some downstream machines have significant idle time *despite* WIP existing elsewhere.
- Upstream bottleneck sequencing causes “lumpy” releases that starve downstream.
- Heat Treat queues explode while Inspection is idle (or the reverse).

**Techniques:**
- Multi-resource timeline: overlay machine states and WIP.
- “Starved time” estimation: idle while upstream WIP exists but not in the right routing state.
- Handover/transfer-time mining: delays between end of op A and queue entry to op B.

---

### 2.5. WIP bullwhip / variability amplification
**Evidence pattern:**
- Large swings in queue lengths and lead times week-to-week.
- Release patterns create peaks that overload bottlenecks.
- Expedite actions create further variability (re-sequencing cascades).

**Techniques:**
- WIP over time derived from event log (count jobs in each queue/processing state at time t).
- Control charts on WIP and cycle time.
- Compare stable vs unstable periods and the triggers (rush releases, breakdown clusters, order-release surges).

---

## 3) Root Cause Analysis: Why the Current Scheduling Is Ineffective

### 3.1. Static/local dispatching rules in a dynamic, coupled system
FCFS/EDD at each work center ignores:
- downstream bottleneck load
- sequence-dependent setup penalties
- stochastic processing times
- breakdown risk
- the fact that prioritizing locally can increase global tardiness (classic job-shop coupling)

Process mining differentiates this by showing:
- lots of time lost in queues (coordination failure)
- high setup losses due to frequent family switching (rule ignores changeover)
- displacement effects from hot jobs (no stability mechanism)

### 3.2. Lack of real-time global visibility and predictive lookahead
Even with good rules, if decisions are made without:
- current queue lengths
- true WIP distribution
- machine states (down, setup, running)
- predicted completion times

…then dispatching becomes reactive.

Process mining provides:
- a “digital truth” of real lead times by job family and routing
- real queue dynamics and bottleneck shifts over time
- early warning indicators (queue growth rates, utilization spikes)

### 3.3. Planning based on inaccurate durations and ignoring variability
If “planned durations” systematically understate reality or ignore:
- operator effects
- complexity differences
- setup variability by sequence
- rework probability

…then due dates and schedules become mathematically impossible.

Process mining quantifies:
- planned vs actual bias by machine/operator/job type
- heavy-tailed duration distributions (p95 >> mean)
- which steps have high variance and drive unpredictability

### 3.4. Ineffective handling of sequence-dependent setups
If the dispatching logic treats setup as fixed/irrelevant:
- it will frequently alternate between dissimilar jobs
- causing inflated setup time, reduced throughput, more WIP and tardiness

Process mining proves this via:
- high cross-family switch frequency
- measurable opportunity gap vs setup-minimizing sequences

### 3.5. Disruptions handled by firefighting rather than robust policies
Breakdowns and hot jobs are inevitable; the issue is *policy*:
- no time fences or rescheduling cadence
- no robust buffers at the bottleneck
- no formal expedite mechanism with bounded collateral damage

Process mining separates *scheduling logic issues* from *capacity limits* by:
- **Capacity feasibility checks:** If bottleneck utilization (including setups) is consistently near/above ~90–95% and due dates still missed even under best-case sequencing in simulation, you have a structural capacity problem.
- **Avoidable delay estimation:** If machines show idle time while jobs wait elsewhere, or setup losses are excessive, much tardiness is avoidable by better coordination/sequence decisions rather than adding capacity.

---

## 4) Advanced Data-Driven Scheduling Strategies (Beyond Simple Static Rules)

Below are three distinct strategies that directly leverage mined insights (durations, setup matrices, disruption patterns, bottleneck behavior). In practice, you often combine them: dispatching for execution + rolling rescheduling + explicit setup optimization at constraints.

---

### Strategy 1 — Dynamic multi-criteria dispatching (setup-aware, due-date aware, bottleneck-aware)
**Core logic:** Replace single-factor rules with a *scoring function* computed at each machine decision point, using mined estimates for remaining work, setup time, and tardiness risk.

A proven family of rules for this environment is **ATCS** (Apparent Tardiness Cost with Setup), extended with downstream load:

For each job \(i\) waiting at machine \(m\), compute:
- \(p_i\): predicted processing time on \(m\) (from log-based model)
- \(s_i\): predicted setup time if processed next, given current last job on \(m\) (from setup transition matrix/model)
- \(d_i\): due date
- \(w_i\): weight from priority/customer penalty
- \(R_i\): predicted remaining work content (sum of expected future ops + setups), mined from historical routings by family
- \(B_i\): downstream bottleneck congestion indicator (e.g., predicted queue time at next critical machine)

Example score (one workable form):
\[
Score_i = \frac{w_i}{p_i} \cdot \exp\left(-\frac{\max(d_i - t - R_i, 0)}{k_1 \bar{p}}\right)\cdot \exp\left(-\frac{s_i}{k_2 \bar{s}}\right)\cdot f(B_i)
\]
Pick job with highest score. Parameters \(k_1, k_2\) and the function \(f(\cdot)\) are tuned via simulation (Section 5).

**How process mining informs it:**
- \(p_i\) and \(R_i\): distributions and predictors from event logs (by machine, operator, job family).
- \(s_i\): sequence-dependent setup predictor from previous-jobnext-job transitions.
- \(B_i\): derived from mined queue dynamics and bottleneck identification.

**Pathologies addressed:**
- Reduces late jobs by explicitly prioritizing low slack / high urgency.
- Reduces setup thrashing by penalizing expensive changeovers.
- Reduces WIP blowups by including downstream congestion (prevents pushing work into already-blocked areas).

**Expected KPI impact (typical):**
- Lower mean/p95 tardiness
- Lower setup ratio on constrained machines
- More stable lead times (reduced variance)
- Often a WIP reduction due to fewer rework/expedite cascades

---

### Strategy 2 — Predictive rolling-horizon scheduling (stochastic times + disruption-aware rescheduling)
**Core logic:** Create a near-term schedule (e.g., next 1–3 shifts) using predicted durations and machine availability risk, then *recompute on a cadence* (e.g., every hour or on major events). This is a shift from purely local dispatching to a controlled “plan + execute + replan” loop.

**Key components mined from logs:**
1. **Processing/setup time prediction** (not just averages):
   - Use quantile regression or probabilistic models to get p50/p80/p95 durations.
   - Features: job complexity, material, tolerance class, operator, shift, rework likelihood.
2. **Breakdown models**:
   - MTBF/MTTR distributions per machine
   - Optional hazard model (risk increases with runtime, tool type, etc.)
3. **Routing likelihoods and rework loops**:
   - From variant mining: probability of needing additional inspection/rework steps by family.

**Scheduling method:**
- Rolling horizon optimization using CP-SAT/MILP/metaheuristics (job shop with sequence-dependent setups is NP-hard, so heuristics are common).
- Objective: minimize weighted tardiness + setup + WIP proxies, subject to:
  - machine capacity and calendars
  - predicted breakdown windows (or risk buffers)
  - time fences (stability constraints so the plan isn’t rewritten every minute)

**Pathologies addressed:**
- Unpredictable lead times: replaced by probabilistic ETA and risk-aware planning.
- Disruptions: schedule is robustified (buffers) and replanned systematically rather than firefighting.
- Poor coordination: explicitly synchronizes across work centers, not just locally.

**Expected KPI impact:**
- Improved due-date promise accuracy (fewer “surprises”)
- Lower expedite frequency
- Better bottleneck protection (fewer last-minute sequence breaks)
- Better customer quoting using probabilistic completion dates (e.g., “90% confidence by date X”)

---

### Strategy 3 — Setup time optimization on bottlenecks (family batching + sequence optimization + DBR)
**Core logic:** Treat the true bottleneck machines as “drums” and explicitly optimize their sequences to reduce sequence-dependent setup loss while still meeting due dates. Then synchronize upstream releases to that sequence (buffer + rope).

**Step A: Build setup families and transition costs**
From the mined setup matrix/model:
- define families where within-family setups are low and cross-family setups are high
- quantify AB changeover cost (median setup duration)

**Step B: Optimize the bottleneck sequence**
For each planning horizon (e.g., daily):
- Solve a **single-machine sequence-dependent setup + weighted tardiness** problem for the bottleneck queue (and near-future arrivals).
- Practical heuristic that works well:
  1. Partition jobs into families.
  2. Within each family, order by due date (or ATC).
  3. Decide family order by “setup cost vs urgency” tradeoff (metaheuristic / local search).
  4. Allow limited “due-date exceptions” (jumping families) when slack is critically negative.

**Step C: Drum-Buffer-Rope / workload control**
- Create a time buffer before the bottleneck (protect it from starvation).
- Control release of new jobs so WIP doesn’t explode elsewhere:
  - release jobs when bottleneck workload falls below a target
  - or use CONWIP-like caps per routing family

**How process mining informs it:**
- Identifies the actual bottleneck(s) and how stable they are over time.
- Provides the setup transition costs and which attributes drive them.
- Quantifies the economic value of setup reduction vs tardiness reduction (tradeoff curve).

**Pathologies addressed:**
- Excessive setup time and capacity loss on bottlenecks
- WIP accumulation caused by uncontrolled releases
- Downstream starvation due to unstable bottleneck sequencing

**Expected KPI impact:**
- Increased effective bottleneck capacity (often the fastest route to throughput improvement)
- Lower WIP and shorter/less variable lead times (via controlled release)
- Reduced tardiness for jobs requiring the bottleneck

---

## 5) Simulation, Evaluation, and Continuous Improvement

### 5.1. Discrete-event simulation (“digital twin”) calibrated by process mining
Before deploying any new strategy, build a discrete-event simulation (DES) parameterized from the mined log:

**Inputs mined from MES logs**
- **Arrival/release patterns:** empirical inter-arrival distributions by customer/priority
- **Routings:** variant probabilities by family (or deterministic routing from ERP + rework probabilities mined)
- **Processing time distributions:** per operation/machine (and conditional on features)
- **Sequence-dependent setups:** setup transition matrix or predictive setup model
- **Breakdowns:** MTBF/MTTR distributions per machine; planned maintenance calendars
- **Operator constraints:** if operators are a limiting shared resource, model them explicitly
- **Transport/queue capacities:** if relevant

**KPIs to compare**
- OTD %, mean/p95 tardiness, weighted tardiness
- Mean/p95 lead time and its variance
- Average WIP and WIP by queue
- Bottleneck utilization and setup ratio
- Schedule stability (sequence changes per shift/day)
- Expedite count / priority change frequency

**Statistical rigor**
- Run multiple replications per scenario (stochasticity matters)
- Use confidence intervals and hypothesis tests to ensure improvements are real, not noise

---

### 5.2. Scenarios to stress-test (what you should simulate)
At minimum, test:

1. **Baseline (current dispatch rules)** vs each proposed strategy
2. **High load** (e.g., +10–20% release rate): reveals whether improvements are robust or only work with slack
3. **High disruption regime**:
   - increased breakdown frequency on bottleneck machines
   - bursts of hot jobs (e.g., 2–3 urgent releases per day)
4. **High setup complexity mix**: more cross-family transitions; checks sequencing robustness
5. **Operator shortage / skill constraints**: if operators drive performance variability
6. **Due-date tightening**: what happens if customers demand shorter lead times?

This yields a decision-ready comparison: not just “better on average,” but “better under the conditions that hurt us most.”

---

### 5.3. Continuous monitoring + adaptive improvement using ongoing process mining
After deployment, treat scheduling as a controlled system with feedback:

**A) Live KPI dashboards from event streams**
- OTD and tardiness (by priority and customer)
- Queue times and WIP by work center
- Bottleneck setup ratio and utilization
- Expedite count and schedule instability

**B) Drift detection**
Automatically detect when the process changes such that the scheduler needs retuning:
- Processing time drift (new materials, new tooling, operator changes)
- Setup matrix drift (new fixtures/programs)
- Breakdown pattern drift (aging equipment)

Methods:
- Statistical tests / control charts on key distributions
- Online drift detectors (e.g., Page-Hinkley/ADWIN on cycle time residuals)
- “Model performance monitoring” for predictive duration models (MAPE by segment)

**C) Closed-loop tuning**
- Periodically re-fit duration/setup predictors from the newest logs
- Re-tune dispatching weights \(k_1, k_2\) via simulation optimization
- Re-identify bottlenecks quarterly (they can shift as mix changes)

**D) Governance**
- Time fences: limit how often schedules can be changed unless major disruption occurs
- Exception workflows: define what qualifies as a hot job and how much displacement is allowed
- A/B testing by shift or cell when feasible to validate changes safely

---

## Bottom line
Process mining turns the MES log into a factual model of:
- actual routings and variants,
- where time is truly lost (queues vs setups vs runs),
- which machines are genuine constraints,
- how sequence-dependent setups behave in reality,
- and how disruptions propagate.

From that, you can implement and *prove* improvements via:
1) setup-aware, due-date-aware dynamic dispatching,  
2) predictive rolling-horizon scheduling with disruption risk, and  
3) explicit setup-optimized bottleneck sequencing + controlled release,  
validated through a calibrated discrete-event simulation and sustained with continuous process-mining-driven monitoring and retuning.