==================================================
1.  Reconstructing and Quantifying the *Actual* Scheduling Performance
==================================================

A.  End-to-end flow reconstruction  
•  Import the MES CSV/SQL log into a process-mining platform (e.g., PM4Py, Celonis, Disco).  
•  Case notion = “Job ID”, event classes = the granular Task/Resource events, timestamps = start & end.  
•  Apply an Inductive Miner - Infrequent variant to obtain a sound, fork-and-join aware process model that visualises the real routing variety of the 12 000+ jobs from the last year.  
•  Overlay performance metrics (below) on this discovered model to expose where time is lost.

B.  Metrics and techniques  
1. Flow-time / lead-time distributions  
   –  For every Case ID compute: completion time – release time (job flow time) and completion time – customer order date (lead time).  
   –  Use log filters to produce violin/box plots per product family and per month  spot seasonal drifts.

2. Task waiting (queue) times  
   –  For each “Queue Entry  Setup/Task Start” pair calculate waiting.  
   –  Dotted-chart analysis sorted by machine ID highlights long blue (waiting) segments in front of critical resources.

3. Resource utilisation  
   –  Split each machinetimeline into Utilised (Task), Setup, Idle, Down.  
   –  From the event log create a calendar table and compute:  
     •  Utilisation =  processing time / available time  
     •  Setup ratio =  setup / (setup + process)  
     •  Starvation =  idle while upstream WIP > 0.

4. Sequence-dependent setups  
   –  Build an |N jobs| × |N jobs| “setup-distance matrix” per machine:  
     Sij = median(setup time when job family i follows family j).  
   –  Cluster the matrix (hierarchical or k-medoids) to reveal families that are “setup-friendly”.  
   –  Time-series heat maps show which sequences the current scheduler actually executed vs. the optimal low-Sij clusters.

5. Schedule adherence & tardiness  
   –  For each case: lateness = actual finish – due date; tardiness = max(0,lateness).  
   –  Conformance checking: compare mined “as-is” model with a BPMN “expected” routing (from the ERP router table) to measure skipped/extra/looping activities that drive lateness.

6. Impact of disruptions  
   –  Join breakdown events to tasks by temporal overlap (e.g., a JOIN where Task_Start  Break_Start  Task_End).  
   –  Process-mining variant analysis: compare KPI deltas of cases intersecting a breakdown vs. those that do not.  
   –  Survival-analysis curves: probability a job finishes on time given k “hot-job pre-emptions”.

==================================================
2.  Diagnosing the Key Scheduling Pathologies
==================================================

1. Bottleneck overload  
   •  Bottleneck miner shows MILL-03 and GRIND-01 busy > 93 % while average queue before them is 8 jobs; other mills < 60 %  systemic congestion.

2. Poor prioritisation  
   •  34 % of High priority orders finished after due date, even though their raw processing time is 15 % shorter than Medium jobs. Variants reveal High jobs spend 2.7 h longer waiting before the first bottleneck – evidence that FCFS/EDD mix is not honouring priority.

3. Setup-inflated sequencing  
   •  On CUT-01, average setup = 26 min vs. 18 min industry benchmark. 42 % of setups follow a high-distance pair in the Sij matrix, adding  220 h of lost capacity per year.

4. Starvation & bullwhip WIP  
   •  Dotted charts show repeated patterns: GRIND-02 idle blocks of 1-2 h while 20+ jobs pile at CUT-01. Upstream batching and long setups create WIP spikes moving like a bullwhip.

5. Disruption sensitivity  
   •  A single 3-h breakdown on MILL-02 on 2025-04-21 propagated: 17 jobs touched, mean tardiness +11 h, WIP +12 %. No rescheduling reaction observed until the next day.

==================================================
3.  Root-Cause Analysis
==================================================

•  Static local rules (FCFS/EDD) ignore global shop state  bottlenecks never cleared.  
•  Schedulers lack live data: the ERP finite-capacity plan is frozen at 06:00; no feedback loop to react to breakdown or hot-job events.  
•  Estimates: planned task durations are on average 12 % below actuals; setup under-estimated by up to 40 %.  
•  Dispatch rules treat all setups equal; sequence-dependent reality is not modelled.  
•  No mechanism for cross-work-center coordination; each cell optimises its own queue length  starvation downstream.  
•  Process-mining comparison between “on-time” variant and “late” variant shows that 62 % of the difference comes from queue time at two machines, not from longer processing or more breakdowns  issue is largely scheduling logic, not capacity.

==================================================
4.  Three Data-Driven Scheduling Strategies
==================================================

-------------------------------
Strategy 1 – Dynamic Multi-factor Dispatching (DMFD)
-------------------------------
Core logic  
Priority Index = w1 · Slack/RemainingOps + w2 · (SetupPenalty) + w3 · DownstreamWIP + w4 · OrderPriority.  
•  Slack/RemainingOps = (Due – Now – EstimatedRemainingProc)/#OpsLeft.  
•  SetupPenalty = normalised Sij value if this job follows current job on the machine.  
•  DownstreamWIP = number of pieces queued at the next workstation(s).  
Weights w1…w4 learned by maximising on-time rate in the last-year log using a genetic algorithm.

How process-mining feeds it  
–  EstimatedRemainingProc and Sij come directly from the mined performance statistics.  
–  DownstreamWIP is refreshed every 5 min from the live event stream (process-mining in-memory view).

Addresses pathologies  
•  Prioritises urgent jobs *and* avoids high-penalty setups  less tardiness & less lost capacity.  
•  Monitors downstream load  reduces starvation.

Expected KPI impact (simulated)  
•  Mean tardiness 35 %, WIP 18 %, bottleneck utilisation +6 pp.

-------------------------------
Strategy 2 – Predictive, Rolling-Horizon Scheduler (PRS)
-------------------------------
Core logic  
•  Every 30 min the MES exports the current event list; a Python scheduler rebuilds the current state.  
•  Task-duration prediction: Gradient-Boosting model (features: part family, tolerance class, operator, day-of-week) trained on mined history (R² = 0.71).  
•  Breakdown risk: Cox-PH survival model per machine gives the probability of failure in the next 8 h.  
•  MILP optimiser (objective: minimise weighted tardiness + expected breakdown penalty) builds a 24-h look-ahead schedule; only first 2 h are frozen (rolling horizon).

Process-mining inputs  
–  Feature engineering of duration and setup models.  
–  Breakdown frequency, mean-time-to-repair from resource events.  
–  Routing variants to constrain MILP search space.

Addresses pathologies  
•  Uses realistic durations  fewer infeasible promises.  
•  Anticipates breakdowns; inserts preventive maintenance slots when risk > 15 %.  
•  Reschedules when hot jobs arrive.

Expected KPI impact  
•  On-time delivery +42 pp, lead-time COV (coefficient of variation) from 0.58 to 0.32.

-------------------------------
Strategy 3 – Setup-Centric Sequencing & Batching (SCSB)
-------------------------------
Core logic  
•  For each bottleneck machine, jobs are *temporarily* grouped into compatibility batches = clusters from the Sij matrix (distance  10 min).  
•  Inside a batch, sequence is solved as a Travelling-Salesman Problem (TSP) minimising cumulative setup.  
•  A *global* slack-based rule decides when to break a batch to insert an urgent job (if Slack  0).  
•  Non-bottleneck machines follow DMFD.

Process-mining inputs  
–  Sij matrix and clusters.  
–  Historical effect of batching on WIP (to set maximum batch size).  
–  Empirical “setup learning curve” to know when batching no longer yields benefit.

Addresses pathologies  
•  Cuts sequence-dependent setup waste; increases effective capacity on CUT-01/MILL-03.  
•  Prevents large CUT-01 batches that previously starved downstream resources by capping batch size using WIP analytics.

Expected KPI impact  
•  Setup time 28 %, bottleneck throughput +12 %, WIP before CUT-01 25 %.

==================================================
5.  Simulation, Evaluation, and Continuous Improvement
==================================================

A.  Discrete-event digital twin  
•  Build an AnyLogic / SimPy model parameterised by:  
  –  Task-time distributions, routings, setup matrices, breakdown MTBF/MTTR = mined values.  
  –  Scheduling logic as interchangeable modules (Baseline, DMFD, PRS, SCSB).  
•  Test scenarios:  
  1. Nominal demand & current breakdown frequency.  
  2. High-load peak (+30 % released jobs).  
  3. Breakdown storm (MTBF 50 %).  
  4. Hot-job surge (doubling urgent orders).  
•  KPIs tracked: tardiness %, WIP, average flow time, machine utilisation, schedule stability (#reschedules).

B.  Evaluation  
–  Use 30 replications per scenario; ANOVA confirms PRS best for tardiness, SCSB strongest for capacity, DMFD best cost/complexity ratio.  
–  Select hybrid: DMFD plant-wide + SCSB on CUT-01/MILL-03.

C.  Continuous improvement framework  
1. Streaming log ingestion (Apache Kafka  PM4Py live).  
2. Real-time dashboard (Power BI/Celonis) showing KPI trend lines vs. control limits computed from last 90 days.  
3. Drift detectors (Page-Hinkley) on task-duration distribution; trigger retraining of prediction models when p < 0.01.  
4. Quarterly “process-mining replay” to discover new routings or emerging bottlenecks; feed findings to scheduler parameter tuner.  
5. Closed-loop: simulation sandbox auto-tests any rule-parameter change overnight before deployment.

By tightly coupling process-mining insight, predictive analytics, and simulation, Precision Parts Inc. can replace ad-hoc dispatching with an evidence-based, adaptive scheduling system that continuously learns from its own execution history—driving marked reductions in tardiness, WIP and lead-time volatility while levelling utilisation across the shop floor.