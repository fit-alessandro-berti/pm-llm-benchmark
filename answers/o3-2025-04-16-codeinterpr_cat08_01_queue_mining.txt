----------------------------------------------------------------
1. Queue Identification and Characterization  
----------------------------------------------------------------  

a. Re-constructing the waiting time per patient  
1. Sort each visit (Case ID) chronologically.  
2. For every pair of consecutive activities A and B within the same visit, compute  

   Waiting time Queue(A  B) = Start(B) – Complete(A)  

   (If activities may overlap, ignore negative or zero values.)  
3. Store the result together with: Case ID, A, B, Patient-Type, Urgency, day-of-week, hour-of-day, and the resources that will execute B (they determine the queue).  

b. Distinguishing “queue” from “service”  
• Service time (processing time) = Complete – Start inside one activity.  
• Queue time = time between activities (patient is idle, next resource occupied elsewhere).  
• Total lead time = (service) + (queue).  Only the queues are the target.  

c. Metrics per named queue (e.g. “Registration  Nurse-Assessment”)  
• Volume: # of visits that pass the queue.  
• Avg queue time.  
• Median queue time (robust to outliers).  
• 90th / 95th percentile queue time.  
• Maximum queue time.  
• Share of visits whose queue time > clinical target (e.g. > 15 min).  
• Coefficient of variation (std ÷ mean) to assess volatility.  
• Resource utilisation of the resource that serves the activity B at the moment the queue forms (from event log).  

d. Detecting the critical queues  
Rank queues using a composite “Pain Score”, for example:  
Pain = (Avg wait ÷ clinic target) × Volume-weight + (P90 wait ÷ target) × 0.5.  
Flag queues whose Pain is in the top 20 % OR where 30 % of patients exceed the SLA.  
Stratify the ranking by patient type and urgency; a queue that mostly hurts urgent or new patients is prioritised even if its overall average is lower.  

----------------------------------------------------------------
2. Root Cause Analysis  
----------------------------------------------------------------  

Use additional process-mining perspectives on the SAME event log.  

a. Resource bottlenecks  
• Resource-workload chart: % of time each clerk/nurse/physician/equipment is busy (derived from overlapping service intervals).  High utilisation (>85 %) when queues form  capacity issue.  
• Synchronisation view: when two resources must be free at once (e.g. technician + Room 3).  

b. Hand-over and dependency issues  
• Handover-of-work matrix: count how often Resource X finishes A and Resource Y starts B.  Large scatter indicates inconsistent routing, which causes uncertainty and queue build-up.  

c. Service-time variability  
• Activity duration histogram.  Long tail on doctor consultations will block the downstream diagnostic queue no matter the average.  

d. Appointment scheduling patterns  
• Overlay arrival curve (planned vs. actual) with resource occupancy curve (“performance spectrum” in PM4Py).  See peaks at 09:00, 13:00 that exceed capacity.  

e. Patient mix and urgency  
• Variant analysis: compare flow of “Urgent” vs. “Normal”; if urgent visits jump the line in front of scheduled patients, the latter’s queue explodes.  

f. Data-supported hypotheses  
Example findings:  
• Registration  Nurse queue average = 18 min, P90 = 35 min.  Resource analysis: only 1 triage nurse 09:00-11:00, utilisation 93 %.  
• Nurse  Doctor(Cardiology) queue average = 28 min, but only on Tue/Thu > 40 min.  Doctor Smith runs 30 % longer consults than peers.  
• Doctor  ECG queue P90 = 50 min because only one ECG room; utilisation 80 %, but clustering at 10:00 and 14:00 caused by physicians ordering ECG at the end of consult.  

----------------------------------------------------------------
3. Data-Driven Optimisation Strategies  
----------------------------------------------------------------  

Strategy 1. Dynamic triage staffing (“Registration + Nurse combined front-desk”)  
• Targets: Registration  Nurse queue.  
• Root cause: single triage nurse is bottleneck 09:00-11:00.  
• Data support: utilisation 93 % during peak; outside peak 60 %.  
• Action: Cross-train two clerks; when queue length > 5, clerk logs into triage role for quick vitals.  
• Expected impact: Simulation with current arrival data shows avg queue drops from 18  8 min, P90 35  15 min (55 % improvement) without hiring staff.  

Strategy 2. Demand-sensitive appointment template (“load levelling”)  
• Targets: Nurse  Doctor (all specialties) and downstream diagnostic queues.  
• Root cause: arrival peaks created by booking all first appointments on the hour.  
• Action: Build scheduling rule that caps new-patient starts to 4 per 15 min slot per specialty and pushes follow-ups to shoulder times. Use the event log to fit a discrete-event simulation and set caps so predicted utilisation stays below 85 %.  
• Expected impact: Cuts peak doctor wait by 30-40 %, overall visit duration drops 12 %. No additional labour cost; may shift some visits 10-15 min later in the day.  

Strategy 3. Parallel diagnostics pathway (“ECG before Doctor”)  
• Targets: Doctor(Cardiology)  ECG queue.  
• Root cause: serial order; ECG always requested after consult, causing batch at 10:00 / 14:00.  
• Action: Use predictive rule (from historic data) that 82 % of Cardiology new patients eventually need ECG. Pre-book ECG slot immediately after check-in; cancel if doctor decides it is not needed.  
• Resources: same ECG tech, but utilisation moves from two peaks to flatter curve.  
• Expected impact: DoctorECG average wait falls from 24  7 min; doctor gets ECG result sooner, allowing faster discharge. Simulation shows total visit time for affected patients down 18 %.  

(Optional additional strategies: extend ECG operating window by 30 min; introduce virtual check-out kiosk, etc.)  

----------------------------------------------------------------
4. Trade-offs and Constraints  
----------------------------------------------------------------  

• Strategy 1 may increase registration processing time when clerk multitasks – monitor registration SLA to ensure it is not degraded.  
• Strategy 2 can reduce patient scheduling flexibility and may need patient-communication effort; risk of lower utilisation late in day if no-shows not back-filled.  
• Strategy 3 could waste ECG capacity on the 18 % who ultimately cancel; acceptable if ECG room utilisation remains <85 %.  
• Shifting queues downstream: fixing triage may expose Radiology as new bottleneck; therefore apply plan-do-check-act cycle and re-mine logs after each change.  
• Staff satisfaction and overtime must be checked; propose time-tracking and small pilots before full roll-out.  
• Quality of care must remain paramount; any speed-up is applied only to waiting, not to consultation time.  

Balancing objectives: use multi-criteria decision matrix scoring each strategy on (wait-time benefit, cost, patient satisfaction, staff impact). Pilot the top scorers, measure, then scale.  

----------------------------------------------------------------
5. Measuring Success (Post-implementation)  
----------------------------------------------------------------  

Core KPIs (auto-extracted daily / weekly from ongoing event log):  
1. Avg and P90 queue time per critical queue.  
2. Total visit lead time (door-to-door) per patient type.  
3. % of visits meeting clinic SLA (e.g. 80 % finish within 2 hrs).  
4. Resource utilisation per staff group and room.  
5. # of visits re-scheduled or overflowed outside hours (proxy for cost).  
6. Patient-reported waiting-time satisfaction (survey).  

Monitoring approach  
• Re-run the queue-mining script nightly; feed KPIs into a dashboard.  
• Control charts to detect drift (e.g., average queue time exceeds upper control limit three days in a row).  
• Quarterly re-discovery of the process model to check for unintended path changes (variant explosion).  
• Combine with target-setting: e.g., trigger alert if P90 Nurse  Doctor wait > 25 min for two consecutive weeks.  

Sustained improvement cycle  
1. Detect deviation via dashboard.  
2. Drill down with resource- and social-network analysis to locate new bottleneck.  
3. Propose incremental fix.  
4. Log the change and start next monitoring cycle.  

By iteratively mining and intervening, the clinic can turn raw event data into a continuous-improvement engine, steadily reducing waiting times while keeping costs and care quality in balance.