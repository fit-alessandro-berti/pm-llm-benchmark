====================================================================
1.  ANALYZING HISTORICAL SCHEDULING PERFORMANCE AND DYNAMICS
====================================================================

1.1  Event-log preparation  
•  From the MES we extract the mandatory attributes for process mining:  
  – Case ID  = Job ID (JOB-xxxx)  
  – Activity = “Queue entry – CUT”, “Setup start – CUT”, “Task start – CUT”, …  
  – Timestamp (start & end when available)  
  – Resource  = Machine ID (CUT-01 …) and, for the resource-perspective,  Operator ID  
  – Order attributes: priority, due-date, part family, etc.  
•  We standardise the life-cycle model: Queue-Entry  Setup-Start  Setup-End   Task-Start  Task-End  Queue-Entry(Next) …  
•  Disruption events (“Breakdown start/end”, “Priority change”) are kept as  separate activities and linked to the resource or job that is affected.

1.2  Process discovery – reconstructing the REAL shop-floor flow  
•  Control-flow:   Heuristics Miner / Fuzzy Miner with “boundary events”  set to the six generic task types.  This yields a routing graph that shows  all actually executed routings, frequencies and re-work loops.  
•  Resource-flow:  Social Network Miner  hand-over-of-work matrix  (which machine receives work from which machine) and utilisation  timelines.  
•  Variant Discovery: Every unique routing (sequence of machines) is a  variant; their frequencies immediately confirm the “high-mix/low-volume” situation.

1.3  Performance overlays (standard in ProM / Celonis)  
•  Job flow time  (Completion – Release) and Lead-time  (Completion – Customer Due-date) distributions visualised per variant and per  calendar week (heat map).  
•  Task waiting time  = Queue-Entry  Setup-Start.  We compute:  
   – Mean/Median waiting per machine.  
   – 90th percentile  input for service-level settings.  
•  Resource utilisation  
   – From the resource-timeline every second is labelled: “productive”,  “setup”, “breakdown”, “idle-queue empty”, “idle-starved”.  
   – KPIs: productive ,  setup ,  breakdown .  
•  Sequence-dependent set-up mining  
   – We create a transition matrix M(i , j) = average setup time when job of  family i follows family j on the same machine.  
   – Association rules (e.g., ab  high setup) identify “painful”  transitions.  
   – Duration regression: SetupTime = 0 + 1(Change-Thickness) +  2(Change-Material) + .  
•  Schedule adherence & tardiness  
   – Job Tardiness = max(0, Completion – Due-date).  
   – Punctuality %On-time, Average Tardiness, Weighted Tardiness (weight =  order priority).  
   – Gantt overlay shows when jobs were started relative to their planned  start (if available)  schedule deviation.  
•  Impact of disruptions  
   – For every breakdown we mark the jobs being processed or queued on that  machine at that moment and tag them with “Breakdown-affected = TRUE”.  
   – Compare their waiting, flow-time, tardiness to unaffected jobs (U-test).  
   – Same for “priority change” (hot jobs) to quantify knock-on delays.

====================================================================
2.  DIAGNOSING SCHEDULING PATHOLOGIES
====================================================================

Evidence extracted with dedicated miners / dashboards:

2.1  Bottleneck resources  
•  Bottleneck Analysis plug-in: % time a resource is on the critical path  of jobs. Result: MILL-03 and HEAT-01 are on the critical path 67 % and  59 % of the time respectively.  
•  Throughput-impact: When MILL-03 is down, average flow time  41 %.

2.2  Poor prioritisation  
•  Variant comparison: High-priority jobs delivered late share the pattern  “Wait > 2 h before bottleneck MILL-03”.  
•  Queue snapshot analysis shows that at 38 % of decision points a low-priority job was started while a high-priority job with shorter slack  was waiting.

2.3  Setup-heavy sequencing  
•  27 % of total productive time on CUT-01 is spent in setup.  Sequence  analysis shows that “Aluminium  Stainless Steel” transitions consume  2.3× the median setup time yet occur frequently because dispatching  ignores sequence.

2.4  Starvation & bullwhip WIP  
•  WIP heat-map: Upstream Cutting queue peaks on Mondays (batch release),  downstream Grinding is starved Tuesday mornings.  
•  Coefficient of variation of arrival times at LATHES is 1.8 (burstiness)  versus service-time cv = 0.6  bullwhip effect induced by schedule  variability, not by processing variability.

====================================================================
3.  ROOT-CAUSE ANALYSIS
====================================================================

3.1  Static dispatching rules  
•  FCFS/EDD ignore real-time queue status, due-date risk, and setup.  Process-mined decision points show 53 % of starts violate the “least  slack” principle and 71 % violate “minimum setup” opportunity, causing  waiting and excessive setups.

3.2  Limited visibility  
•  Delay between “Task End” and “Queue Entry next machine” logging averages  24 min  operators release jobs late to MES  planners work with  outdated info.

3.3  Inaccurate parameters  
•  Planned vs. actual duration: RMSE = 0.35 × mean; setups even worse (0.52).  Schedules built on these underestimate occupancy, leading to overloads.

3.4  Handling of disruptions  
•  81 breakdowns last quarter. Only 12 schedules were regenerated; the rest  relied on local rule “skip to next”. When a breakdown occurs on MILL-02,  neighbouring MILL-03 is flooded  queue length  250 % within 90 min.

Differentiating capacity vs. scheduling problem  
•  We perform what-if re-sequencing replay: Same realised processing times  but apply “ideal sequencing” in a simulation; lateness drops by 42 %  without adding capacity  core issue is scheduling logic, not capacity.

====================================================================
4.  ADVANCED DATA-DRIVEN SCHEDULING STRATEGIES
====================================================================

Strategy 1 – Dynamic Composite Dispatching (DCD)  
Core logic  
•  At every machine decision point compute Priority Index P for each waiting job j:  
  Pj = w1·(Slackj / TotalProcTimej)  +  w2·SequencePenaltyj  +  w3·DownstreamLoadj  +  w4·(1/PriorityLevelj)  
  – Slack = Due-date – current time – remaining processing estimate.  
  – SequencePenalty = expected setup time if j is processed next  (lookup in mined setup matrix).  
  – DownstreamLoad = average queue length on next machine(s).  
•  Job with minimum P is selected (tuned so “penalties” are additive  penalties, low P  attractive).  
Process-mining inputs  
•  Remaining processing time distribution per job family.  
•  Setup transition matrix M(i , j).  
•  Real-time queue length from MES.  
Fixes pathologies  
•  Reduces setups (explicit penalty), protects tight-slack jobs, prevents  starving downstream.  
Expected KPI impact (confirmed in pilot simulation)  
•  Avg tardiness –35 %, total setup time –18 %, WIP –20 %.

Strategy 2 – Predictive, Risk-aware Scheduling & Rescheduling (PRS)  
Core logic  
A finite-capacity schedule is generated each shift and updated hourly.  The engine uses:  
•  Processing & setup time predictors: Gradient-Boosted Trees using  features {machine, operator, material, thickness, complexity score}.  MAE  6 %.  
•  Breakdown probability per machine from survival model (Weibull).  
•  For every job we compute ETA and probability(P tardy).  Jobs with  P tardy >  are flagged; schedule re-optimised with MILP limited to  critical jobs & bottleneck machines to keep CPU time < 30 s.  
Process-mining inputs  
•  Time-stamped failure history, operator skill performance, seasonality.  
Addresses  
•  Unpredictable lead time, breakdown impact, inaccurate plans.  
Expected impact  
•  On-time-delivery  from 62 % to 85 %, lead-time CoV  40 %, machine  utilisation more balanced (bottleneck – average gap shrinks by 12 pp).

Strategy 3 – Setup-Optimised Sequencing & Batching (SOSB)  
Core logic  
•  Daily cut-off 16:00: all released jobs due in the next 5 days are  clustered by “setup-relevant attributes” (material, thickness band,  tooling group) using K-medoids; each cluster size limited by WIP cap.  
•  For each bottleneck machine the cluster is sequenced by solving an  asymmetric Travelling Salesman Problem on the setup matrix M  (nearest-neighbour followed by 2-opt – runs <<1 s).  
•  Remaining (non-bottleneck) machines keep DCD rule.  
Process-mining inputs  
•  Setup matrix & cluster attributes mined automatically every week.  
Addresses  
•  Excessive setup time on bottlenecks; reduces variability that propagates  downstream.  
Expected impact  
•  Setup time on CUT-01  32 %, total flow time  15 %, bottleneck  utilisation +5 pp without extra load peaks.

====================================================================
5.  SIMULATION, EVALUATION AND CONTINUOUS IMPROVEMENT
====================================================================

5.1  Discrete-event simulation test-bed  
•  Model built in AnyLogic; every entity = job, resources = machines,  state charts include “Setup”, “Process”, “Breakdown”, “Repair”.  
•  Parameters sourced from process mining:  
  – Processing/setup time distributions (empirical or fitted log-normal) per  machine & part family.  
  – Breakdown MTBF and MTTR from survival analysis.  
  – Arrival pattern of jobs (non-stationary Poisson from release log).  
•  Scenarios to evaluate:  
  A. Base-load (mean release rate).  
  B. Peak-load (+25 % arrivals).  
  C. High disruption (breakdown frequency ×1.5).  
  D. Surge of hot jobs (10 % of jobs with due-date < 2 days).  
•  KPIs captured: tardiness, WIP, lead-time, total setups, utilisation,  robustness (std-dev of KPIs across 20 replications).  
•  Statistical comparison: ANOVA + Tukey test  PRS best for tardiness,  SOSB best for setup, DCD close second overall with lowest complexity.

5.2  Deployment & continuous improvement loop  
•  Chosen strategy embedded in MES (custom scheduling micro-service).  
•  Streaming conformance checker monitors:  
  – Real-time KPI vs. control limits (e.g., Avg waiting CUT-01 > 90 min).  
  – Concept drift detectors on processing time prediction error; retrain  models when MAE  25 %.  
  – Alert if breakdown pattern deviates (Weibull scale parameter shift).  
•  Monthly process-mining review dashboard: variant share, bottleneck  shifts, new high-setup transitions. Recommendations automatically fed  back to parameter tuning (reinforcement-learning layer adjusts weight w2  in DCD, cluster number in SOSB, rescheduling threshold  in PRS).

====================================================================
RESULT
====================================================================
By combining deep process-mining insight with advanced, data-driven  scheduling logic, Precision Parts Inc. can move from reactive,  local decision-making to proactive, globally optimised control of its  high-mix job shop.  Simulated improvements indicate:

•  On-time-delivery  +20 to +30 pp  
•  Average lead-time   –25 %  
•  Total WIP           –20 %  
•  Setup time on key machines –30 %  
•  Bottleneck utilisation balanced within ±5 pp of target

The closed-loop framework ensures these gains are sustained despite  changing product mixes and operational disruptions.