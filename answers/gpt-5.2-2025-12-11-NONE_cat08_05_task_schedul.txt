## 1) Analyzing Historical Scheduling Performance and Dynamics (from MES logs)

### 1.1 Reconstruct the *actual* job flows and machine sequences
**Goal:** turn raw MES events into (a) per-job “case” traces and (b) per-resource “machine diaries” that reflect the true execution order (including setups, waiting, interruptions).

**Event log preparation (critical in job shops):**
- **Case notion:** `Case ID = Job ID` for job-flow analysis.
- **Resource notion:** `Resource (Machine ID)` and optionally `Operator ID` for resource-centric views.
- **Lifecycle mapping:** map events into a consistent lifecycle, e.g.  
  - `Queue Entry`  waiting start  
  - `Setup Start/End`  setup duration  
  - `Task Start/End`  processing duration  
  - `Breakdown Start/End`  downtime intervals  
- **Normalize activities:** unify labels (`Task Start (Cutting)`  activity = `Cutting`, lifecycle=`start`).
- **Concurrency & preemption:** detect if tasks are paused (missing in snippet) by inferring gaps or explicit pause/resume events if available.
- **Timestamps & ordering:** enforce a deterministic order for same-timestamp events (e.g., end before start).

**Two complementary process mining views:**
1. **Case-centric discovery** (job routing reality):
   - Apply **Heuristics Miner** / **Inductive Miner** to discover the “as-executed” routing patterns.
   - Because routings are highly variable, use:
     - **Directly-Follows Graph (DFG)** with frequency/performance annotations.
     - **Trace clustering** (e.g., by product family, complexity, or routing similarity) to avoid “spaghetti models.”
2. **Resource-centric reconstruction** (true machine sequencing):
   - Build a **resource Gantt** from the event log:
     - For each machine: order all `Setup Start/End`, `Task Start/End`, breakdown intervals.
   - This reveals sequence decisions, changeovers, idle gaps, and starvation.

### 1.2 Metrics and techniques to quantify performance

#### (a) Flow time, lead time, makespan distributions
- **Job flow time / lead time** (log-derived):
  - `Lead time = (Job Completed timestamp) – (Job Released timestamp)`
  - If “Job Completed” is not explicit: use last task end as completion proxy.
- **Flow time decomposition**:
  - `Lead time = (queue time) + (setup time) + (process time) + (interruption/downtime impact)`
- **Distribution analysis:**
  - Use histograms + quantiles (P50/P80/P95) per job type/priority/customer.
  - Segment by routing cluster to separate “inherently long” from “unexpectedly delayed.”

- **Makespan** (period-level):
  - For a week/month: `max(completion) - min(release)` across jobs in that horizon.
  - Track makespan vs WIP and load to detect congestion regimes.

**Process mining technique:** *Performance DFG* (time on edges) + *Case duration statistics*.

#### (b) Task waiting (queue) times at each work center/machine
- Waiting time per operation:
  - `Queue time(op) = Task Start(op) – Queue Entry(op)`
  - If queue entry missing: infer via `Task End(prev op)` to `Task Start(next op)` (careful: includes transport/hold).
- Compute:
  - Average/median/P95 queue time per machine, per shift, per priority.
  - **Queue time share** of lead time (major driver in job shops).

**Process mining technique:** bottleneck/performance analysis on activities; **queue heatmaps** by resource and time-of-day.

#### (c) Resource utilization: productive / idle / setup / down
For each machine (and optionally operator):
- **Busy processing time:** sum of task processing durations.
- **Setup time:** sum of setup durations.
- **Down time:** sum of breakdown intervals (and planned maintenance if tagged).
- **Idle time:** available time – (processing + setup + down).  
  (Define “available” by shift calendar and planned downtime.)

Key KPIs:
- **OEE-like decomposition** (without speed losses if not available):
  - Availability = (available – down) / available
  - Utilization = (processing + setup) / (available – down)
  - Setup ratio = setup / (processing + setup)
- **Starvation vs blockage indicators:**
  - Starvation: machine idle while upstream WIP is low / no eligible jobs available.
  - Blockage: upstream completes but downstream queue explodes (detected via WIP/queue events).

**Process mining technique:** *Resource analysis* + calendar-aware utilization.

#### (d) Sequence-dependent setup times (SDS): quantification from logs
Objective: estimate `SetupTime(machine, prev_job_attributes  next_job_attributes)`.

Steps:
1. For each machine, create an ordered sequence of executed jobs (from `Task End` times).
2. Identify setup intervals:
   - `Setup duration = Setup End – Setup Start`
   - Link each setup to:
     - the **next job** (the one whose task starts after setup)
     - the **previous job** on that same machine (the one that ended immediately before setup start)
3. Engineer “changeover features”:
   - Material, thickness, tooling family, part family, tolerance class, program ID, fixture type, etc.
   - If not in log, join from ERP/BOM/routing master data.
4. Build an SDS model:
   - Simple: lookup table by (prev_family, next_family)
   - Better: regression/gradient boosting predicting setup minutes from changeover features
   - Also compute variability (std dev / quantiles), not just mean.

Outputs:
- **Changeover matrix** per machine (heatmap): expected setup time from family AB.
- Identify **high-penalty transitions** and “setup killers.”

**Process mining technique:** this is “resource sequence mining” + data mining on derived transitions.

#### (e) Schedule adherence and tardiness
Even if no “planned schedule” exists beyond dispatching, you can measure:
- **Due-date performance:**
  - Tardiness = `max(0, completion – due_date)`
  - Earliness = `max(0, due_date – completion)`
  - % late jobs, average tardiness, P95 tardiness by priority/customer.
- **Operation-level lateness propagation:**
  - Compare each operation end vs an internal planned checkpoint (if available) or vs *latest allowable start* computed backward from due date using mined typical times.

If a formal schedule baseline exists (even daily):
- **Adherence**:
  - Sequence adherence on a machine: similarity between planned vs actual job order (Kendall tau / inversion count).
  - Time adherence: start-time deviation and completion-time deviation per operation.

**Process mining technique:** conformance checking (if schedule model exists) + lateness analytics.

#### (f) Impact of disruptions (breakdowns, priority changes, hot jobs)
Use event correlation:
- Build “disruption windows” around `Breakdown Start/End`, `Priority Change`, “Hot job released”.
- Quantify deltas:
  - Queue time increase on affected machine + downstream machines.
  - Tardiness increase for jobs in queue during breakdown.
  - WIP spikes following recovery (catch-up waves).
- Use causal-ish comparisons:
  - Compare similar load days with/without breakdowns.
  - Before/after priority change: how many jobs got overtaken, and tardiness shift.

**Process mining technique:** performance comparison over time windows + queue dynamics; “what-if” replay on mined simulation model (see §5).

---

## 2) Diagnosing Scheduling Pathologies (evidence-driven)

### 2.1 Bottlenecks and their true impact
**Evidence to extract:**
- Machines with highest:
  - utilization (processing+setup) near shift capacity,
  - queue time P95,
  - WIP accumulation in front of them,
  - downstream starvation when they are down.

**Process mining methods:**
- **Bottleneck analysis** (activity/resource with highest waiting contribution to lead time).
- **Throughput contribution**: correlate machine busy time and overall completions/week.
- **Drum identification**: machine whose schedule dominates completion dates (often heat treat, grinding, or a specialized CNC).

**Pathology:** local dispatching ignores system bottleneck; non-bottlenecks overproduce  WIP rises without improving throughput.

### 2.2 Poor prioritization (EDD/FCFS locally  global due-date performance)
Look for:
- High-priority or near-due jobs spending long time in queues behind low-priority work.
- Excessive overtaking and priority inversions.

**Process mining evidence:**
- **Variant comparison:** compare traces of on-time vs late jobs:
  - late jobs may show longer queues at specific machines, more rework/inspection loops, or more waiting after priority changes.
- **Queue discipline analysis:** infer actual dispatch rule behavior:
  - For each machine decision point, compare the chosen job vs alternatives available at that timestamp (requires queue state reconstruction).  
  - Compute how often the selected job minimized slack, setup, or due date.

**Pathology:** dispatching is myopic—optimizes a local metric (e.g., due date) but triggers large setups or blocks bottleneck capacity.

### 2.3 Suboptimal sequencing inflating setup time (SDS ignored)
Evidence:
- Large fraction of bottleneck time consumed by setups.
- High frequency of “expensive transitions” (from SDS matrix).
- Repeated alternation between incompatible families (ABA) indicating no batching.

**Process mining evidence:**
- **Transition mining** per machine: frequency × expected setup cost.
- **Setup Pareto:** top 10 transitions causing 60–80% of setup loss.
- Compare “good days” vs “bad days” on same machine to show controllability (same mix, different sequencing).

**Pathology:** local dispatch picks the “next” job without accounting for setup penalties and downstream impact.

### 2.4 Starvation and blocking across work centers
Evidence:
- Downstream machines idle while upstream queues are high (blocking) or upstream produces the “wrong mix” (starvation for specific job types).
- Heat treat or inspection queues oscillate (waves).

**Process mining evidence:**
- Time-series of:
  - queue length proxies (count of queue-entry minus task-start events),
  - machine idle gaps,
  - WIP between steps (tokens-in-state if you model with a Petri net).
- Identify synchronization points (e.g., inspection batches, heat treat lots).

**Pathology:** lack of coordination; dispatching does not consider downstream readiness/capacity.

### 2.5 WIP “bullwhip” due to variability and reactive expediting
Evidence:
- Frequent “hot jobs” cause preemption  other jobs become late  more expediting  instability.
- Wide lead time distribution with fat tail; WIP spikes follow disruptions.

**Process mining evidence:**
- Detect **expedite loops**: priority change  overtaking events  queue reshuffles.
- **Stability metrics:** variance of queue times and daily WIP; correlation with number of priority changes/breakdowns.

---

## 3) Root Cause Analysis (why the current approach fails)

### 3.1 Dispatching-rule limitations in a dynamic job shop
- FCFS/EDD at each machine optimizes *local* order, not the end-to-end job completion.
- Ignores:
  - sequence-dependent setup,
  - downstream bottlenecks,
  - slack/remaining work content,
  - disruption risk.

**Process mining contribution:** quantify decision quality at each dispatch point (chosen job vs best job under alternative objectives).

### 3.2 Lack of real-time global visibility and state estimation
In practice, schedulers lack a reliable view of:
- current queue composition by machine,
- machine health and expected downtime,
- WIP position of urgent orders,
- realistic remaining time to due date.

**Process mining contribution:** reconstruct “digital twin state” from events to show what *could* have been known at the time.

### 3.3 Inaccurate durations / setups used for planning
If planned durations are static or wrong:
- due-date promising is unreliable,
- slack calculations are wrong,
- schedules become fiction.

**Process mining contribution:**
- Compare planned vs actual by machine/operator/job family.
- Produce calibrated distributions and confidence intervals (not single-point estimates).

### 3.4 Ineffective handling of SDS and batching opportunities
- Without SDS awareness, the shop pays unnecessary setup tax, especially on bottlenecks.

**Process mining contribution:** identify controllable setup drivers and quantify how much tardiness is attributable to setup loss vs processing.

### 3.5 Breakdowns and hot jobs handled by ad-hoc expediting
- Reactive resequencing increases turbulence and WIP.

**Process mining contribution:** quantify “disruption amplification”: how one breakdown increases lateness downstream and triggers priority escalations.

### 3.6 Separating “bad scheduling” from “true capacity limits/variability”
Use decomposition:
- If a resource is near saturation even under optimal sequencing (high utilization, low idle, minimal setup), it’s a **capacity constraint**.
- If there is significant avoidable setup, high idle due to poor feeding, or large overtaking-induced queues, it’s **scheduling logic/coordination**.

**Process mining approach:**
- Counterfactual analysis via simulation (see §5): keep same arrivals and processing times, change scheduling policy; the improvement gap estimates “scheduling loss.”

---

## 4) Advanced Data-Driven Scheduling Strategies (beyond static rules)

Below are three distinct strategies designed to work together (you can pilot them independently).

### Strategy 1 — Dynamic multi-criteria dispatching with SDS + slack + bottleneck awareness
**Core logic (at each machine decision):**
Score each available job \(j\) for machine \(m\):
- **Slack / urgency:** \( \text{slack}_j = \text{due}_j - \text{now} - \widehat{RPT}_j \)  
  where \( \widehat{RPT}_j \) = predicted remaining processing + expected queues on critical steps.
- **Sequence-dependent setup penalty:** \( \widehat{setup}(prev\_job, j, m) \)
- **Downstream load penalty:** favor jobs whose next step is not currently overloaded, or that feed the bottleneck (“drum”).
- **Priority class:** hard constraints (urgent jobs must be within a service level) rather than always preempting.

Example scoring (illustrative):
\[
score(j)= w_1 \cdot \text{tardiness-risk}(j) - w_2 \cdot \widehat{setup}(prev,j,m) - w_3 \cdot \text{downstream\_congestion}(j) + w_4 \cdot \text{priority}(j)
\]

**How process mining informs it:**
- \( \widehat{setup} \) comes from the mined SDS model (transition matrix/regression).
- \( \widehat{RPT} \) comes from mined duration distributions by job family/operator/machine.
- Bottleneck identification and downstream congestion metrics come from mined queue/utilization evidence.

**Addresses pathologies:**
- Reduces avoidable setups (especially on bottlenecks).
- Improves due-date performance by incorporating realistic remaining time (not just due date).
- Reduces downstream starvation/blocking by respecting system state.

**Expected KPI impact:**
- Lower average setup time per job at key machines.
- Reduced tardiness tail (P95 tardiness).
- Lower WIP (less “overproduction” into bottlenecks).

---

### Strategy 2 — Predictive scheduling / rolling-horizon optimization with probabilistic times + disruption risk
**Core logic:**
- Implement a **rolling horizon schedule** (e.g., re-optimize every 1–2 hours or on major events).
- Use **stochastic durations**:
  - processing time distributions conditioned on job family + machine + operator + shift,
  - setup distributions from SDS model,
  - breakdown risk model (hazard rate / MTBF & MTTR distributions).
- Objective: minimize expected tardiness + WIP cost + setup loss, subject to machine availability and precedence constraints.

Practical solver choices:
- For near-term horizon on bottlenecks: **MILP/CP-SAT** (e.g., Google OR-Tools CP-SAT) with sequence-dependent setups.
- For full-shop: hierarchical approach  
  - optimize bottleneck sequence first (“drum”),  
  - then dispatch others with Strategy 1 constrained to feed that plan.

**How process mining informs it:**
- All distributions and correlations come from log mining:
  - processing variability,
  - setup variability by transition,
  - breakdown frequency/repair times by machine,
  - rework/inspection loops probabilities.
- Predictive “risk of lateness” per job using survival/ML models trained on historical traces.

**Addresses pathologies:**
- Converts “unpredictable lead times” into quantified risk with confidence bounds.
- Reduces turbulence by event-driven re-optimization rather than constant manual expediting.
- Proactively reroutes or resequences when predicted lateness risk increases (e.g., expected breakdown).

**Expected KPI impact:**
- Higher due-date adherence with fewer emergency interventions.
- More stable WIP and lead times (reduced variance).
- Better utilization balance (less starvation due to lookahead).

---

### Strategy 3 — Setup-time optimization via family batching + “campaigning” on bottleneck machines (with guardrails)
**Core logic:**
- For each bottleneck machine, define **job families** (tooling/fixture/material/program similarity) using mined setup drivers.
- Run **campaign windows**:
  - batch similar jobs to exploit low-changeover sequences,
  - but add due-date guardrails (max wait-to-batch, slack thresholds).
- Use an explicit **changeover cost matrix** from the SDS model and solve:
  - a Traveling-Salesman-like sequencing subproblem (min setup) with due-date constraints (min tardiness).

Operationally:
- Only apply strict batching on true bottlenecks (where setup dominates capacity loss).
- Non-bottlenecks follow Strategy 1 to avoid creating excessive waiting.

**How process mining informs it:**
- Family definitions validated by observed setup-time clustering.
- Quantifies trade-off: how much tardiness increases if you wait to batch vs how much capacity you gain by reducing setups.
- Identifies which transitions are worth avoiding and which are negligible.

**Addresses pathologies:**
- Directly targets excessive setup loss due to poor sequencing.
- Increases effective bottleneck capacity (often the biggest lever in job shops).
- Stabilizes flow by reducing schedule thrash at critical resources.

**Expected KPI impact:**
- Significant reduction in setup ratio on bottlenecks.
- Increased throughput and reduced WIP upstream of bottlenecks.
- Improved on-time delivery once bottleneck capacity is freed.

---

## 5) Simulation, Evaluation, and Continuous Improvement

### 5.1 Discrete-event simulation (DES) parameterized by process mining
**Purpose:** test strategies safely under realistic variability and disruptions.

**Build a DES “digital twin” with mined parameters:**
- **Arrivals:** interarrival distribution by customer/priority; hot-job arrival process.
- **Routings:** from discovered/clustered variants; routing probabilities by job family.
- **Processing times:** distributions conditioned on machine/job family/operator/shift.
- **Sequence-dependent setups:** predictive model or transition matrix.
- **Breakdowns:** MTBF/MTTR distributions by machine; optional condition-based risk.
- **Calendars:** shifts, planned maintenance, operator availability constraints.
- **Dispatching/scheduling logic:** plug in baseline (FCFS/EDD) vs Strategies 1–3.

**Evaluation metrics (compare across policies):**
- % late, mean tardiness, P95 tardiness.
- Lead time distribution (median and tail).
- Average WIP and WIP by buffer.
- Bottleneck utilization and setup ratio.
- Schedule stability: number of resequencing events, overtakes, priority inversions.

**Scenarios to test:**
- High load (90–110% of nominal capacity) vs normal load.
- Frequent breakdown regime vs improved maintenance regime.
- Hot-job surge weeks (higher priority-change frequency).
- Mix shift (more complex jobs / more tight tolerances).
- Reduced staffing/operator constraints (to see robustness).

Use multiple replications (Monte Carlo) because stochastic variability matters; rank policies by expected performance and risk (variance).

### 5.2 Continuous monitoring + adaptation using ongoing process mining
Implement a closed loop:

1. **Operational KPI cockpit (near real-time):**
   - queue times by machine (median/P95),
   - bottleneck setup ratio,
   - predicted lateness risk per job,
   - WIP by buffer,
   - adherence to bottleneck campaign plan.

2. **Automatic drift detection:**
   - Statistical monitoring of processing/setup distributions (e.g., KS test) to detect when models become stale.
   - Breakdown rate drift detection (machine health degradation).
   - Changeover matrix drift (new product families/tooling).

3. **Policy governance:**
   - A/B testing in simulation for proposed parameter changes (weights in Strategy 1, campaign window length in Strategy 3).
   - Periodic re-training of predictive models (Strategy 2) using the latest logs.

4. **Root-cause alerts (process-mining-driven):**
   - “Setup spike” alerts: a high-cost transition happening unusually often this week.
   - “Starvation” alerts: a machine idle >X while eligible WIP exists upstream (signals coordination/dispatch failure).
   - “Expedite spiral” alerts: priority changes exceed threshold and correlate with WIP growth.

---

### Practical rollout recommendation (to reduce risk)
- **Phase 1:** process mining baseline + bottleneck/SDS quantification; implement Strategy 1 on 1–2 pilot work centers.
- **Phase 2:** add Strategy 3 on the primary bottleneck (biggest capacity win).
- **Phase 3:** implement rolling-horizon predictive scheduling (Strategy 2) for bottleneck-first planning, with DES-validated parameters.

If you want, I can translate this into a concrete data schema (tables needed from MES/ERP), plus example KPIs and the exact algorithms/optimization formulations to use on the bottleneck machine first.