## 1. Analyzing Resource Behavior and Assignment Patterns

### 1.1 Agent and Tier Performance Metrics

Using the event log, you can reconstruct each ticket’s lifecycle and all resource touches. Start by building a case table (one row per `Case ID`) with aggregates, then a resource table (one row per agent).

Key metrics per **agent** and **tier** (L1, L2, L3):

1. **Workload distribution**
   - Tickets handled (started) per agent:
     - `#cases_started_by_agent` (per period, overall and per priority/category/required skill).
   - Effort per agent:
     - `Total handling time` = sum of (Work Start  Work End) per agent.
     - `Average handling time per ticket` by priority & category.
   - Compare:
     - Gini coefficient or simple quartile analysis of tickets per agent to quantify workload imbalance.
     - Heatmaps: Agents on rows, weeks on columns, cell = #tickets or hours.

2. **Throughput and SLA-related performance**
   - Per ticket: `End-to-End Resolution Time` = `Ticket Created`  final close.
   - Per agent/tier:
     - `Average waiting time before work start` (queue delay) for tickets after being assigned.
     - `Average active processing time` (sum of work intervals) vs waiting time.
     - `SLA breach rate` for tickets last handled by that agent/tier.
   - Breakdown by Priority (P1–P4) to see if P2/P3 have systematically worse performance at specific tiers.

3. **First-contact / First-tier resolution**
   - Define L1 “span” for each ticket: `Work L1 Start (first)`  either `Escalate L2`, `Close`, or `Reassign`.
   - Metrics:
     - `L1 first-touch resolution rate`:
       - % of tickets that are created and resolved without escalation beyond L1.
     - By category / required skill / priority:
       - e.g., L1 resolves 80% of “Software-OS” P4 but only 10% of “Network” P3.
   - Per L1 agent:
     - Resolution-to-escalation ratio.
     - Identify “strong” L1 agents and areas where training or guidelines can be adjusted.

4. **Frequency of handling specific ticket types / skills**
   - For each agent:
     - Distribution of tickets by:
       - `Ticket Category`
       - `Required Skill`
       - `Priority`
   - This shows whether agents actually work on what their documented skills suggest:
     - Example: Agent B12 (L2) has `App-CRM, DB-SQL` but 70% of their workload is generic P4 `Hardware`.

5. **Reassignment and escalation metrics**
   - Per ticket:
     - `# of distinct agents`, `# of intra-tier reassignments`, `# of inter-tier escalations`.
   - Per agent:
     - `Reassignment-out rate`: % of tickets they touch that later get reassigned (indicates mis-assignment or poor diagnosis).
     - `Reassignment-in rate`: often a specialist receiving misrouted tickets.
   - Per tier:
     - Average # of hops before the “right” tier is reached.
     - E.g., P3 Software-App tickets needing `Database-SQL` often bounce L1L2 (App-CRM)  L2 (SQL)  L3.

### 1.2 Using Process Mining Techniques

1. **Resource Interaction Analysis / Social Network Analysis (SNA)**
   - Build a **handover-of-work network**:
     - Nodes = agents or roles (L1/L2/L3/skill clusters).
     - Edge AB exists if A logs a completion activity and, subsequently, B logs the next work activity on the same case.
     - Weight edges by:
       - # of handovers.
       - Total delay between handover and next start.
   - Insights:
     - Identify “handover hotspots”: e.g., L1  certain L2 agents with very high volume and delay.
     - See if there is a “hub specialist” (e.g., one SQL expert) causing a bottleneck.
     - Detect “ping-pong” patterns: L1L2L1 or L2L2 recurrent loops.

2. **Role Discovery**
   - Cluster resources based on:
     - Activities performed (work types), categories, required skills.
     - Performance patterns (time, resolution rate).
   - Outcome:
     - Actual roles may differ from formal tiers: some L1 agents behave like “mini-L2” for specific categories.
     - Some L2 agents may be doing mainly L1-type work.
   - Compare discovered roles with documented tiers and skill profiles to see misalignments.

3. **Discovery of real assignment patterns**
   - Mine the process with **resource perspective**:
     - Event attributes provide `Agent Tier`, `Agent Skills`, `Required Skill`, `Ticket Category`, `Priority`.
   - Use **decision mining / rule mining**:
     - Predict next resource/tier given the ticket state:
       - “When a P2 Network ticket comes from L1 and requires Networking-Firewall, which agent/tier is actually chosen?”
   - Compare:
     - Intended logic (round-robin ± manual) vs actual logic uncovered from data:
       - Manual overrides.
       - Informal “hidden routing rules” (e.g., dispatcher prefers certain L2 agents).

### 1.3 Skill Utilization Analysis

1. **Skill-based workload views**
   - For each skill (e.g., `Networking-Firewall`, `Database-SQL`):
     - `#tickets requiring skill`.
     - Agents who touched such tickets and their documented skills.
     - Share of skilled vs unskilled agents handling those tickets.
   - Example: 500 tickets with Required Skill = `Database-SQL`:
     - 60% touched initially by L1 with no DB skills.
     - Only 2 DB-skilled agents exist; they handle 90% of late-stage work.

2. **Skill-to-work match**
   - For each agent:
     - `Skill match rate` = % of their handled tickets where `Required Skill  Agent Skills`.
   - For specialists:
     - Are they spending many hours on tickets whose required skill could be covered by broader skill sets or generalists?
     - Example: L3 security expert doing password resets (Access Management) that L1 could handle.

3. **Under/over utilization of skills**
   - Time-based:
     - For each specialized skill, compute:
       - Total time spent on tickets requiring that skill.
       - Total time specialized agents spend on non-specialized work.
   - Compare across skills:
     - Some skills are bottlenecked (high demand, low supply and high queues).
     - Some skill areas are underutilized (low usage despite many skilled agents).

---

## 2. Identifying Resource-Related Bottlenecks and Issues

Using the metrics above, you can systematically identify issues.

### 2.1 Bottlenecks from Skill Scarcity

Method:

1. For each skill (or combination of skill & priority):
   - Compute:
     - Average queue time before first work by a skilled agent.
     - # of unique skilled agents and their utilization.
   - Compare to other skills.

Diagnosis examples:

- **Indicators of bottleneck**:
  - `Database-SQL` P2/P3 tickets:
    - Median wait from Assign L2  Work L2 Start far above others.
    - Only 2 agents consistently perform the DB work.
    - They show >85% utilization while other L2 agents are <50%.
- Quantification:
  - Extra waiting time vs baseline:
    - e.g., P3 tickets with DB skill wait 3 hours longer on average than other P3 tickets  direct contribution to SLA breaches.

### 2.2 Delays from Reassignments and Escalations

Method:

1. For each case:
   - Identify segments between assignments:
     - `Assign X`  next `Work X Start`.
   - Mark every reassignment / escalation:
     - Count them; measure added idle time between end of previous work and next start.

2. Aggregate:
   - `Average added delay per reassignment`.
   - `Average added delay per escalation tier hop (L1L2, L2L3, L2L2)`.

Example outcome:

- Cases with 2 reassignments:
  - Average resolution time 2.5 × longer than cases without reassignment.
  - Each reassignment adds 45 minutes of waiting on average.

### 2.3 Impact of Incorrect Initial Assignments

Method:

1. Identify cases where:
   - First handler is L1 or specific agent who does not have the skill that the ticket eventually requires (as evidenced by later required skill at L2/L3).
   - Diagnostic: “skill mismatch at initial assignment”.

2. Compare:
   - Resolution times & SLA breach rates for:
     - Tickets with initial skill match vs mismatch.
   - # of hops and time spent at mismatched tiers.

Quantification:

- Example: For P3 Software-App:
  - Tickets with correct initial skill routing:
    - Avg resolution 5h; SLA breaches 10%.
  - Tickets with initial mismatch:
    - Avg resolution 11h; SLA breaches 35%.
  - This frames the cost of poor initial assignment.

### 2.4 Overloaded / Underperforming Agents or Teams

Method:

1. For each agent:
   - Plot:
     - Load (#tickets, handling hours) vs performance (avg resolution time, SLA hit/miss when they are last handler).
   - Identify:
     - High load + good performance  candidates for promotion/training others.
     - High load + poor performance  overloaded or mis-skilled; needs support/redistribution.
     - Low load + adequate skills (e.g., same skillset as overloaded peers)  underutilization.

2. For each tier:
   - Compare workload and performance across weeks/months:
     - Are some shifts/teams systematically overloaded?

### 2.5 Correlation Between Assignment Patterns and SLA Breaches

Method:

1. Label each case:
   - SLA met / missed, using priority-based SLA thresholds.

2. Feature engineering:
   - `#reassignments`, `#tiers involved`.
   - `Initial tier & skill match`.
   - `Time to first correct-skilled agent`.
   - `Skill scarcity indicator` (e.g., required skill among “bottleneck” skills).
   - `Workload at arrival` (queue size, utilization of relevant tier at time of assignment).

3. Use:
   - Statistical analysis (groupwise comparisons).
   - Or logistic regression / ML to identify which features drive SLA breach probability.

Example results:

- Every extra reassignment increases breach odds by 20%.
- Tickets not touching a correctly skilled agent within first 60 minutes have 3× breach probability.
- Tickets routed to a specific overloaded L2 cluster have significantly higher breach.

---

## 3. Root Cause Analysis for Assignment Inefficiencies

Link metrics to hypotheses.

### 3.1 Deficient Assignment Rules (Round-Robin, No Skill/Load Awareness)

Symptoms:

- Some agents are overloaded and others idle.
- Frequent reassignments from generalists to specialists.
- High proportion of tickets initially with skill mismatch.

Root Cause:

- Current routing ignores:
  - Required skill tags.
  - Agent current workload.
- Dispatchers or L1 agents manually “correct” assignments after discovering a mismatch.

Evidence via process mining:

- Decision mining:
  - Discover that the dispatcher’s actual choices correlate more with “who was assigned last time” than with skills or workload.
- Variant comparison:
  - Smooth cases: often directly assigned to known specialists.
  - Problematic cases: follow the official round-robin pattern at L2, then get corrected via reassignment.

### 3.2 Inaccurate or Incomplete Agent Skill Profiles

Symptoms:

- Some agents regularly handle certain skills despite not being flagged with them.
- Others rarely use a documented skill.

Root Cause:

- Skill matrix not in sync with reality:
  - Outdated HR data.
  - Learning on the job but database not updated.

Evidence:

- Resource-role discovery:
  - Actual “role” of an agent (based on tasks handled) includes `Networking-Firewall`, but their documented skill does not.
- Decision rules:
  - Tickets with “Networking-Firewall” often end up with Agent B08, regardless of official skill list.

### 3.3 Poor Initial Categorization / Skill Requirement Identification

Symptoms:

- Tickets often change `Ticket Category` or `Required Skill` mid-process.
- Many tickets with early L1 time, then a later reclassification at L2.

Root Cause:

- L1 or intake process cannot consistently capture correct category/skill from description.
- No automated assistance in classification.

Evidence:

- Compare “smooth” vs “problematic” variants:
  - Smooth variant: categories stable; no reassignments.
  - Problematic variant: early “Software-App”  later changed to “Database-SQL”.
- Decision mining:
  - Certain combinations of text features (short description, keywords) highly predict eventual skill, but initial manual classification is often different.

### 3.4 Lack of Real-Time Workload Visibility

Symptoms:

- Some agents always overloaded, others always underutilized.
- Assignment often goes to a “favorite” agent despite long queues.

Root Cause:

- Dispatchers and automatic routing cannot see real-time queue lengths or agent status (busy, free).
- Round-robin uses static lists rather than current load.

Evidence:

- Process mining with temporal utilization:
  - Show times when certain agents have queue lengths far above others.
  - Show that newly arriving tickets at that time are not routed to less busy but equally skilled agents.

### 3.5 Insufficient L1 Capability/Empowerment

Symptoms:

- High escalation rate from L1 to L2/L3, especially for ticket types that historically can be resolved at L1 by top performers.
- L1 time is mostly triage, not resolution.

Root Cause:

- Training gaps.
- Strict policies limiting what L1 can do (procedural, security, or tool access limitations).

Evidence:

- Variant analysis:
  - Subset of agents/times where L1 resolves many tickets in one step.
  - Others escalate the same type of issues almost always.
- Benchmark:
  - Compare L1 agents: resolution rate differences for identical categories/priorities.

---

## 4. Developing Data-Driven Resource Assignment Strategies

Below are five concrete strategies (at least three required) tied directly to the insights from process mining.

### Strategy 1 – Skill-Based, Proficiency-Weighted Routing

**Issue Addressed**

- Misassignments, excessive reassignments, specialists doing non-specialist work, poor SLA for skill-intensive tickets.

**Concept**

- Implement routing rules that:
  - Match `Required Skill` (and priority, category) to agents whose documented and **observed** skills cover the need.
  - For each skill, maintain a roster of agents with **proficiency level**:
    - Derived from historical performance:
      - Resolution rate.
      - Handling time.
      - Reassignment-out rate for that skill.

**Use of Process Mining Insights**

- From skill utilization analysis:
  - Identify which agents truly perform well on each skill.
- From decision mining:
  - Extract “who actually gets assigned when it goes well” and codify that as a rule.

**Data Needed**

- Cleaned/updated skill matrix per agent:
  - Augment manual profiles with mined skill clusters.
- Event attributes:
  - `Ticket Category`, `Required Skill`, `Priority`.
- Performance metrics per agent per skill from the historical log.

**How It Works**

- For each new ticket:
  - Determine required skill (see Strategy 3 below for prediction).
  - Build candidate list = [agents who have this skill].
  - Score agents:
    - `score = (proficiency weight) – (current load weight) – (fairness term)`.
  - Assign to top-scoring agent.

**Expected Benefits**

- Reduced reassignments:
  - Tickets go to a competent agent from the start.
- Better SLA compliance for P2/P3:
  - Especially where skill bottlenecks were evident.
- More effective utilization of specialists:
  - They are prioritized for complex issues; simpler issues routed to trained L1 or broader-skill L2.

---

### Strategy 2 – Workload-Aware Assignment and Dynamic Balancing

**Issue Addressed**

- Overloaded vs underutilized agents, long queues at specific specialists, delays due to static round-robin.

**Concept**

- Use real-time (or near real-time) workload indicators in routing decisions:
  - Queue length, current active tickets, recent throughput per agent.

**Use of Process Mining Insights**

- From utilization analysis:
  - Identify load patterns by hour/day and by skill/tier.
- From correlation analysis:
  - Show that high agent utilization beyond a threshold correlates with SLA breaches and longer waits.

**Data Needed**

- Live data:
  - Tickets in queue per skill/tier.
  - Agent status (available/busy/away).
- Historical thresholds:
  - Derived from process mining (e.g., performance degrades above 70% utilization).

**How It Works**

- Routing engine:
  - For each eligible agent:
    - Compute load index (e.g., weighted sum of assigned-but-not-started plus active tickets).
  - Use a **load-balanced, skill-based** assignment:
    - Among skilled agents, prefer those below threshold load.
- Dynamic rebalancing:
  - If backlog for a specific skill/tier exceeds threshold:
    - Flag potential backup agents with moderate skill (e.g., upskilled L1), escalate to supervisors to temporarily shift them.

**Expected Benefits**

- More even workload:
  - Reduced stress on certain specialists.
- Shorter queue times:
  - Tickets distributed to capable but previously underutilized agents.
- Lower SLA breach rate:
  - Especially in peak periods, because load spikes are better absorbed.

---

### Strategy 3 – Predictive Classification & Assignment

**Issue Addressed**

- Poor initial categorization, late discovery of correct required skill, unnecessary L1 time for tickets clearly needing L2/L3.

**Concept**

- Use historical data to build a **classification model**:
  - Input: ticket metadata at creation (free text description, category chosen by user, channel, past similar tickets).
  - Output: predicted `Required Skill`, predicted complexity/tier, and estimated probability of L1 resolution.

**Use of Process Mining Insights**

- From variant analysis:
  - Identify which features (keywords, categories) lead to later recategorization and reassignments.
- From decision mining:
  - Learn rules like “If description mentions ‘timeout’ and ‘SQL’ and category is Software-App, final required skill is DB-SQL 90% of the time.”

**Data Needed**

- Historical event log enriched with:
  - Ticket descriptions, custom fields, configuration items (CIs).
  - Final required skill and final tier that resolved the ticket.
- Text processing features:
  - Keyword presence, TF-IDF, simple domain-specific dictionaries.

**How It Works**

- At ticket creation or first L1 touch:
  - Model predicts:
    - Required Skill.
    - Likelihood that L1 can resolve (based on similar past tickets).
  - Routing logic:
    - If high probability L1 can resolve and skill fits L1 agent capabilities  keep at L1.
    - If low probability and clearly specialist skill  route directly to L2/L3 specialist (skipping ineffective L1 time).

**Expected Benefits**

- Fewer late-stage recategorizations.
- Reduced time spent in wrong tier:
  - Particularly for P2/P3 complex tickets that now bypass unproductive L1 troubleshooting.
- Improved SLA adherence through faster “right first time” assignment.

---

### Strategy 4 – Data-Driven Escalation and L1 Empowerment

**Issue Addressed**

- Over-escalation, L2/L3 doing work that L1 could handle, variability in L1 performance.

**Concept**

- Use historical L1 performance by ticket type to:
  - Define clear policies:
    - What types L1 should attempt to resolve (with scripts, knowledge base).
    - When to escalate (after specific checks).
  - Identify L1 agents with high resolution rates and use them as role models:
    - Training content based on their sequences of activities.

**Use of Process Mining Insights**

- From activity sequences:
  - For categories where L1 does resolve tickets:
    - What steps are taken? In what order? How long do they spend before escalation?
- From variance across L1 agents:
  - Identify L1 “champions” and standardize their practices.

**Data Needed**

- Event sequences at L1 level:
  - Activities like “Run Diagnostic X”, “Apply Patch”, “Reset Password”.
- Ticket outcomes:
  - Resolved vs escalated; final tier.

**How It Works**

- Refine **L1 decision trees**:
  - For each category/priority:
    - “If steps A, B, C not effective within 15 minutes, escalate with clear notes.”
  - Provide better guidance to L1 via updated scripts/knowledge articles.
- Adjust permissions/tools:
  - Give L1 the rights needed for high-frequency, low-risk resolutions discovered to be safe.

**Expected Benefits**

- Higher L1 resolution rate, particularly for P3/P4 and some P2.
- L2/L3 freed from low-complexity tasks.
- Fewer escalations, fewer handovers, shorter resolution times.

---

### Strategy 5 – Dynamic “Virtual Pools” and Temporary Skill Reallocation

**Issue Addressed**

- Structural skill bottlenecks at certain times, misalignment between fixed tier boundaries and actual demand.

**Concept**

- Create **virtual pools** based on skills/work types across tiers:
  - E.g., all agents capable of `Networking-Firewall` (L1, L2, L3) form a pool for Level 1 triage vs advanced diagnosis.
- Dynamically adjust who is available in each pool based on observed demand.

**Use of Process Mining Insights**

- Demand patterns per skill/tier over time:
  - e.g., Monday mornings: many `Access Management` tickets.
  - Month-end: lots of `Database-SQL` performance issues.
- Identify L2/L3 agents who frequently handle “simple” tickets and can temporarily support L1.

**Data Needed**

- Forecasts of arrival rates by category/skill.
- Skill matrix with “primary” and “backup” skills across tiers.

**How It Works**

- During peaks for certain skills:
  - Automatically assign some L2/L3 agents to help triage/resolve simpler cases in that domain.
- Outside peak times:
  - They revert to complex tasks and improvement work.
- Routing engine:
  - Uses “active pool membership” to decide eligible handlers at any point.

**Expected Benefits**

- Better handling of demand spikes without permanent staffing changes.
- Reduced backlog for skills that periodically become bottlenecks.
- More flexibility than rigid tier boundaries, improving overall SLA performance.

---

## 5. Simulation, Implementation, and Monitoring

### 5.1 Simulation with Mined Models

Before implementation, validate strategies via **business process simulation**:

1. **Model Building**
   - Use discovered process model (e.g., BPMN or Petri net) from the event log with:
     - Control-flow: activities (Create, Assign L1, Work L1, Escalate L2, etc.).
     - Resource assignments: probabilities of going to different tiers/agents as extracted.
   - Calibrate:
     - Activity duration distributions per agent/skill/tier.
     - Arrival rates by priority and category.
     - Current routing policies (baseline scenario).

2. **Define Alternative Scenarios**
   - Scenario A: Current round-robin, no skill-awareness.
   - Scenario B: Skill-based routing (Strategy 1).
   - Scenario C: Skill-based + workload-aware (Strategies 1 + 2).
   - Scenario D: Add predictive classification (Strategies 1–3).

3. **Simulation Runs**
   - Run each scenario over a synthetic year (or same volume as real year).
   - Compare metrics:
     - Average resolution time per priority.
     - SLA breach rates per priority/skill.
     - Avg # of reassignments per ticket.
     - Utilization metrics per agent/skill.

4. **Interpretation**
   - Use results to:
     - Tune thresholds (e.g., acceptable utilization).
     - Adjust assignment rules (e.g., when to bypass L1).
   - Present evidence-based benefits to stakeholders for change approval.

### 5.2 Implementation Roadmap (High Level)

1. **Data Preparation & Baseline**
   - Clean and standardize event log (timestamps, agent IDs, skills).
   - Build baseline dashboards with KPIs (see next section) to quantify “before” state.

2. **Skill Matrix Reconciliation**
   - Use mining outputs to validate and update documented skills.
   - Business validation with team leads.

3. **Pilot Routing Engine**
   - Start with one or two skills/categories (e.g., `Networking-Firewall`, `App-CRM`).
   - Implement Strategy 1 and 2 in parallel with existing system; compare pilot vs control group.

4. **Progressive Rollout**
   - Expand to more skills and tiers.
   - Integrate predictive classification as confidence in models grows.

5. **Process Changes and Training**
   - Update SOPs, escalation policies.
   - Train dispatchers and L1 on new rules and tools.

### 5.3 Monitoring with Process Mining Dashboards

Set up **continuous monitoring** with resource-focused process mining dashboards.

**Key KPIs**

1. **Assignment Quality & Reassignments**
   - Avg # reassignments per ticket, per category/priority.
   - Reassignment rate per agent and per dispatcher.
   - Time lost due to reassignments (cumulative per month).

2. **Skill Match and Utilization**
   - Skill match rate: % tickets where initial agent possessed required skill.
   - Specialist time spent on non-specialist work.
   - Time-to-first-skilled-agent.

3. **Workload and Utilization**
   - Real-time and historical:
     - Tickets in queue per skill/tier.
     - Utilization per agent (active work vs idle).
   - Overload indicators:
     - % of time agents are above utilization threshold.

4. **SLA and Performance**
   - SLA compliance per priority, by:
     - Category.
     - Required skill.
     - Final tier.
   - Correlate SLA breaches with:
     - #reassignments.
     - initial skill mismatch.
     - time to first skilled agent.

5. **L1 Effectiveness**
   - L1 first-contact resolution rate by category/skill.
   - L1 escalation rate vs historical baseline.
   - Time spent at L1 for tickets eventually resolved at higher tiers.

**Process Views**

- **Handover Network View**:
  - Updated monthly to see if:
    - Ping-pong loops are decreasing.
    - Certain hubs (bottleneck agents) are relieved.

- **Variant View**:
  - Track proportion of “smooth” variants (Create  Right-tier  Resolve) vs “bumpy” ones (multiple reassignments).
  - Aim for trend: smooth variants increasing over time.

- **Heatmaps**:
  - Time-of-day / day-of-week heatmaps for:
    - Queue length per skill.
    - SLA breach frequency.

By combining these monitoring dashboards with periodic deep-dive process mining analyses, TechSolve can ensure the new assignment strategies are not only effective initially but remain aligned with evolving demand, staff changes, and technology shifts.