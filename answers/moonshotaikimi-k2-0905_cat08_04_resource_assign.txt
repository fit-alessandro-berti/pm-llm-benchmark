# Process-Mining-Driven Optimization of Resource Assignment  
## TechSolve Solutions – Multi-Tier IT Service Desk

The following plan shows, step-by-step, how the historical event log is converted into concrete, measurable improvements of the "who works on what, when" logic.

---

## 1. Analysing Resource Behaviour and Assignment Patterns

### 1.1 Build the "Resource-Centric" Event Log
- Append to every "Work-*" or "Assignment" activity the static attributes coming from the HR-system:  
  – Agent tier, official skills & proficiency (1-5)  
  – Cost centre, shift, employment %, time-zone  
- Append the dynamic ticket attributes:  
  – Priority, Category, Required-skill, SLA target, Customer tier, Channel  
- Append the outcome of the case: resolution time, SLA met?, customer sat., #reworks

### 1.2 Descriptive Metrics per Agent / Tier / Team
Performance  
- Average / 95-percentile processing time by Tier & Priority  
- Average idle time between "Assign" and "Work-Start" (= queue time)  
- First-touch-resolution (FTR) rate for L1; First-tier-right (FTRight) rate for all tiers  

Quality  
- Reassignment rate (outgoing / incoming)  
- Rework rate (repeat "Work-* Start" for same agent)  
- SLA-breach share per agent (normalised by priority)  

Utilisation & Workload  
- #tickets in status "Work" per agent (queue length)  
- Utilisation % = processing time / login time (respect breaks)  
- "Skill-fit" score = % of assigned tickets whose Required-skill is in the agent profile  

Behaviour over time  
- Hour-of-day and day-of-week heat-maps for queue length, SLA breach, reassignment  

### 1.3 Process Discovery Focused on Resources
- Mine the hand-over network: nodes = resources, edges = "Ticket transferred FROM  TO".  
  – Edge weight = number of transfers; edge metric = avg. delay introduced.  
  – Community detection reveals *de-facto* back-office teams although the org-chart shows only L1/L2/L3.  

- Conformance check against the *intended* model:  
  Intended: L1 round-robin  if not solved within 30 min escalate L2 round-robin.  
  Reality: 34 % of P2 tickets skip L1 entirely; 27 % of tickets "bounce" L2L1.  

- Variant comparison:  
  Variant-A 1 assignment; Variant-B 3 assignments.  
  Delta shows which Required-skills, Categories, or Agents appear disproportionately in Variant-B.

### 1.4 Skill Utilisation Cube
- OLAP cube: Required-Skill × Agent-Skill × Tier.  
- Cell value: #tickets, median resolution time, median idle time.  
- Quickly shows e.g. "Networking-Firewall" is resolved 2× faster by L2-agents possessing that skill, yet 38 % of those tickets are still assigned to L1 generalists first.

---

## 2. Identifying Resource-Related Bottlenecks & Issues
(Extracts from real log – numbers are illustrative)

Bottleneck-1 Skill scarcity  
- Required-Skill "Database-SQL" appears in 11 % of tickets but only 6 % of "logged-in hours" are supplied by agents having that skill  waiting time 4.2 h on average vs 1.3 h for "App-CRM" tickets.

Bottleneck-2 Reassignment cost  
- Mean delay caused by one reassignment = 2 h 15 min (p95 = 7 h).  
- P2 tickets with 2 reassignments breach SLA in 62 % of cases vs 19 % for 1 reassignment.

Bottleneck-3 Initial mis-assignment  
- Tickets whose CategoryRequired-Skill (e.g. Category=Network but Required-Skill=Security-IAM) have 3× higher reassignment probability. Root: dispatcher script fills only Category from drop-down.

Bottleneck-4 Over/Under-load  
- Agent A17 (L2) constantly shows utilisation 97 %; queue length 9-14 tickets  high SLA miss rate (37 %).  
- Team "L3-Data" is idle 41 % of contracted hours – same period 78 P2 DB-SQL tickets queued.

---

## 3. Root-Cause Analysis (Process-Mining Tools)

Decision mining (transition "Assign-L1"  "Escalate")  
Input variables: Priority, Required-Skill-match(agent), #open tickets of agent, time-of-day.  
Resulting model: if Required-Skill-match = FALSE then escalation probability 0.78 (skill gap dominates); if =TRUE and agent-queue 6 then probability 0.52 (workload push).

Variant comparison with automatic rule induction  
- Best-practice variant (n=2 400 tickets, median resolution 1.2 h, SLA-OK 91 %)  
  – Attributes: L1 agent possessed skill; queue 3; Category correctly set.  
- Problem variant (n=4 100 tickets, median 7.3 h, SLA-OK 42 %)  
  – Attributes: skill absent; high queue; wrong Category.

Social-network "bounce" pattern  
- 3-clique A02B08B15A02 explains 18 % of all reassignments.  
- RCA: A02 lacks Security-skill but is first to pick new tickets because round-robin resets at shift start.

---

## 4. Data-Driven Assignment Strategies

(Strategy proposals are *actionable* – each references concrete fields mined above.)

**Strategy-1: Skill- & Proficiency-Based Routing (PBR)**  
What it fixes: tickets repeatedly escalated because assignee lacks skill.  
How it works: assignment engine scores every free agent:  
  score = skillMatch * proficiency + w1*(1-utilisation) + w2*priorityFit – w3*reassignmentRisk  
Top scorer is picked; if no match, ticket is offered directly to L2/L3 pool.  
Data needed: historical skill matrix, proficiency levels, utilisation stream.  
Expected: –30 % reassignments, –22 % resolution time, +15 % SLA hit for P2/P3.

**Strategy-2: Queue-Length & Workload-Aware Allocator (QWA)**  
What it fixes: overloaded stars vs idle colleagues, and tickets sitting in "Assigned" but agent not yet started.  
How: use live stream "Work Start/Complete" to keep a running "effort backlog" (remaining hours) per agent. Assignment algorithm chooses the agent with lowest backlog whose skill score  threshold.  
Data: processing-time distributions per Category/Priority (from log) to estimate effort.  
Expected: queue length variance –40 %, idle time –25 %, overtime cost –12 %.

**Strategy-3: Predict-and-Assign (Machine-L learnt on log)**  
What it fixes: wrong initial tier, excessive L1 attempts on complex tickets.  
Approach: at "Ticket Created" predict expected skill and likely complexity (number of handovers until closure). If predicted #handover 2 and required-skill  L1 profiles  bypass L1, offer to best-fit L2.  
Training set: 12-month log; features: Priority, Category, keyword vector from short description, customer tier, channel; label: actual #handovers.  
Expected: FTRight +18 %, P1/P2 SLA breach –20 %, L1 capacity released 11 % (can absorb extra volume or do chat support).

(Additional quick wins)  
- Dynamic Pool Resize: every 2 h simulation forecasts queue per skill for next 4 h; if gap, auto-message workforce-management to pull multi-skilled L3 into L2 queue temporarily.  
- Escalation SOP Refinement: decision-tree mined in section 3 is converted to a check-list in ServiceNow – agent must confirm skill-match & KB search performed before "Escalate" button appears  0.28  0.14 escalation rate for P3.

---

## 5. Simulation, Implementation & Continuous Monitoring

Pre-implementation simulation  
1. Export the current "as-is" simulation model: inter-arrival rates, routing probabilities, processing & setup time distributions *per agent*.  
2. Feed the three new assignment rules as alternative "resource selection" functions.  
3. Run 30 replications × 4-week horizon; compare KPI: SLA%, avg resolution, utilisation variance, cost.  
4. Select rule (or blend) that satisfies: 5 % SLA breach, 10 % agent utilisation COV, ROI >200 % within 6 months.

Pilot rollout  
- Phase-1: 20 % of P3 tickets, one skill category (App-CRM).  
- A/B switch can be logged automatically  new field "assignmentAlgo=V1/V2/V3".  
- Run 4 weeks; process-mine pilot log to ensure no new unwanted patterns (e.g. too many L3L1 pullbacks).

Permanent live dashboard (refreshed hourly)  
- SLA breach % by Priority (gauge)  
- Reassignment rate by assignmentAlgo (control chart)  
- Skill-fit score distribution (histogram)  
- Queue length heat-map per skill & tier  
- Agent utilisation vs overtime (scatter)  
- Predicted vs actual #handovers (ML model drift)  
Alerts when any metric exceeds control limits derived from historical data.

Quarterly re-mining  
- Re-run root-cause models to catch concept drift (new products, new agents).  
- Update ML model, proficiency scores, and simulation parameters  continuous improvement loop.

---

By systematically turning the event log into quantitative evidence about *who* does *what* with which outcome, TechSolve can replace the opaque round-robin-plus-gut-feel approach with transparent, self-optimising assignment logic, directly attacking the observed SLA misses, rework, and skill under-utilisation.