## 1. Analyzing Resource Behavior and Assignment Patterns

### 1.1 Metrics for agents and tiers (L1, L2, L3)

Using the event log, you can build a resource-centric view. For each agent and tier:

**Workload & capacity usage**
- Tickets handled per agent/tier (overall and by priority/category/skill).
- Active handling time per agent (sum of `Work * Start–End` durations).
- Share of time spent waiting vs. working:
  - Queue time before first work at each tier.
  - Queue time between reassignments/escalations.

**Performance & speed**
- Mean/median handling time per activity and ticket type:
  - `Work L1 Start–End`, `Work L2 Start–End`, etc.
  - Broken down by:
    - Priority (P1–P4).
    - Category.
    - Required Skill.
- SLA-related:
  - % of tickets closed within SLA per agent/tier.
  - Average time-to-first-action per agent/tier.

**Effectiveness (especially L1)**
- First-contact/first-tier resolution:
  - L1 First Resolution Rate:
    - % of tickets that:
      - Have `Work L1 Start/End`, and
      - Never see `Escalate L2` or `Escalate L3`.
  - By ticket type: category, required skill, priority.
- Escalation pattern:
  - Escalation rate per L1 agent, and per ticket type.
  - Average L1 handling time before escalation (is L1 “parking” tickets?).

**Assignment & routing patterns**
- Tickets per agent by:
  - Ticket Priority.
  - Category.
  - Required Skill.
- Mismatch counts:
  - Cases where an agent’s documented skills do not cover `Required Skill` of the ticket.
  - How often such mismatches lead to subsequent reassignments or escalations.

This yields:
- Who is overloaded (many tickets, long active time, high queue)?
- Who is underutilized (few tickets, low active time)?
- Who is efficient (short handling times, high first-tier resolution)?
- Where are skill mismatches frequent?

---

### 1.2 Using process mining techniques on assignments, handovers, roles

**a) Resource interaction / social network analysis (SNA)**  
Construct handover networks using activity sequences per ticket:

- Node: agent (or agent + tier).
- Directed edge: A  B if the same ticket is handled by A, then by B (e.g., `Work L1 End (A05)`  `Work L2 Start (B12)`).
- Weight: number of such handovers; color/label by average delay between handover.

Insights:
- Common escalation paths: which L1 agents send most work to which L2/L3 specialists.
- “Ping-pong” patterns: A  B  A or A  B  C  B, indicating unclear ownership or poor assignment.
- Bottleneck experts: nodes with very high in-degree/in-flow and long queues.

Compare to intended logic:
- Intended: L1 pool  skill-based L2 pool  occasional L3; minimal back-and-forth.
- Actual: SNA might show:
  - Multi-hop escalations.
  - Frequent reassignments within L2 or L3 before the right specialist is found.
  - Certain agents acting as unofficial “routers” or fire-fighters.

**b) Role discovery**

Use clustering on event logs where attributes include:
- Agent ID.
- Activities performed.
- Ticket types handled (priority/category/required skill).

Goal:
- Discover de facto roles: e.g., “Network-P2 responder”, “CRM escalation specialist”, “generalist L2”.
- Compare discovered roles to formal tiers and skills:
  - Some L1s may function as quasi-specialists.
  - Some L2s may mostly do basic work (contrary to their skills).

**c) Process variants per assignment pattern**

Derive process variants focusing on:
- Path from `Ticket Created` to `Resolution/Close`.
- Sequence of `Assign *`, `Escalate *`, `Reassign`.

Examples:
- Variant A: Created  Assign L1  Work L1  Close (no escalation).
- Variant B: Created  Assign L1  Work L1  Escalate L2  Work L2  Close.
- Variant C: Created  Assign L1  Work L1  Escalate L2  Assign L2  Reassign L2  Reassign L2/L3  …

Measure:
- Throughput time and SLA compliance per variant.
- Reassignment count per variant.

This shows where assignment logic diverges from intended target behavior.

---

### 1.3 Analyzing skill utilization

Using `Agent Skills` and `Required Skill`:

**Skill coverage and usage**
- For each skill S:
  - Number of agents that have S.
  - Number of tickets requiring S.
  - Load per skilled agent (tickets requiring S per agent).
- For each agent:
  - % of handled tickets requiring each skill they have.
  - % of handled tickets *not* requiring any of their specialized skills (generic work).

**Over-qualification vs under-qualification**
- Over-qualification:
  - Tickets handled by high-skill L2/L3 where `Required Skill`  basic skills (or unspecified).
  - Duration of time spent by specialists on such tickets.
- Under-qualification:
  - Tickets handled where agent lacks the declared required skill.
  - Downstream effect: probability of subsequent reassignments, extra hops, delays.

**Skill-specific performance**
- For each skill S:
  - Average time-to-resolution when assigned to:
    - Agents with S vs without S.
  - Escalation/reassignment rate by whether initial agent had skill S.

This indicates:
- Whether your specialist skills are being consumed on low-complexity tasks.
- Where additional training or skill re-labeling is necessary.
- Which skills represent true bottlenecks.

---

## 2. Identifying Resource-Related Bottlenecks and Issues

Using the metrics and analyses above, you can systematically detect problems.

### 2.1 Skill bottlenecks

Indicators:
- Skills where:
  - Few agents with skill S.
  - Many tickets requiring S (high demand).
  - High average queue time before first L2/L3 “Work Start” for S.
- Time-profile:
  - Peak hours/days when tickets requiring S spike.
  - Corresponding queue build-up for agents with that skill.

Quantification:
- For each skill S:
  - Average waiting time before first skilled agent starts work.
  - % of SLA breaches among tickets requiring S vs overall.
  - Share of global SLA breaches attributable to skill S.

Example: “Networking-Firewall”
- Average delay before first specialized work: 3 hours (vs 45 min overall).
- 40% of P2 firewall tickets miss SLA vs 18% overall  clear skill bottleneck.

---

### 2.2 Delays from reassignments and escalations

Compute for each ticket:
- Number of `Escalate *` and `Reassign` events.
- Total handover time:
  - Sum of time from `Work End` of one agent to `Work Start` of next agent.

Metrics:
- Average/median # of reassignments per ticket type.
- Time impact:
  - Average added delay per reassignment:
    - E.g., compare tickets with 0 vs 1 vs 2+ reassignments.
  - Regression: total resolution time ~ reassign count (control for priority).

Expected finding:
- Each additional reassignment adds X minutes/hours to resolution.
- Tickets with 2 reassignments have Y% higher SLA breach rate.

---

### 2.3 Impact of incorrect initial assignments

Define “incorrect initial assignment” heuristically as:
- Tickets where:
  - Initial agent lacks the required skill; and/or
  - First agent’s tier is lower than the tier that ultimately resolves (e.g., L1 gets a ticket that *always* ends up at L3 for that type); and/or
  - Ticket is reassigned/escalated within very short L1 handling time (e.g., <5 minutes).

Analysis:
- Compare:
  - Tickets resolved without escalation vs those escalated after minimal L1 work.
- By ticket characteristics:
  - Which categories/skills/priorities have the highest ratio of “initially misrouted” tickets?
- By agent or dispatcher:
  - Misassignment rate per assigning entity.

Quantification:
- For tickets with misassignment:
  - Average additional waiting time before first correctly skilled resource starts.
  - % of SLA breaches involving misassignment.
  - Difference in overall resolution time vs correctly routed tickets of same type.

---

### 2.4 Over/under-performing and overloaded/underutilized agents

Use combined metrics per agent:

**Performance / effectiveness**
- SLA compliance for tickets primarily handled by the agent.
- Average handling time vs peers for similar tickets.
- Reassignment rate *away from* agent (signals possible skill gaps or triage issues).
- Reassignment rate *to* agent (signals they act as “fixer”).

**Workload & utilization**
- Average number of active tickets in progress.
- Average queue length in front of the agent.
- Distribution of work over the day/week.

Classification:
- High load, high SLA compliance  key performers (possible risk of burnout).
- High load, low SLA  overloaded bottleneck or mismatch.
- Low load, high SLA  underutilized potential.
- Low load, low SLA  performance or staffing issue.

---

### 2.5 Correlation between assignment patterns and SLA breaches

Join ticket-level assignment patterns with outcome:

- Features:
  - # reassignments.
  - # escalations.
  - Time to first correct-tier work (e.g., first L2/L3 start).
  - Initial agent skill match (yes/no).
  - Ticket type (priority, category, required skill).
- Outcome:
  - SLA met (yes/no).
  - Total resolution time.

Use:
- Descriptive stats: SLA breach rate by buckets of reassignments, misassignment, skill.
- Logistic regression or decision trees:
  - Predict SLA breach from assignment features.
  - Identify strongest predictors.

Result:
- For instance: “Tickets with 2+ reassignments and initial skill mismatch have a 3× higher odds of SLA breach.”
- Quantify contribution: X% of all SLA breaches occur in tickets with initial misrouting or skill mismatch.

---

## 3. Root Cause Analysis for Assignment Inefficiencies

### 3.1 Assignment rule deficiencies

From process discovery:
- If you see heavy use of `Assign L1` and `Assign L2` without any data-based routing, and the sequence of agents appears random within tiers:
  - Confirms “round-robin first, think later” behavior.
- Evidence:
  - Strong dependence of reassignments on specific ticket types.
  - No clear relationship between initial agent skill and ticket required skill.

Root cause:
- Assignment logic doesn’t consider:
  - Skill match.
  - Current workload and queue.
  - Historical resolution patterns.

---

### 3.2 Inaccurate or incomplete skill profiles

Indicators:
- An agent frequently resolves tickets requiring skill S, but S is not in their documented skills.
- Conversely, an agent has skill S recorded but their success/resolution performance on S is poor or they rarely receive S-tickets.

Root cause hypotheses:
- Skill inventory not maintained / outdated.
- Certification vs practical competence mismatch.
- Skills recorded at too coarse a level (e.g., “Network” vs “Network-Firewall/Router/WLAN”).

Use process mining to:
- “Infer” functional skills from history: agents who often successfully resolve “DB-SQL” tickets probably have that skill.
- Identify skills to add/remove or refine in profiles.

---

### 3.3 Poor initial ticket categorization and skill requirement identification

From variant analysis:
- Tickets of certain categories/required skills show:
  - High misassignment rates.
  - Multiple reclassifications or reassignments early in their lifecycle.
- Text analysis (outside strict process mining but using same logs):
  - NLP on descriptions to see if misrouted tickets share textual patterns.

Likely causes:
- Inconsistent or unclear categorization guidelines for L1/dispatchers.
- UX issues in the web portal (callers pick wrong category).
- Missing “triage wizard” or decision support for L1.

---

### 3.4 Lack of real-time visibility into workload and availability

Evidence:
- Some agents show long idle gaps while others are overloaded.
- Tickets are assigned to a resource who already has large queue when other similarly skilled agents are free.

Root cause:
- Dispatchers rely on static queues & round-robin rather than:
  - Real-time view of WIP per agent.
  - Expected time-to-availability per agent.

---

### 3.5 Insufficient L1 capability / empowerment

Indicators:
- High proportion of tickets escalated from L1 for categories that *could* be resolved at L1 (based on historical data).
- L1s spend minimal time on certain categories before escalating (no real attempt at resolution).

Use variant analysis:
- Compare “smooth” cases for a given category (L1 resolves) vs “escalated” cases:
  - Are there certain L1 agents with higher resolution rates? What do they do differently?
  - Do escalated cases lack certain actions (e.g., diagnostic steps)?

Root cause:
- Training gaps: some L1 agents lack skill to resolve common issues.
- Policy: L1 is told to escalate anything that’s not trivial.
- Tooling: L1 lacks knowledge base or diagnostic scripts.

---

### 3.6 Using variant analysis & decision mining explicitly

**Variant analysis**:
- Define:
  - Good variants: 0–1 assignment, timely L1 resolution or one-step escalation.
  - Bad variants: 2 reassignments, multi-tier ping-pong, SLA breach.
- Compare:
  - Ticket attributes: which priority/categories/skills dominate bad variants?
  - Assigners: which dispatcher/L1 agents show higher proportion of bad variants?
  - Time-of-day/day-of-week: spikes of bad variants at certain times.

**Decision mining**:
- From the event log, mine decision rules at key points:
  - When L1 decides “Resolve” vs “Escalate L2”.
  - When dispatcher selects agent X vs Y.
- Compare mined decision logic to desired logic:
  - e.g., Real rule might be: “Assign next on round-robin ignoring skill” or “Escalate network tickets to any L2 available” instead of “assign to Network-Firewall skill holders”.
- Then check: cases where the mined rule predicted “escalate” but actual decision was “resolve at L1” – how did they perform? This indicates where decisions deviate from what historically works best.

---

## 4. Developing Data-Driven Resource Assignment Strategies

Below are four concrete strategies, each grounded in the analyses above.

### Strategy 1: Skill-Based, SLA-Aware Routing (Replace pure round-robin)

**Issue addressed**
- Misassignments and reassignments.
- Specialists doing basic work.
- SLA breaches due to tickets not landing with the right skill quickly.

**Concept**
- At every `Assign *` step, route tickets based on:
  - Required Skill(s).
  - Priority/SLA target.
  - Agent skill matrix.
  - Current workload per candidate agent.

**How it uses process-mining insights**
- The mined skill utilization & performance:
  - Identify which skills are truly critical and which agents reliably resolve which ticket types.
- SNA and decision mining:
  - Suggest likely “best” agent groups (micro-pools) for each category/skill/priority combination.

**Implementation data requirements**
- Cleaned skill profiles:
  - Updated with inferred skills from historical resolutions.
- SLA targets per priority (P1–P4).
- Real-time stats:
  - Current #tickets in queue per agent.
  - Current WIP and expected time-to-availability.
- Historical performance:
  - For assigning weights when multiple agents share skill S.

**Operational logic (illustrative)**
1. Determine required skill(s) from:
   - Ticket category.
   - Keywords in description.
   - Historical mapping (decision model) for similar tickets.
2. Identify candidate agents:
   - Agents whose skill set includes required skill.
   - Tier appropriate for ticket (L1 vs L2/L3).
3. Rank candidates by:
   - Skill proficiency (from historical success rate on such tickets).
   - Current load (favor those with shorter queue / WIP).
4. Assign to top-ranked agent.

**Expected benefits**
- Fewer reassignments and escalations.
- Faster time to first skilled touch, especially for P2/P3.
- Better utilization of specialists (they get tickets matching their skills).
- Improved SLA compliance for problematic categories.

---

### Strategy 2: Workload-Aware Dynamic Balancing and Micro-Pools

**Issue addressed**
- Uneven workload distribution and agent over/underutilization.
- Bottlenecks around specific experts.

**Concept**
- Build “micro-pools” of agents by skill and tier (e.g., L2-CRM, L2-Network-Firewall).
- Within each micro-pool, dynamically assign based on:
  - Workload threshold per agent.
  - Cross-coverage by adjacent skills.

**How it uses process-mining insights**
- Role discovery and skill utilization:
  - Determine functional groups of agents who already handle similar work.
- Workload analysis:
  - Identify which pools frequently experience overload vs underuse.
- Time-of-day patterns:
  - Align micro-pool sizing and cross-cover rules with peak periods (e.g., morning firewall surges).

**Implementation data requirements**
- Clustering results: agent-to-micro-pool mapping.
- Workload capacity planning:
  - Target max concurrent tickets per agent (by complexity).
- Real-time tracking:
  - # of assigned tickets per agent and pool.

**Operational elements**
- Auto-redistribution:
  - If micro-pool A is overloaded, and micro-pool B has overlapping skills:
    - Temporarily allow B to receive some subset of A’s tickets.
- Priority-aware capacity:
  - Ensure enough capacity reserved in each pool for P1/P2.

**Expected benefits**
- Reduce chronic overload on a few “star” specialists.
- Raise utilization of capable but underused agents.
- Lower average waiting time and more predictable SLA performance.

---

### Strategy 3: Predictive Assignment and Triage Using Historical Patterns

**Issue addressed**
- Poor initial categorization and skill-tagging at L1, leading to early misrouting.
- Excessive and unnecessary escalations.

**Concept**
- Use historical data to build predictive models that:
  - Infer required skill and target tier at ticket creation (based on category, description, past similar tickets).
  - Predict probability of L1 resolution vs needed escalation.
- Use this to:
  - Suggest best initial assignment.
  - Flag tickets that should bypass L1 and go directly to L2/L3.

**How it uses process-mining insights**
- Decision mining:
  - Identify which ticket attributes correlate with successful L1 resolution vs escalation.
- Variant analysis:
  - Find patterns where direct L2 assignment led to better outcomes than initial L1 handling.
- Text + process data:
  - Combine description and actual path to train models.

**Implementation data requirements**
- Historical event log enriched with:
  - Final resolution tier.
  - Presence/absence of reassignments.
  - SLA outcome.
- Feature set:
  - Ticket metadata (priority, category, requestor type, channel).
  - Text features from description.
- Model:
  - e.g., gradient boosted trees or logistic regression to predict:
    - Required skill(s).
    - Probability ticket will be resolved at L1.

**Operational logic**
- Upon ticket creation:
  - Suggest “Recommended required skill(s)” with confidence.
  - Suggest “Recommended initial tier” (L1 vs L2).
  - If model predicts low chance of L1 resolution and high risk of SLA breach, route directly to L2/L3.

**Expected benefits**
- Fewer “quick escalations” after token L1 handling.
- Reduced time-to-skill for complex tickets.
- Improved L1 hit-rate on tickets really solvable at L1.
- Lower reassignments and faster resolution for P2/P3.

---

### Strategy 4: Data-Driven Escalation and Training Program for L1

**Issue addressed**
- Excessive and sometimes unnecessary escalations from L1.
- Underutilization of L1’s potential, leading to overloading L2/L3.

**Concept**
- Use process mining to:
  - Identify ticket patterns that *could* be resolved at L1 (because some L1s already do).
  - Codify them into knowledge base articles and scripts.
  - Update L1 training and escalation rules.
- Introduce “smart escalation”:
  - L1 system prompts: “Similar tickets were resolved by L1 with steps X,Y,Z. Try these before escalating?”

**How it uses process-mining insights**
- From L1 FCR analysis:
  - For each category/skill, discover:
    - FCR rates per agent.
    - Which activities (diagnostics, checks) appear in resolved vs escalated cases.
- Lay out “best practice sequences” as discovered process fragments.

**Implementation data requirements**
- Detailed trace of L1 activities (not only high-level “Work L1 Start/End” but key steps, if logged).
- Outcomes (resolved vs escalated).
- Ability to tag and cluster similar incident types.

**Operational elements**
- Training:
  - Focus on categories with high L1 variance (some do well, some escalate a lot).
- Escalation policy:
  - Require that certain checks/steps are performed before escalation (except in P1 or known L2-type).
- Feedback loop:
  - For escalated tickets: L2 indicates whether it “could have been resolved at L1”; use to refine model and training.

**Expected benefits**
- Higher L1 resolution rates.
- Reduced L2/L3 workload on simple cases.
- Shorter overall resolution time and improved SLA compliance.

---

## 5. Simulation, Implementation, and Monitoring

### 5.1 Simulation of proposed strategies

Use business process simulation integrated with mined models:

**Inputs**
- Mined process model:
  - Including typical paths (variants) and activities.
- Statistical distributions:
  - Inter-arrival rates of tickets (by priority, category).
  - Service time distributions per activity and per agent/tier.
  - Routing probabilities (e.g., probability of escalation per type).
- Resource parameters:
  - Number of agents per skill and tier.
  - Working calendars.
  - New assignment rules (skill-based, workload-aware, predictive routing).

**Scenario setup**
- Baseline scenario:
  - Current routing (round-robin and manual escalation).
- Strategy scenarios:
  - Skill-based routing only.
  - Skill-based + workload-aware micro-pools.
  - Predictive direct-to-L2 for certain ticket types.
  - Combined strategies.

**KPIs to compare in simulation**
- Average and percentile resolution times per priority.
- SLA compliance per priority and skill.
- Average queue lengths and waiting times per tier and skill.
- Reassignment and escalation rates.
- Utilization per agent and per skill pool.

Use simulation results to:
- Tune parameters (e.g., thresholds when to bypass L1; micro-pool sizes).
- Estimate benefit before investing in full implementation.

---

### 5.2 Implementation roadmap (high-level)

1. **Data preparation & baseline analysis**
   - Clean event log, align agent skill matrix.
   - Run the analyses outlined in sections 1–3; establish baseline KPIs.

2. **Quick wins**
   - Adjust obvious misconfigurations:
     - Rebalance workload where extreme overload/underload is clear.
     - Correct skill profiles where inferred vs declared skills differ strongly.
   - Create initial skill-based routing for the most problematic categories (e.g., Network-Firewall, DB-SQL).

3. **Pilot new assignment logic**
   - Implement Strategy 1 + 2 in a limited scope (e.g., P2/P3 only, or only for certain categories).
   - Monitor impact in near real time.

4. **Introduce predictive triage (Strategy 3)**
   - Build initial predictive model from historical data.
   - Deploy as “recommendation” tool, not as hard rule at first.
   - Validate performance and refine.

5. **Training and escalation refinement (Strategy 4)**
   - Create targeted L1 training modules based on process mining insights.
   - Update escalation policies; integrate into tooling.

6. **Iterative improvement**
   - Use monitoring dashboards (below) to track and refine.

---

### 5.3 Monitoring with process mining dashboards

Use continuous process mining to track resource-related KPIs and process views.

**Core KPIs**
- SLA metrics:
  - SLA compliance per priority, category, required skill.
  - SLA breaches attributable to:
    - Misassignment.
    - High reassign count.
    - Skill bottlenecks.
- Reassignments & escalations:
  - Average # reassignments per case; distribution.
  - Reassignment rates by category, skill, agent, tier.
  - Time lost due to reassignments.

**Resource & skill utilization**
- Utilization per agent and per skill pool.
- Tickets per agent by type and priority.
- Over/underload indicators:
  - Time in overload (above threshold queue) per agent/pool.

**L1 effectiveness**
- First-tier resolution rate overall and per L1 agent.
- Escalation rate from L1 and categories where it remains high.
- Average L1 handling time before escalation.

**Routing quality**
- % of tickets where initial agent had required skill.
- % of tickets routed directly to correct tier (based on eventual resolving tier).
- Performance of predictive triage:
  - Model accuracy (on sample checks).
  - SLA impact of direct-to-L2/L3 routing.

**Process views**
- Variant charts:
  - Share of “good” vs “bad” variants over time (trend line).
- Social network:
  - Handover networks over time: see whether ping-pong patterns shrink.
- Bottleneck analysis:
  - Activity and resource bottlenecks identified in near real time.

**Feedback & governance**
- Monthly or quarterly reviews:
  - Evaluate if any skill pools under strain  trigger hiring or cross-training.
  - Identify top performers and systematically capture their practices.
- Continuous refinement:
  - Update predictive models and assignment rules periodically with new log data.

---

By combining these process-mining-based analyses, targeted strategies, simulation, and ongoing monitoring, TechSolve can systematically optimize its resource assignment logic—reducing reassignments and SLA breaches, balancing workload, and ensuring that the right tickets get to the right people at the right time.