## 1) Identifying instance-spanning constraints and quantifying their impact

### 1.1 Build the “performance baseline” from the event log
**Data prep/enrichment (from START/COMPLETE):**
- **Activity duration** = `complete(activity) – start(activity)` (within-instance processing time).
- **Queue / waiting time before an activity** = `start(activity) – complete(previous activity)` (this is where *between-instance* effects show up).
- **End-to-end lead time** = `complete(last) – complete(Order Received)` (or first timestamp).

**Process mining analyses to run first:**
- **Process discovery** (Inductive Miner / similar) to confirm the dominant control-flow and variants (rework, skipped QC, etc.).
- **Performance overlay** on the discovered model: median / p90 waiting times per transition (PickingPacking, PackingQC, QCLabel).
- **Segmented performance** by attributes: `Order Type`, `Requires Cold Packing`, `Hazardous`, `Destination Region`, time-of-day/week.

This establishes *where* time is lost, and *for whom* (e.g., Cold+Express vs Cold+Standard).

---

### 1.2 Constraint A — Shared (limited) Cold-Packing stations
**Formal identification (from the log):**
- Define the **resource pool** `ColdStations = {C1..C5}` using `Resource` values (e.g., Station C2).
- Filter cases with `Requires Cold Packing = TRUE` and analyze their `Packing` events.

**Key metrics (impact and severity):**
- **Cold packing queue time**  
  `WT_cold_pack = start(Packing) – complete(Item Picking)` for cold-required orders.
- **Cold station utilization** (per station and pooled)  
  `Util = busy_time / available_time`, where `busy_time` is sum of (complete-start) intervals on C-stations.
- **Time-at-capacity** for the pool (how often all 5 are busy):  
  `%time( concurrent_cold_packing = 5 )`
- **Demand > capacity indicator**:  
  distribution of `#cold-orders-ready-for-packing` vs `#cold-stations-available` (queue length proxy).
- **Lead-time contribution**: fraction of total order lead time attributable to cold packing queue time, by segment.

**Quantifying “resource contention” specifically (between-instance):**
For each cold order when it becomes *ready* for packing (`t_ready = complete(Picking)`), compute:
- `ColdOcc(t)` = number of cold packing activities running at time `t`.
If `ColdOcc(t_ready) = 5`, then waiting is *directly consistent* with cold-station contention.

---

### 1.3 Constraint B — Batching for Shipping Label Generation (region batches)
**Formal identification:**
- If the log contains **Batch IDs** (e.g., “System (Batch B1)”), treat **Batch** as an object and link orders to batch.
- If Batch ID is not consistently present, infer batches by:
  - `Destination Region` equality, and
  - label generation timestamps clustering (e.g., labels created in “bursts”), and/or
  - a derived “dispatch wave” identifier from WMS/TMS cutoffs.

**Key metrics:**
- **Batch waiting time per order**  
  `WT_batch = time(Shipping Label Gen) – complete(Quality Check)`
- **Batch formation time (oldest order penalty)**  
  For each batch `b`:  
  `Formation(b) = time(LabelGen(b)) – min_ready_time_in_batch(b)`  
  where `min_ready_time_in_batch(b)` is earliest QC completion among its orders.
- **Batch size distribution** (by region and time window)
- **SLA-risk metric**: probability an order breaches SLA *because of* batch waiting (`WT_batch` over threshold).

**Attributing delay to batching (between-instance):**
After QC, there is typically no scarce human resource; orders “wait” due to synchronization logic. If:
- `QC complete` is followed by a long idle gap, and
- label generation occurs aligned with a batch event (same batch id / same region burst),
then that gap is **batch-induced** rather than within-instance processing.

---

### 1.4 Constraint C — Priority handling (Express orders interrupt/override Standard)
This is an instance-spanning *scheduling* rule: an express case changes how other cases get served.

**Formal identification approaches (depending on log fidelity):**
1. **Queue overtaking analysis (works with START times):**  
   For each shared resource pool (packing stations, QC staff), build a “ready-to-start” list:
   - `t_ready(case, activity)` = completion of predecessor activity.
   - Compare ordering by `t_ready` vs ordering by `start(activity)`.
   - Count **overtakes** where an Express order starts before a Standard order that was ready earlier and competes for the same pool.
2. **Preemption detection (if you have pause/resume events):**  
   Measure number and duration of interruptions on standard orders.
3. **If no pause/resume is logged (common):** infer disruption via:
   - spikes in activity durations for Standard when Express load is high,
   - station timelines showing frequent switching patterns,
   - regression of Standard waiting time against Express arrival rate/WIP.

**Key metrics:**
- **Express vs Standard queue time** at each step (especially Packing and QC).
- **Standard delay attributable to Express pressure**:  
  Compare Standard waiting times in periods of high Express share vs low Express share (same weekday/hour).
- **Overtake rate**:  
  `% of Standard work items overtaken by Express after being ready`
- **Fairness/starvation indicator**:  
  tail latency (p95/p99 waiting) of Standard orders when Express volume spikes.

**Between-instance attribution:**
If Standard cases show increased queue time correlated with Express WIP at the same step/resource pool, that’s a between-instance scheduling effect (not within-instance duration).

---

### 1.5 Constraint D — Hazardous materials simultaneous-processing limit (10 in Packing or QC)
This is a **global concurrency constraint** spanning cases.

**Formal identification:**
- Filter `Hazardous Material = TRUE`.
- Create activity intervals for hazardous orders:
  - Packing interval: `[start(Packing), complete(Packing)]`
  - QC interval: `[start(QC), complete(QC)]`
- Define facility-level hazardous-in-process count over time:
  - `HazOcc(t) = #haz intervals covering t (packing OR QC)`

**Key metrics:**
- **Compliance**:
  - Max `HazOcc(t)`
  - `%time(HazOcc(t) > 10)` (violations)
  - `%time(HazOcc(t) = 10)` (saturated; bottleneck regime)
- **Hazard gating delay** (throughput impact):
  - `WT_haz_pack = start(Packing) – complete(Picking)` for hazardous orders
  - `WT_haz_qc = start(QC) – complete(Packing)`
  - And crucially: waiting *conditional on* `HazOcc(t_ready) = 10` (strong evidence of gating).
- **System throughput reduction proxy**:
  - Compare hazardous throughput per hour when saturated vs when not saturated.

**Between-instance vs within-instance differentiation:**
- If a hazardous order is ready for Packing/QC but `HazOcc(t_ready)=10`, then its wait is attributable to the **instance-spanning cap**, not its own processing time.

---

### 1.6 Separating within-instance vs between-instance causes (practical method)
For each activity execution, decompose:
- **Processing** (within-instance): `PT = complete – start`
- **Waiting** (mostly between-instance): `WT = start – prev_complete`

Then **classify waiting cause** using “system state” reconstructed from the log at `t_ready`:
- **Resource contention**: all eligible resources in the pool are busy during most of the waiting window.
- **Batching**: next step is label generation and the order’s wait aligns with batch closure logic (batch id / region burst / cutoff).
- **Regulatory gating**: hazardous + `HazOcc(t_ready)=10`.
- **Priority pressure**: wait increases with express WIP / overtakes are observed.

This turns vague “idle time” into attributable delay buckets.

---

## 2) Analyzing interactions between constraints (why single-factor fixes often fail)

### Typical interaction patterns to test (using the mined “system state” features)
1. **Express × Cold-Packing**
- If Express orders disproportionately require cold packing, then Express prioritization effectively reallocates a scarce pool (5 stations), increasing cold queue times for Standard cold orders.
- Look for: cold queue time of *Standard cold* orders rising with **Express-cold WIP**.

2. **Batching × Hazardous**
- If a region batch tends to include many hazardous orders, the hazardous cap can slow down completion of those orders, delaying batch closure and causing **non-hazardous orders** in the same region to wait longer for labels.
- Look for: batch formation time increasing with “hazardous share in region” and with `%time(HazOcc=10)`.

3. **Express × Hazardous cap**
- An Express hazardous order cannot legally “jump the line” if the cap is reached; instead it must wait for a hazardous slot, potentially breaking Express SLA.
- This creates a policy decision: reserve hazardous capacity for express, or redesign the flow to reduce hazardous processing time.

4. **Cold-Packing × Hazardous**
- If some SKUs are both cold and hazardous, they hit *two* global constraints (cold stations and hazardous cap). These “double-constrained” orders often dominate the long-tail lead time.

### How to analyze interactions concretely
- Build a dataset at each “ready-to-start” point with:
  - current cold station occupancy,
  - current hazardous occupancy,
  - current express queue length / WIP,
  - open batch age for the destination region,
  - time-of-day/shift.
- Use:
  - **root cause analysis / decision trees** on extreme lead times,
  - **regression / survival models** with interaction terms (e.g., `ColdRequired × ExpressWIP`),
  - **heatmaps** (waiting time vs (cold occupancy, hazardous occupancy)).

Understanding interactions is crucial because improving one bottleneck (e.g., faster QC) may *increase* pressure elsewhere (e.g., cold packing queue or hazardous saturation), worsening end-to-end performance.

---

## 3) Constraint-aware optimization strategies (concrete, interdependency-aware)

### Strategy 1 — Cold-Packing dispatching + flexible capacity (reduce cold-station contention without breaking Express)
**Primarily addresses:** Shared Cold-Packing + interaction with Express priority  
**Change proposed:**
- Introduce **rule-based dispatching** for cold stations:
  - Reserve **1 cold station** (or time slices) for Express-cold orders during peak windows.
  - Remaining cold stations use **due-date / SLA-aware** sequencing (e.g., earliest promised ship time).
- Add **flex capacity**:
  - Convert 1–2 standard stations to “cold-capable” during peaks (portable insulation/ice, dedicated workflow, trained staff).
  - Cross-train packers so staff can switch pools when cold queue exceeds threshold.

**How it leverages data:**
- Use historical arrival patterns to forecast cold demand by hour/day (time series on `Requires Cold Packing` arrivals).
- Use mined service-time distributions to size how many flex stations are needed to keep `p90(WT_cold_pack)` under target.

**Expected outcomes:**
- Lower `WT_cold_pack` and reduce `%time(cold pool saturated)`.
- Protect Express SLAs without starving Standard cold orders (reduced long-tail).

---

### Strategy 2 — SLA-aware batching redesign (reduce “completed but waiting” time)
**Primarily addresses:** Shipping batching delays + interaction with Hazardous/Express  
**Change proposed (practical options):**
1. **Max-wait batching rule** per region:
   - Generate labels when either:
     - batch size  N, **or**
     - oldest order in region has waited  T minutes since QC complete.
2. **Express bypass**:
   - Express orders (especially cold/express) generate labels immediately (no batching), or join a “micro-batch” with a tiny T.
3. **Decouple label generation from route optimization**:
   - Generate labels per-order earlier, then do route grouping at dispatch/loading (reduces hard synchronization before label step).

**How it leverages data:**
- Estimate the relationship between batch size and transport efficiency vs the penalty of batch waiting:
  - “Savings per additional batched order” vs “extra waiting minutes incurred”.
- Optimize `(N, T)` per region/season using historical region volumes.

**Expected outcomes:**
- Reduce `WT_batch` and batch formation time tail.
- More predictable shipping cutoffs; less SLA breach caused by synchronization.

---

### Strategy 3 — Replace disruptive preemption with controlled priority (fast Express without inflating Standard cycle time)
**Primarily addresses:** Priority handling (Express) + interaction with cold/hazard caps  
**Change proposed:**
- Move from “pause a standard order mid-work” to **non-preemptive priority** at job start:
  - When a station finishes its current order, it pulls the highest-priority available job next.
- Add **aging** (fairness) so Standard jobs don’t starve:
  - priority score increases with waiting time.
- Where needed, create a small **dedicated Express lane** (e.g., 1 standard packing station + 1 QC staff during peak).

**How it leverages data:**
- Quantify the *true cost* of preemption using the log:
  - increased packing durations, rework probability, or higher variance.
- Use simulation (Section 4) to tune:
  - how much capacity is reserved for express,
  - whether aging is needed and at what rate.

**Expected outcomes:**
- Express still gets fast response (reduced queue time), but Standard duration variability and long waits shrink.
- Lower operational turbulence (less stop/start) often improves quality and reduces rework.

---

### Strategy 4 — Hazardous WIP control (“token” system) + upstream release smoothing
**Primarily addresses:** Hazardous simultaneous limit + interactions with batching and express  
**Change proposed:**
- Implement a **facility-wide hazardous token system**:
  - 10 tokens exist; a hazardous order must acquire a token to start Packing or QC; token released when leaving QC (or leaving the restricted zone).
- **Smooth release upstream**:
  - If tokens are scarce, throttle hazardous picking releases so you don’t build a huge hazardous queue right before Packing/QC.
- Optionally dedicate a **hazardous-capable QC sub-team** to reduce time-in-QC (shorter occupancy  more throughput under a fixed cap).

**How it leverages data:**
- Use mined hazardous activity durations to estimate occupancy time and identify whether Packing or QC dominates hazardous occupancy.
- Identify peak times of hazardous arrivals and shift staff accordingly.

**Expected outcomes:**
- Guaranteed compliance (no accidental >10 in process).
- Reduced hazardous waiting variance (more stable flow), which also stabilizes region batch closure if hazardous orders are part of those batches.

---

## 4) Simulation and validation (constraint-respecting digital twin)

### How simulation fits with process mining
Use process mining to **parameterize and validate** a discrete-event simulation (DES) “digital twin”:
- Arrival patterns by hour/day and by segment (Express/Standard, Cold, Hazardous, Region).
- Service time distributions per activity and segment (not just averages; capture variance).
- Resource calendars (shifts, breaks), pools (ColdStations, StandardStations, QC staff).
- Routing probabilities / rework loops from discovered variants.

### What the simulation must explicitly model (to respect instance-spanning constraints)
1. **Resource contention**
- Packing stations as capacity-constrained servers (standard vs cold pools).
- Assignment rules (current vs proposed dispatching).

2. **Batching logic**
- Region-based batch objects, batch closure rules (current inferred vs proposed max-wait/size rules).
- “Oldest order waits” metric in each batch.

3. **Priority / interruption policy**
- Current policy: preemptive vs non-preemptive (choose what matches reality).
- Proposed policies: reserved capacity, aging, express lane.

4. **Hazardous global cap**
- A shared “token/semaphore” resource with capacity 10 spanning Packing+QC.
- Correct definition of when a token is acquired/released (matches regulation wording).

### Validation approach
- Calibrate the baseline model until it matches historical:
  - end-to-end lead time distribution (median, p90, p95),
  - queue times at Packing/QC,
  - batch waiting distribution,
  - cold station utilization and saturation time,
  - hazardous occupancy profile.
- Then run what-if scenarios for each strategy and combinations, producing confidence intervals via multiple replications.

---

## 5) Monitoring post-implementation (dashboards + conformance + early warnings)

### Core KPI dashboard (overall + segmented)
- **SLA compliance** (% on-time) overall and by:
  - Express vs Standard
  - Cold vs non-cold
  - Hazardous vs non-hazardous
  - Destination Region
- **End-to-end lead time**: median + p90/p95 (tail matters most in fulfillment).

### Constraint-specific operational dashboards
1. **Cold-Packing dashboard**
- `WT_cold_pack` trend (median/p90) and volume of cold orders.
- Cold station utilization + `%time at full capacity (5/5 busy)`.
- Estimated cold queue length over time (from ready timestamps).

2. **Batching dashboard**
- `WT_batch` per region (median/p90).
- Batch size distribution, batch formation time, and “oldest order waiting” per batch.
- Alerts when a region’s open-batch age exceeds the max-wait threshold.

3. **Priority handling dashboard**
- Express queue time at Packing and QC (should be consistently low).
- Standard queue time tail (p95) and overtaking rate.
- If preemption exists: number/duration of interruptions (should drop after policy change).

4. **Hazardous compliance dashboard**
- Time series of `HazOcc(t)` (hazardous in Packing or QC).
- `%time at cap (10)` and any violations (`>10`).
- Hazard gating wait times (`WT_haz_pack`, `WT_haz_qc`) and their correlation with `HazOcc`.

### Process mining conformance checks (rule monitoring)
Set automated checks that trigger alerts:
- Cold-required orders **must** be packed on ColdStations.
- Express orders **must not** exceed allowed wait thresholds at key steps.
- Hazardous concurrency **never** exceeds 10 (and investigate near-cap saturation).
- Batching rule compliance (max-wait respected; no “forgotten” orders stuck post-QC).

### Continuous improvement loop
- Weekly: re-run root cause on late orders using the “system state at ready time” features.
- Monthly/seasonal: refresh simulation parameters and re-optimize batching thresholds and reserved capacities as volumes shift.

---

This approach turns the event log into (1) a quantified breakdown of where time is lost due to *instance-spanning* constraints, (2) an understanding of how the constraints amplify each other, and (3) a set of data-backed policy and capacity changes that can be validated safely through simulation and continuously controlled via conformance + KPI monitoring.