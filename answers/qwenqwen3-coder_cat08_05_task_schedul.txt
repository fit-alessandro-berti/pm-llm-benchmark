# **Advanced Scheduling Strategy for Precision Parts Inc. Using Process Mining**

---

## **1. Analyzing Historical Scheduling Performance and Dynamics**

To effectively address the operational inefficiencies at Precision Parts Inc., we begin by leveraging **process mining** on the rich MES event logs to reconstruct and analyze the actual execution of jobs across the shop floor. This enables us to move beyond intuition-based scheduling decisions and into a data-driven understanding of system behavior.

### **Reconstructing Actual Job Flows and Task Sequences**

Using process mining tools like ProM, Disco, or PM4Py, we parse the event log to generate **process maps** that visualize:
- The complete lifecycle of each job (from release to completion).
- The sequence of tasks per job (e.g., Cutting  Milling  Lathing).
- The interplay between jobs and machines, including setup times and resource contention.

Each trace in the log corresponds to a job, and each event represents a state transition (e.g., task start, end, setup). By mapping these traces, we can identify deviations from standard routings, bottlenecks, and disruptions.

### **Quantifying Key Metrics with Process Mining**

#### **Job Flow Times, Lead Times, Makespan Distributions**
- **Flow Time**: Time from job release to completion. Extracted directly from timestamps.
- **Lead Time**: Time from order receipt to delivery (includes pre-release time if available).
- **Makespan**: Total time to complete all jobs in a batch (for batch analysis).

We compute these using **event log filtering** and **timestamp arithmetic**.

#### **Waiting Times (Queue Times)**
We isolate intervals where a job is in a queue (e.g., “Queue Entry” until “Setup Start”) and aggregate average/median waiting times per machine. This reveals which resources are causing delays.

#### **Resource Utilization**
We calculate:
- **Productive Time**: Sum of actual task durations.
- **Idle Time**: Periods when a machine is not being used.
- **Setup Time**: Time spent preparing for a job.

By grouping events by `Resource ID`, we derive utilization profiles for each machine and operator.

#### **Sequence-Dependent Setup Times**
From the event log, we extract:
- Previous job processed on a machine (`Notes` field).
- Setup start and end timestamps.

We build a **setup matrix** showing how setup time varies based on the transition from one job to another. For example:
```
Setup(JOB-A  JOB-B) = 25 min
Setup(JOB-B  JOB-A) = 10 min
```
This matrix is crucial for optimizing sequencing and dispatching rules.

#### **Schedule Adherence and Tardiness**
We compute:
- **Due Date Deviation**: Actual completion time – Due date.
- **Tardiness Frequency**: % of jobs completed after due date.
- **Earliness**: % of jobs completed significantly early.

We filter jobs by due date and completion status to assess adherence and identify patterns in lateness.

#### **Impact of Disruptions**
We tag events like:
- “Breakdown Start”
- “Priority Change”

We then correlate these with:
- Delays in downstream tasks.
- Increase in WIP.
- Changes in job sequence.

This allows us to measure the **ripple effect** of disruptions and quantify their cost in terms of KPI degradation.

---

## **2. Diagnosing Scheduling Pathologies**

With a clear understanding of historical performance, we can now diagnose systemic inefficiencies:

### **Bottleneck Resources**
Using **resource-based process maps** and **bottleneck analysis**, we identify machines with:
- Highest utilization (>90%).
- Longest average queue times.
- Greatest contribution to job delays.

For example, if `CUT-01` consistently shows high queue times and long setups, it’s a prime bottleneck.

### **Poor Task Prioritization**
We perform **variant analysis**:
- Compare traces of on-time vs. late jobs.
- Identify if high-priority jobs are being delayed due to poor dispatching.

If urgent jobs (JOB-7005) are queued behind low-priority jobs, this indicates a flaw in dispatching logic.

### **Suboptimal Sequencing and Setup Times**
We analyze:
- Sequence of jobs on each machine.
- Corresponding setup times.

If the job sequence leads to frequent, long setups (e.g., alternating between vastly different job types), it’s a major inefficiency.

### **Starvation of Downstream Resources**
We track:
- Queue entry times for subsequent operations.
- Completion times of upstream tasks.

If downstream machines like `MILL-03` frequently sit idle despite upstream activity, it indicates poor coordination or upstream bottlenecks.

### **Bullwhip Effect in WIP**
We plot **WIP levels over time**, grouped by work center. High variability in WIP suggests:
- Poor synchronization of job releases.
- Reactive scheduling causing bursts of activity.

---

## **3. Root Cause Analysis of Scheduling Ineffectiveness**

The root causes of the scheduling issues stem from a combination of **systemic limitations** and **data visibility gaps**:

### **Static Dispatching Rules**
Current rules like FCFS or EDD:
- Don’t consider real-time machine status.
- Ignore sequence-dependent setups.
- Fail to adapt to disruptions.

This leads to suboptimal decisions and cascading delays.

### **Lack of Real-Time Visibility**
Each work center operates in isolation. Without a centralized view:
- Resources are not allocated efficiently.
- Bottlenecks aren’t addressed proactively.

### **Inaccurate Estimations**
Planned durations often differ from actual times due to:
- Operator variability.
- Job complexity.
- Machine wear.

This leads to unreliable Gantt charts and missed due dates.

### **Ineffective Setup Handling**
Without historical setup data, operators and schedulers cannot:
- Sequence jobs optimally.
- Batch similar jobs effectively.

This increases total setup time and reduces throughput.

### **Poor Coordination**
Work centers operate independently, leading to:
- Imbalanced resource loading.
- Inefficient handoffs.

### **Inadequate Disruption Handling**
When disruptions occur:
- There’s no mechanism to dynamically reschedule.
- High-priority jobs are not expedited effectively.

---

## **4. Developing Advanced Data-Driven Scheduling Strategies**

Based on the diagnosis, we propose three data-driven strategies:

---

### **Strategy 1: Enhanced Dynamic Dispatching Rules**

**Core Logic:**
Implement **multi-criteria dispatching rules** that consider:
- Remaining processing time.
- Due date urgency.
- Sequence-dependent setup time.
- Downstream machine load.
- Job priority.

**Process Mining Input:**
- Setup time matrix.
- Historical task durations.
- Resource utilization trends.

**Example Rule:**
```
Priority Score = (1/Due Date) * Weight1 + (Remaining Time) * Weight2 + (Setup Time) * Weight3
```

Weights are tuned using regression or machine learning models trained on historical tardiness data.

**Impact:**
- Reduces tardiness by prioritizing jobs with imminent deadlines and minimal setups.
- Balances load across machines.
- Improves WIP flow.

---

### **Strategy 2: Predictive Scheduling with Digital Twin**

**Core Logic:**
Use **discrete-event simulation** powered by process mining insights to:
- Predict job completion times under various scenarios.
- Identify likely bottlenecks in advance.
- Re-optimize schedule dynamically.

**Process Mining Input:**
- Task duration distributions (normal, lognormal, etc.).
- Routing probabilities.
- Machine breakdown frequency and duration.
- Setup time variability.

**Implementation:**
- Build a **digital twin** of the shop floor in a simulation tool (e.g., AnyLogic, Simio).
- Run Monte Carlo simulations to test schedule robustness.
- Trigger rescheduling when predicted delays exceed thresholds.

**Impact:**
- Proactive scheduling reduces last-minute delays.
- Increased predictability of lead times.
- Better handling of disruptions.

---

### **Strategy 3: Sequence Optimization for Setup Reduction**

**Core Logic:**
Use **clustering and sequencing algorithms** to minimize total setup time:
- Group similar jobs using job attributes (material, dimensions, required tools).
- Apply **Traveling Salesman Problem (TSP)** or heuristic solvers to sequence jobs within clusters.

**Process Mining Input:**
- Setup matrix showing job-to-job transition costs.
- Historical job attributes (from log metadata).

**Implementation:**
- At bottleneck machines, re-sequence job queues to reduce setups.
- Use **constraint programming** or **genetic algorithms** for optimization.

**Impact:**
- Reduces total setup time by up to 30%.
- Increases throughput at bottlenecks.
- Reduces WIP and tardiness.

---

## **5. Simulation, Evaluation, and Continuous Improvement**

### **Simulation Framework**

We develop a **discrete-event simulation model** using process mining-derived parameters:

#### **Inputs from Process Mining:**
- Empirical task time distributions.
- Routing logic (based on event traces).
- Setup time models.
- Breakdown frequency/duration.
- Priority change events.

#### **Scenarios Tested:**
- High load periods.
- Frequent breakdowns.
- Introduction of hot jobs.

We compare:
- Current static dispatching.
- Enhanced dispatching rules.
- Predictive scheduling.
- Setup-optimized sequencing.

### **Evaluation Metrics:**
- % of on-time deliveries.
- Average WIP.
- Makespan.
- Resource utilization.
- Setup time reduction.

### **Continuous Improvement Framework**

We implement a **closed-loop scheduling system** with:

#### **Ongoing Process Mining**
- Weekly re-analysis of logs.
- Detection of new bottlenecks or inefficiencies.
- Drift detection in task times or setup behavior.

#### **KPI Monitoring Dashboard**
Real-time tracking of:
- Tardiness rate.
- WIP levels.
- Machine utilization.

#### **Adaptive Logic**
- Retrain dispatching models with new data.
- Update setup matrices.
- Adjust scheduling weights dynamically based on performance.

---

## **Conclusion**

Precision Parts Inc. can significantly improve its scheduling performance by adopting a **process mining-driven approach**. Through detailed analysis of historical operations, diagnosis of systemic inefficiencies, and implementation of **advanced scheduling strategies**—including dynamic dispatching, predictive scheduling, and setup optimization—the company can:
- Reduce tardiness.
- Lower WIP.
- Improve lead time predictability.
- Increase resource utilization.

Combined with **simulation testing** and **continuous improvement loops**, these strategies form a robust, scalable solution for managing the complexity and variability inherent in high-mix, low-volume job shop environments.