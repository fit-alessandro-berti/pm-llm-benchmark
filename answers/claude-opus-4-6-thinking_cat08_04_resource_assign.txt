# Comprehensive Process Mining Approach for Optimizing Resource Assignment at TechSolve Solutions

---

## 1. Analyzing Resource Behavior and Assignment Patterns

### 1.1 Agent and Tier Performance Metrics

The event log provides a rich dataset from which to extract granular performance and behavioral metrics. I would structure the analysis across three levels: **individual agent**, **tier/team**, and **skill group**.

**Individual Agent Metrics:**

| Metric | Derivation from Event Log | Purpose |
|---|---|---|
| **Case Throughput** | Count of distinct Case IDs where agent appears as resource in "Work" activities, per time period | Measures raw productivity |
| **Average Handling Time (AHT)** | Difference between "Work Lx End" and "Work Lx Start" timestamps, aggregated per agent | Identifies speed variation across agents |
| **Active Working Time vs. Available Time** | Ratio of sum of handling durations to total shift time | True utilization rate |
| **Reassignment Origination Rate** | Count of times an agent triggers a "Reassign" event / total cases handled | Indicates potential skill mismatch or diagnostic difficulty |
| **First-Touch Resolution Rate (L1)** | Cases resolved at L1 by agent without escalation / total cases handled | Measures L1 effectiveness |
| **Ticket Type Distribution** | Frequency distribution of ticket categories and required skills per agent | Reveals whether assignments align with documented skills |
| **SLA Compliance Rate** | Percentage of tickets handled that met SLA target, per agent | Links individual performance to business outcomes |
| **Queue Wait-to-Pickup Time** | Time between "Assign Lx" and "Work Lx Start" | Measures responsiveness and overload signals |

**Tier-Level Metrics:**

For each tier (L1, L2, L3), I would compute:

- **Escalation Rate Out**: Percentage of tickets arriving at a tier that are escalated upward. The event log snippet shows L1 escalating both INC-1001 and INC-1002 — tracking this systematically across all tickets reveals whether L1 escalation rates are excessive relative to ticket complexity.
- **Bounce-Back Rate**: Tickets escalated to a higher tier that are subsequently sent back down (indicating the escalation was unnecessary). This is detectable by a pattern of "Escalate L2"  "Work L2 Start"  "Reassign"  "Assign L1".
- **Intra-Tier Reassignment Rate**: Tickets reassigned *within* the same tier, as seen with INC-1001 being reassigned from Agent B12 to Agent B15 at L2. This directly signals skill mismatch at the point of assignment.
- **Tier Throughput Time**: End-to-end time a ticket spends within a given tier (from assignment to resolution or escalation), broken down by priority and category.
- **Capacity Utilization by Time Window**: Hourly/daily patterns of active agents vs. incoming ticket volume, revealing temporal capacity mismatches.

**Skill Utilization Analysis:**

For each documented skill (e.g., `Networking-Firewall`, `Database-SQL`, `App-CRM`):

- **Demand Frequency**: How often tickets requiring this skill arrive (derived from the `Required Skill` attribute).
- **Supply Depth**: Number of agents possessing this skill across tiers.
- **Skill-Match Rate on First Assignment**: Percentage of times a ticket requiring skill X is initially assigned to an agent who possesses skill X.
- **Specialist Downgrading Rate**: Frequency with which L2/L3 specialists handle tickets whose required skill is `Basic-Troubleshoot` or whose complexity (as measured by resolution time and activity count) is below the tier's typical threshold. If Agent B12 (skills: `App-CRM`, `DB-SQL`) regularly handles tickets that L1 agents resolve elsewhere, that specialist's capacity is being wasted.

### 1.2 Process Mining Techniques for Revealing Actual Assignment Patterns

**Process Discovery and Variant Analysis:**

Using automated process discovery (e.g., Inductive Miner, Heuristics Miner), I would generate the as-is process model filtered by different dimensions:

- **By Priority**: Discover separate process maps for P1, P2, P3, P4 tickets. We expect P1 tickets to have tighter, more streamlined flows. If P2 and P3 flows show extensive looping (repeated escalation/reassignment activities), this directly explains SLA breaches.
- **By Ticket Category / Required Skill**: Discover flows for `Network` vs. `Software-App` vs. `Security` tickets. Some categories may exhibit consistently clean flows while others show chaotic reassignment patterns.

**Variant Analysis** is critical here. I would sort process variants by frequency and identify:

- **Happy Path Variants**: Tickets following a clean flow — Created  Assign L1  Work L1  Resolved. Or Created  Assign L1  Work L1  Escalate L2  Assign L2  Work L2  Resolved.
- **Problematic Variants**: Tickets with multiple reassignments (like INC-1001, which went L1  L2 Agent B12  Reassign  L2 Agent B15). Variants with 3+ reassignment/escalation steps would be flagged. I would measure what percentage of total volume these represent and their disproportionate contribution to SLA breaches.

**Social Network Analysis (Handover of Work):**

By constructing a handover-of-work social network — where nodes are agents (or agent groups/tiers) and directed edges represent ticket handovers — I can reveal:

- **High-frequency handover pairs**: If Agent A05 consistently escalates to Agent B12, who then reassigns to Agent B15, this triple-handover chain is a systemic pattern, not an anomaly. The edge weights quantify frequency.
- **Cluster Analysis**: Agents who frequently pass work among themselves form clusters. If these clusters don't align with the intended tier structure, it reveals informal routing patterns.
- **Centrality Analysis**: Agents with high betweenness centrality are "broker" nodes through whom many tickets pass. These are potential bottlenecks — if they're overloaded, the entire flow stalls. Similarly, agents with very low degree centrality may be underutilized or bypassed by the actual routing logic.
- **Cross-Tier Patterns**: The network reveals whether escalations follow the expected L1L2L3 path or if there are shortcuts (L1L3), backflows (L2L1), or lateral transfers (L2 Agent X  L2 Agent Y) that weren't part of the designed process.

**Role Discovery:**

Using techniques like organizational mining, I would cluster agents based on their *actual activity profiles* (which activities they perform, on what ticket types, at what times) rather than their *assigned* tier. This can reveal:

- Agents formally in L1 who functionally operate as L2 specialists (handling complex tickets without escalation).
- L2 agents whose activity profile is indistinguishable from L1 agents (handling routine tasks).
- De facto "super-users" who act as informal escalation targets regardless of the dispatcher's assignments.

**Comparison with Intended Logic:**

The intended logic is described as "round-robin within tiers with manual escalation decisions." I would formalize this as a normative model and use **conformance checking** to measure deviations:

- If round-robin is intended, the distribution of ticket assignments across agents within a tier should be approximately uniform. I would compute the coefficient of variation (CV) of assignment counts per agent per tier per time period. A high CV indicates the round-robin is either not implemented, not enforced, or frequently overridden.
- If escalation rules specify criteria (e.g., "escalate P2 Network tickets to L2 Networking pool"), I would check whether actual escalation decisions conform to these rules. Decision mining on the escalation activity can reveal what attributes *actually* drive escalation decisions versus what the rules prescribe.

### 1.3 Skill Utilization Depth Analysis

To determine whether specialized skills are being used effectively, I would construct a **Skill Demand-Supply-Utilization Matrix**:

| Skill | Monthly Demand (Tickets) | Agents with Skill | Avg. Tickets/Agent | % of Agent's Time on Skill-Matched Work | % of Skill-Matched Tickets Resolved Without Reassignment |
|---|---|---|---|---|---|
| App-CRM | 320 | 4 (L2), 1 (L3) | 64 | 58% | 72% |
| Networking-Firewall | 180 | 3 (L2), 2 (L3) | 36 | 71% | 85% |
| Database-SQL | 95 | 2 (L2), 1 (L3) | 32 | 45% | 60% |
| Security-IAM | 150 | 2 (L2), 2 (L3) | 38 | 62% | 68% |
| OS-WindowsServer | 240 | 5 (L2), 1 (L3) | 40 | 50% | 75% |

*(Hypothetical illustrative data)*

Key questions this matrix answers:
- **Skill Scarcity**: Is `Database-SQL` a bottleneck skill with only 3 agents and 45% skill-matched utilization (meaning 55% of their time is spent on non-DB tasks)?
- **Specialist Misuse**: If `Networking-Firewall` specialists spend 29% of their time on tickets that don't require firewall skills, that's capacity wasted on tasks others could handle.
- **Skill Gaps**: If `App-CRM` tickets have only a 72% resolution-without-reassignment rate despite being assigned to CRM-skilled agents, the required skill may be misclassified or agents may need deeper training.

I would also overlay a **temporal dimension**: are there predictable spikes in demand for specific skills (e.g., end-of-month Access Management requests, post-patch-Tuesday OS issues) that could inform proactive staffing?

---

## 2. Identifying Resource-Related Bottlenecks and Issues

### 2.1 Skill-Availability Bottlenecks

**Method**: For each `Required Skill`, compute the average queue wait time (time between "Assign Lx" and "Work Lx Start") segmented by skill. If `Database-SQL` tickets average 45 minutes queue wait versus 12 minutes for `OS-WindowsServer`, there is a clear capacity constraint for SQL-skilled agents.

**Quantification**: Cross-reference with SLA data:
- Compute: *"Of all P2 tickets that breached SLA, X% required `Database-SQL` skill, while `Database-SQL` tickets represent only Y% of total P2 volume."* If X >> Y, skill scarcity for SQL is a disproportionate driver of SLA breaches.
- Measure the **skill bottleneck index**: `(Average queue wait for skill S) × (Volume of tickets requiring skill S)` — this product identifies the highest-impact bottlenecks.

### 2.2 Reassignment-Induced Delays

**Method**: For every reassignment event in the log, compute the **reassignment penalty time** — the elapsed time from the reassignment event to the point when the new agent starts productive work. This includes re-queueing, re-assignment, and the new agent's ramp-up (re-reading the ticket).

From the event log snippet, INC-1001's reassignment timeline:
- Agent B12 begins work: 10:05:50
- Reassignment initiated: 11:15:00 (B12 worked ~70 min before determining mismatch)
- New assignment to B15: 11:16:10
- Presumed B15 work start: some later time (not shown, but likely includes queue wait + re-familiarization)

The total penalty is not just the queue wait for B15, but also the **wasted effort** by B12 (70 minutes) and the **diagnostic duplication** by B15 (re-reading the ticket history, estimated at 10-15 minutes based on industry benchmarks).

**Quantification**:
- **Average reassignment count per ticket**: Compute across all tickets. Industry benchmarks suggest each reassignment adds 30-90 minutes to resolution time.
- **Aggregate impact**: If 22% of tickets experience at least one reassignment, and each reassignment adds an average of 55 minutes, the total wasted capacity per month is: `(Monthly ticket volume × 22% × 55 min)`. For 5,000 monthly tickets, that's approximately **1,008 agent-hours/month** lost to reassignment overhead.
- **Reassignment-SLA Correlation**: Build a binary classifier: for each ticket, flag whether it was reassigned (yes/no) and whether it breached SLA (yes/no). Compute the odds ratio. I hypothesize a strong positive correlation (e.g., tickets with reassignments are 3-4× more likely to breach SLA).

### 2.3 Incorrect Initial Assignment Impact

**Method**: Identify tickets where the first "Assign" activity directed the ticket to an agent whose documented skill set does not include the ticket's `Required Skill`. In the snippet, INC-1001 requires `App-CRM` and is initially assigned to Agent A05 at L1 with only `Basic-Troubleshoot` — this is expected for L1 triage. However, when it's escalated to L2 and assigned to Agent B12 (skills: `App-CRM, DB-SQL`), B12 initially appears to be a match. But the reassignment note says "Needs different skill (DB)" — meaning the ticket's required skill was *initially misidentified* as `App-CRM` when it actually required `Database-SQL`.

This reveals two distinct failure modes:
1. **Skill-mismatch assignment**: Agent assigned doesn't have the required skill.
2. **Skill-misidentification**: The `Required Skill` field itself is wrong, causing even a correct skill-based assignment to fail.

**Quantification**:
- **Misidentification Rate**: Percentage of tickets where the `Required Skill` field changes between activities (as with INC-1001 changing from `App-CRM` to `Database-SQL`). If this exceeds 15%, it indicates a systemic categorization problem.
- **First-Assignment Accuracy**: Of tickets escalated to L2/L3, what percentage are assigned to an agent with the correct `Required Skill` on the first try? Values below 80% indicate the dispatching logic is inadequate.

### 2.4 Agent Workload Imbalance

**Method**: Compute concurrent case load per agent over time using timestamp data. Plot each agent's active ticket count as a time series. Calculate:
- **Peak concurrent case load** per agent
- **Gini coefficient** of total case volume distribution within each tier — a Gini closer to 0 indicates balanced distribution; values above 0.3 suggest significant imbalance.
- **Work-in-progress (WIP) per agent at SLA-critical moments**: When a P2 ticket enters the queue, what is the assigned agent's current WIP? High WIP at assignment time predicts longer handling times and SLA breaches.

**Identification of overloaded/underutilized agents**: Rank agents by total active time / available time ratio. Agents consistently above 90% utilization are overloaded (and likely producing lower-quality work due to context-switching), while agents below 50% utilization represent untapped capacity.

### 2.5 Correlation Between Assignment Patterns and SLA Breaches

**Method**: Build a comprehensive ticket-level feature set:
- Number of reassignments
- Number of escalation steps
- Skill match on initial L2/L3 assignment (binary)
- Initial `Required Skill` accuracy (binary: did it change?)
- Assigned agent's concurrent WIP at time of assignment
- Queue wait time before first productive work at each tier
- Time of day / day of week of ticket creation
- Priority and Category

Run a **logistic regression** or **decision tree** with SLA breach (yes/no) as the dependent variable. The feature importances reveal which resource-related factors most strongly predict SLA failure.

**Hypothesized findings** (to be validated with data):
- Tickets with 2 reassignments have an SLA breach rate of ~65% vs. ~12% for zero-reassignment tickets.
- Tickets assigned to agents with WIP  5 at the time of assignment have 3× higher breach probability.
- Initial skill misidentification increases breach probability by 40 percentage points for P2 tickets.

---

## 3. Root Cause Analysis for Assignment Inefficiencies

### 3.1 Systematic Root Cause Exploration

I would investigate the following root causes using specific analytical techniques:

**Root Cause 1: Round-Robin Ignores Skills and Workload**

The current round-robin assignment within tiers treats all agents as interchangeable. However, agents within L2 have diverse skill sets (e.g., B12 has `App-CRM, DB-SQL` while B08 might have `Networking-Firewall, OS-WindowsServer`). A round-robin dispatcher assigning a Network ticket to B12 creates an immediate mismatch.

*Evidence from process mining*: Compute the conditional probability `P(Reassignment | Skill Mismatch at Assignment)` versus `P(Reassignment | Skill Match at Assignment)`. If the former is 5-10× higher, the round-robin is directly causing reassignment chains.

**Root Cause 2: Inaccurate Agent Skill Profiles**

If the skill profiles in the HR/ITSM system haven't been updated to reflect actual competencies (agents who've learned new skills through experience, or whose skills have atrophied), assignments based on these profiles will be flawed.

*Evidence from process mining*: Compare the documented skill profile of each agent against their *revealed skills* — derived from which ticket types they successfully resolve (i.e., resolve without reassignment). If Agent B12 is documented as `App-CRM, DB-SQL` but consistently resolves `OS-WindowsServer` tickets successfully and reassigns `DB-SQL` tickets, their profile is inaccurate.

**Root Cause 3: Poor Initial Ticket Categorization**

The INC-1001 example is illustrative: initially categorized as `App-CRM` required, but actually needing `Database-SQL`. If L1 agents or the web portal's category selection don't accurately identify the required skill, all downstream routing will be wrong.

*Evidence*: Track the `Required Skill` field across the lifecycle of each ticket. Calculate the **re-categorization rate** — the percentage of tickets where this field changes at least once after initial creation. Segment by input channel (phone vs. email vs. web portal) and by the L1 agent who performed initial triage. Some channels or agents may have systematically higher misclassification rates.

**Root Cause 4: Lack of Real-Time Workload Visibility**

If dispatchers assign tickets based on a static roster without seeing each agent's current queue depth, they cannot make load-balanced assignments.

*Evidence*: Correlate queue wait time with the assigned agent's WIP at the moment of assignment. If tickets assigned to agents with WIP > 5 consistently wait >30 minutes while tickets assigned to agents with WIP < 2 wait <5 minutes, the dispatcher lacks — or fails to use — workload information.

**Root Cause 5: L1 Under-Empowerment Leading to Excessive Escalation**

If L1 agents escalate tickets they could resolve (with appropriate tools, knowledge base access, or training), they're creating unnecessary load on L2/L3.

*Evidence from process mining*: 
- Compute L1 resolution rates by ticket category and required skill. If some L1 agents resolve 40% of `Software-App` tickets at L1 while others escalate 95% of the same type, there's a training/empowerment gap.
- Identify tickets escalated from L1 to L2 where the L2 agent resolves them in under 10 minutes with a single activity. These "quick L2 resolutions" suggest the ticket was within L1's potential capability. If these represent 15-25% of escalated tickets, significant L2 capacity could be freed.

### 3.2 Variant Analysis and Decision Mining

**Variant Analysis: Smooth vs. Problematic Cases**

Split the case population into two groups:
- **Group A (Smooth)**: Tickets resolved with 1 escalation and 0 reassignments, within SLA.
- **Group B (Problematic)**: Tickets with 1 reassignment, 2 escalations, or SLA breach.

Compare these groups across dimensions:

| Dimension | Group A (Smooth) | Group B (Problematic) |
|---|---|---|
| Avg. Required Skill rarity (agents with skill / total agents) | 0.35 | 0.12 |
| Skill match on first L2 assignment | 91% | 47% |
| Initial categorization accuracy | 94% | 61% |
| Avg. assigned agent WIP at assignment | 2.3 | 5.8 |
| Input channel distribution | 60% web portal | 55% phone |
| L1 handling time before escalation | 18 min | 8 min |
| Day-of-week pattern | Evenly distributed | 40% Monday |

*(Hypothetical — but this comparative table immediately identifies discriminating factors.)*

The finding that Group B tickets have lower initial categorization accuracy (61% vs. 94%) and lower skill match rates (47% vs. 91%) pinpoints categorization and dispatching as the primary failure points.

The observation that Group B L1 handling times are shorter (8 min vs. 18 min) might suggest that L1 agents are "giving up" too quickly on complex tickets rather than performing thorough triage — or that tickets arriving in high-volume periods (Monday) receive less L1 attention.

**Decision Mining at Escalation Points:**

Apply decision mining (e.g., decision trees) at the "Escalate L2" activity to identify which ticket attributes predict escalation. The decision tree might reveal:

```
IF Required Skill  {Networking-Firewall, Security-IAM, Database-SQL}  Escalate (95%)
IF Required Skill = App-CRM AND Ticket Description contains "integration"  Escalate (88%)
IF Required Skill = OS-WindowsServer AND Priority = P4  Resolve at L1 (72%)
IF Required Skill = App-CRM AND Priority = P3  Escalate (55%) / Resolve (45%)
```

The last rule reveals an opportunity: nearly half of P3 App-CRM tickets are resolved at L1. By understanding what distinguishes the resolved vs. escalated cases (perhaps specific sub-issues), we can provide L1 agents with targeted knowledge base articles or scripts to increase their resolution rate for this common ticket type.

**Decision Mining at Dispatcher Assignment:**

Apply decision mining at the "Assign L2" activity to understand what drives the dispatcher's agent selection. If the decision tree shows the dispatcher primarily selects based on agent name order (alphabetical) or recent assignment history (confirming round-robin) rather than skill match or workload, the root cause of misassignment is confirmed as a process design deficiency, not individual dispatcher error.

---

## 4. Data-Driven Resource Assignment Strategies

### Strategy 1: Skill-Weighted, Proficiency-Tiered Routing

**Issue Addressed**: Skill mismatch at assignment, reassignment chains, specialist misuse.

**Description**: Replace round-robin dispatching with an algorithm that matches ticket `Required Skill` to agent skill profiles, weighted by proficiency level. Each agent's skills are rated on a proficiency scale (1 = basic familiarity, 2 = competent, 3 = expert) derived from process mining data.

**Proficiency Derivation from Process Mining Data**:
- **Resolution Rate for Skill**: Percentage of tickets requiring skill S that the agent resolves without reassignment  higher rate = higher proficiency.
- **Average Handling Time for Skill**: Faster resolution relative to peers  higher proficiency.
- **Reassignment Origination Rate for Skill**: Lower rate  higher proficiency.
- **Escalation Avoidance**: If the agent frequently resolves tickets at their tier that others escalate  higher proficiency.

Combine these into a composite **Proficiency Score** per agent per skill.

**Assignment Logic**:
```
For incoming ticket T requiring skill S at tier N:
1. Filter agents at tier N possessing skill S
2. Among qualifying agents, rank by:
   a. Proficiency Score for skill S (weight: 0.5)
   b. Current WIP inverse — fewer current tickets = higher rank (weight: 0.3)
   c. SLA urgency multiplier — P1/P2 tickets prioritize highest-proficiency agents (weight: 0.2)
3. Assign to top-ranked available agent
4. If no agents at tier N have skill S and WIP < threshold, escalate to tier N+1
```

**Data Required**: Agent skill profiles (initially from HR, continuously updated via process mining), real-time WIP per agent, ticket `Required Skill` attribute, priority.

**Expected Benefits**:
- **50-70% reduction in intra-tier reassignments** (based on the gap between current 47% skill-match rate for problematic tickets vs. a target of >90%).
- **20-30% reduction in average resolution time** for tickets currently experiencing reassignment delays.
- **Improved SLA compliance** for P2/P3 tickets, targeting a 15-25 percentage point improvement.

---

### Strategy 2: Predictive Ticket Classification and Skill Requirement Engine

**Issue Addressed**: Inaccurate initial categorization, Required Skill misidentification, incorrect escalation.

**Description**: Deploy a machine learning model trained on historical ticket data to predict the *true* required skill and anticipated complexity at ticket creation time, overriding or augmenting the manual/portal-selected category.

**Model Design**:
- **Training Data**: Historical tickets with their *final* required skill (after any re-categorizations) as the label. Features include:
  - Ticket description text (NLP features: TF-IDF, word embeddings)
  - Initial category selected by user/L1
  - Input channel
  - Reporter's department/role
  - Time of day / day of week
  - Historical patterns for this reporter (if available)
- **Output 1**: Predicted Required Skill (classification)
- **Output 2**: Predicted Complexity Level (simple/moderate/complex) — derived from historical resolution tier (L1/L2/L3) and handling time for similar tickets
- **Output 3**: Confidence Score — if below threshold, flag for manual L1 triage

**Leveraging Process Mining Insights**:
- The variant analysis identified that 39% of problematic tickets had incorrect initial categorization. The model targets this specific failure.
- Decision mining on re-categorization events reveals which initial categories are most error-prone (e.g., "Software-App" tickets often reclassified to "Database" issues).
- The model training uses the *corrected* skill labels from the event log (the final Required Skill value), learning from the organization's own correction patterns.

**Data Required**: Historical ticket descriptions and metadata, final resolution skill assignments, event log with re-categorization events.

**Expected Benefits**:
- **Reduction in initial misclassification from ~30% to <10%**, dramatically reducing downstream reassignments.
- **Accurate complexity prediction enables direct-to-L2 routing** for tickets predicted as complex, bypassing L1 triage for cases where L1 escalation is near-certain (saving 15-25 minutes per ticket).
- **15-20% improvement in first-assignment accuracy** at L2/L3 level.

---

### Strategy 3: Dynamic Workload Balancing with Real-Time Demand-Capacity Alignment

**Issue Addressed**: Uneven workload distribution, agent overload, temporal demand mismatches, underutilization.

**Description**: Implement a real-time workload management system that dynamically adjusts assignment pools and, where feasible, agent allocation across tiers/skill groups based on current demand patterns.

**Component A — Workload-Aware Assignment Queue:**

Rather than a simple FIFO queue per tier, implement a priority-weighted queue that considers:
- Ticket priority (P1 > P2 > P3 > P4)
- Time-to-SLA-breach (tickets closer to breach get priority)
- Assigned agent's current workload (tickets route to agents with lower WIP first)

**Component B — Dynamic Agent Flex-Pooling:**

Process mining analysis reveals temporal demand patterns (e.g., Monday morning spike, end-of-month access requests). Based on these patterns:
- Maintain a pool of "flex agents" — L1 agents with partial L2 skills — who can be dynamically promoted to handle specific L2 ticket types during surge periods.
- When `Database-SQL` queue depth exceeds threshold T (derived from historical analysis as the point beyond which SLA breach probability exceeds 50%), automatically activate flex agents with SQL competency.

**Component C — Agent Utilization Dashboard with Rebalancing Alerts:**

Real-time dashboard showing:
- Per-agent WIP, current utilization, and projected SLA risk for their assigned tickets.
- Per-skill queue depth vs. available agent capacity.
- Automated alerts when: (a) any agent's WIP exceeds historical 90th percentile; (b) any skill queue depth exceeds SLA-safe threshold; (c) Gini coefficient of workload distribution within a tier exceeds 0.3.

**Leveraging Process Mining Insights**:
- The workload imbalance analysis identified specific agents consistently >90% utilized while peers were <50%. This strategy directly addresses the Gini coefficient gap.
- Temporal demand patterns mined from the event log inform proactive flex-pool scheduling (e.g., scheduling additional Networking-Firewall flex agents for Tuesday mornings when firewall change-related tickets historically spike post-Monday-maintenance-window).
- The correlation analysis showing that agent WIP > 5 at assignment time triples SLA breach probability provides the evidence-based threshold for the assignment algorithm.

**Data Required**: Real-time ticket queue data, agent status (available/busy/offline), historical demand patterns (from process mining), agent skill profiles with proficiency levels, SLA clock for each ticket.

**Expected Benefits**:
- **40-60% reduction in workload Gini coefficient** within tiers.
- **25-35% reduction in average queue wait time** for skill-scarce categories.
- **Flexible capacity response** to demand surges without additional headcount, estimated to absorb 15-20% demand spikes through flex-pooling alone.
- **Reduced burnout and turnover** for overloaded specialists — a significant indirect cost saving.

---

### Summary Comparison of Strategies

| Strategy | Primary Issue Addressed | Key Metric Impact | Implementation Complexity |
|---|---|---|---|
| Skill-Weighted Routing | Skill mismatch, reassignment chains | -60% reassignments, +20% SLA compliance | Medium |
| Predictive Classification | Misclassification, wrong routing | -65% reclassification rate, -20% resolution time | High (requires ML pipeline) |
| Dynamic Workload Balancing | Uneven load, temporal mismatches | -40% workload imbalance, -30% queue wait | Medium-High |

These strategies are **complementary and mutually reinforcing**: accurate classification (Strategy 2) improves skill-based routing (Strategy 1), while workload awareness (Strategy 3) ensures that skill-matched agents aren't selected when already overloaded.

---

## 5. Simulation, Implementation, and Monitoring

### 5.1 Pre-Implementation Simulation

**Approach**: Use **business process simulation** (tools such as AnyLogic, Simul8, or simulation modules within process mining platforms like Celonis Simulation or BIMP) parameterized with the mined process models and resource characteristics.

**Building the Simulation Model:**

1. **Process Model**: Use the discovered process model as the simulation skeleton — activities, branching probabilities (escalation rates, reassignment probabilities), and routing logic.

2. **Arrival Rates**: Extract inter-arrival time distributions from the event log, segmented by ticket priority, category, and time-of-day/day-of-week. Fit appropriate distributions (Poisson for arrival counts, exponential/gamma for inter-arrival times).

3. **Processing Times**: For each activity at each tier, extract handling time distributions per agent skill group. Use the empirical distributions rather than assuming normality.

4. **Resource Pools**: Model agents with their documented (and process-mining-refined) skill profiles, availability schedules, and tier assignments.

5. **Assignment Logic**: This is the key variable. Create three simulation configurations:
   - **Baseline (As-Is)**: Current round-robin + manual escalation logic.
   - **Scenario 1**: Skill-weighted routing (Strategy 1).
   - **Scenario 2**: Strategies 1 + 2 (skill routing + predictive classification, modeled by reducing misclassification probability from 30% to 10%).
   - **Scenario 3**: All three strategies combined.

**Simulation Outputs to Compare:**

| KPI | Baseline (Simulated) | Scenario 1 | Scenario 2 | Scenario 3 |
|---|---|---|---|---|
| Average End-to-End Resolution Time | x hours | y hours | z hours | w hours |
| P2 SLA Compliance Rate | a% | b% | c% | d% |
| P3 SLA Compliance Rate | a'% | b'% | c'% | d'% |
| Average Reassignment Count per Ticket | e | f | g | h |
| Agent Utilization Gini Coefficient | j | k | l | m |
| L2/L3 Specialist Utilization on Skill-Matched Work | n% | o% | p% | q% |

**Sensitivity Analysis**: Vary key parameters (ticket volume ±20%, agent headcount ±10%, skill demand shifts) to test robustness of each strategy under different operating conditions.

**What-If Analysis**: 
- *"What if we train 5 L1 agents in basic `App-CRM` troubleshooting?"*  Modify the resource pool, re-simulate, and measure the impact on L2 CRM queue depth and overall resolution time.
- *"What if P2 ticket volume increases 30% next quarter?"*  Stress-test the strategies to identify breaking points.

The simulation provides quantitative confidence in the expected ROI of each strategy before incurring implementation costs, and identifies which combinations yield the greatest improvement.

### 5.2 Post-Implementation Monitoring Plan

**Monitoring Framework: Process Mining Dashboard for Resource Assignment Effectiveness**

I recommend a continuous monitoring dashboard built on live event log data (streamed or batch-imported daily), structured into four layers:

**Layer 1 — Executive KPIs (Real-Time)**

| KPI | Target | Alert Threshold |
|---|---|---|
| Overall SLA Compliance Rate (by Priority) | P1: 99%, P2: 95%, P3: 90%, P4: 85% | Below target by >3pp for 3 consecutive days |
| Average Resolution Time (by Priority) | P2: <4h, P3: <8h | Above target by >20% |
| Average Reassignment Count per Ticket | <0.15 (down from ~0.5) | >0.25 for any week |
| First-Call Resolution Rate (L1) | >35% (up from ~20%) | Below 30% for any week |
| Skill-Match Rate at First L2/L3 Assignment | >90% | Below 85% |

**Layer 2 — Operational Process Views (Daily)**

- **Live Process Map with Bottleneck Overlay**: Continuously mined process map colored by average waiting time at each activity. Reassignment loops should shrink over time; any re-emergence triggers investigation.
- **Conformance Checking Dashboard**: Automated comparison of new tickets' flows against the target process model. Track the percentage of non-conformant cases (deviations from expected routing) — this should decrease and stabilize.
- **Queue Depth by Skill**: Real-time visualization of ticket backlogs per required skill, with SLA countdown timers. Enables proactive flex-pool activation.

**Layer 3 — Resource Analytics (Weekly)**

- **Agent Performance Heatmap**: Matrix of agents × metrics (AHT, reassignment rate, resolution rate, utilization) with color-coded performance relative to tier benchmarks.
- **Workload Distribution Chart**: Box-and-whisker plots of daily case count per agent within each tier. Monitor for re-emergence of imbalance (Gini coefficient trend).
- **Handover Network Trend**: Monthly social network analysis snapshots. Track changes in network density (fewer handovers = better), centrality distribution (less concentrated = less bottleneck risk), and cross-tier flow patterns.
- **Skill Proficiency Score Drift**: Track each agent's proficiency scores over time. Identify agents developing new competencies ( update their profile and expand routing eligibility) or declining performance in specific skill areas ( training needs).

**Layer 4 — Continuous Improvement Analytics (Monthly)**

- **Predictive Model Accuracy Monitoring**: Track the predictive classification model's precision and recall on a rolling basis. If accuracy degrades (concept drift — e.g., new application types, new ticket patterns), retrain the model.
- **Strategy Impact Attribution**: Compare SLA compliance, reassignment rates, and resolution times month-over-month, with attribution to specific strategy components. Use controlled rollout (e.g., A/B testing across agent groups) where possible.
- **Emerging Pattern Detection**: Use process mining's drift detection capabilities to identify shifts in ticket patterns, new failure modes, or changes in agent behavior that may require strategy adjustments.
- **Feedback Loop Closure**: Monthly review combining quantitative dashboard data with qualitative feedback from agents and dispatchers. Agents often identify issues that data alone doesn't surface (e.g., "the routing is sending me tickets I'm technically skilled for, but they require access to a system I don't have permissions for").

**Governance and Iteration:**

- Establish a monthly **Process Mining Review Board** including service desk management, dispatcher leads, and the analytics team.
- Define a **strategy tuning protocol**: If any Layer 1 KPI falls below its target for two consecutive weeks, trigger an investigation using Layer 2-3 analytics, followed by parameter adjustments (e.g., modifying proficiency weights, retraining the predictive model, adjusting flex-pool thresholds).
- Schedule **quarterly model refreshes**: Re-mine the full process to capture structural changes, update proficiency scores, recalibrate simulation models with recent data, and validate that the strategies remain optimally configured.

---

## Conclusion

The approach outlined above transforms TechSolve's resource assignment from a reactive, rule-of-thumb process into a data-driven, continuously optimizing system. The key insight is that process mining doesn't merely *diagnose* the current state — it provides the *continuous feedback loop* necessary to maintain optimization over time. By combining social network analysis, variant comparison, decision mining, and predictive analytics with robust simulation and monitoring, TechSolve can expect measurable improvements across all pain points: SLA compliance, reassignment frequency, specialist utilization, and workload equity. The three proposed strategies address complementary failure modes and, when deployed in concert, create a self-reinforcing system where accurate classification feeds skill-matched routing, which operates within a workload-balanced framework — all continuously monitored and refined through process mining.