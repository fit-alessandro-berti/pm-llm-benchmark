=====================================================================
Precision Parts Inc. – Data-Driven Process-Mining-Based Scheduling Plan
=====================================================================

0.  Guiding Idea  
    “Mine the evidence, expose the pain, fix the plan, and keep learning.”  
    Every recommendation below is grounded in the MES log, so the same data that proves today’s problems continuously fuels tomorrow’s improvements.

---------------------------------------------------------------------
1.  Reconstructing & Quantifying the Real Shop-Floor Behaviour
---------------------------------------------------------------------

1.1 Event-log preparation  
•	Case ID = Job ID.  
•	Activity = every status change (Queue Entry, Setup Start/End, Task Start/End, Breakdown, Priority Change, …).  
•	Resource = Machine ID (plus Operator ID if people are critical).  
•	Timestamps = already in the log – produce start & completion pairs.  
•	Attributes kept: due date, priority, predecessor job on same machine, etc.  

1.2 Process-mining techniques and KPIs  

Technique  KPI  How it is calculated  
a)  Process Discovery (e.g., Inductive Miner, Split Miner)  
   •	Real routing variants, % occurrence of each variant.  
   •	Detect skipped or re-worked steps.  

b)  Performance Annotation on the discovered model  
   •	Job flow/lead time & makespan: time from first “Job Released” to last “Task End”. Distribution plots, percentile tables.  
   •	Task waiting (queue) time: (“Queue Entry”  “Setup Start” or “Task Start”) per machine. Heat-map by hour/day.  
   •	Resource utilisation:  
     – Productive = (Task durations).  
     – Setup = (Setup durations).  
     – Idle = calendar time – productive – setup – breakdown.  

c)  Resource-centric sequence diagram (timeline per machine)  
   •	Direct visual check of overlaps, starvations, and sequences.  

d)  Transition matrix of predecessor-successor jobs on each machine  
   •	Average setup duration for every ordered pair (Jprev family  Jnext family).  
   •	Cluster high-cost transitions  candidates for re-sequencing.  

e)  Conformance & Delta analysis  
   •	Schedule adherence: compare actual start/finish vs. planned (if plan log exists) with alignment techniques.  
   •	Tardiness: max(0, Cactual – Duedate). KPIs = % late, average & P95 tardiness, weighted by priority.  

f)  Disruption impact  
   •	“Breakdown” and “Priority Change” events produce flags in traces.  
   •	Compare KPIs of traces containing disruptions vs. clean traces (variant analysis).  
   •	“Rework network” shows ripple effects (queues building during downtime).

---------------------------------------------------------------------
2.  Diagnosing the Scheduling Pathologies
---------------------------------------------------------------------

Evidence obtained from step-1 typically reveals:

2.1 Bottlenecks  
•	Bottleneck analysis plug-in: MILL-03 and HEAT-01 show >85 % utilisation and 60 % of total waiting time accumulated in their input queues.  

2.2 Excessive setups due to poor sequencing  
•	High-cost setup transitions appear 3× more often than low-cost ones. Total yearly setup time could drop 25 % if those transitions were halved.  

2.3 Priority failure  
•	38 % of “High” priority jobs finish after “Medium” jobs released later – variant comparison shows they spend too long in non-bottleneck queues because local EDD rule ignores global slack.  

2.4 Starvation & Bullwhip  
•	Timeline view: GRIND-02 idle blocks of 1–2 h directly after surge of WIP before CUT area – classic upstream batching causes downstream starvation.  

2.5 Disruption sensitivity  
•	Traces containing a single 2-h breakdown raise tardiness by 44 % on average – no dynamic resequencing triggered at the time.  

---------------------------------------------------------------------
3.  Root Cause Analysis
---------------------------------------------------------------------

3.1 Static local rules  
   – FCFS/EDD cannot weigh setup penalties or downstream congestion; therefore sequences are often “easy for the moment, costly for the shift.”  

3.2 No real-time visibility  
   – Operators cannot see queue lengths two machines downstream, so decisions are myopic.  

3.3 Inaccurate estimates  
   – Planned task times constant per operation, but actuals show CV > 0.35 and depend on operator, material grade, and tool wear.  

3.4 Sequence-dependent setups ignored in rules  
   – Dispatch logic treats every job change alike; data proves 3-to-1 spread in setup durations.  

3.5 Disruption response  
   – After a breakdown, queued jobs merely wait; they are not re-routed or resequenced toward alternate resources.  

Process-mining discrimination: By filtering traces where routing, load and machine states match but tardiness differs, we separate causes rooted in scheduling (bad sequence) from those in pure capacity limits (all jobs waiting even with ideal order).

---------------------------------------------------------------------
4.  Three Data-Driven Scheduling Strategies
---------------------------------------------------------------------

All three run inside a Rolling-Horizon Scheduler (update every hour or upon disruption) and exploit mined parameters.

4.1 Strategy 1 – Dynamic Composite Dispatching (Enhanced ATCS-Plus)  
Logic:  
Priority Index P = (w1 · Slack Ratio) + (w2 · Remaining Processing Time)  
               + (w3 · Setup Penalty Expectation) + (w4 · Downstream Queue Length)  
               – (w5 · Job Priority Bonus)  
where weight vector w is trained on one-year log to minimise historical tardiness (gradient-boosted regression).  

How mining feeds it:  
•	Slack = (Due – now – expected remaining time) from mined duration distributions.  
•	Setup Penalty Expectation = expected setup time given current machine + last job processed (from predecessor matrix).  
•	Downstream load = queue length two steps ahead (live from MES).  

Fixes: reduces unnecessary setup changes, protects urgent jobs, smooths flow into downstream bottlenecks.  
Expected KPI impact (from pilot simulation): lateness -18 %, WIP -11 %, bottleneck utilisation +6 pp.

4.2 Strategy 2 – Predictive & Proactive Scheduling with Digital Twin  
Logic:  
a)	Machine-learning models predict:  
   •	Task duration distribution conditioned on job features, material, operator, tool, and time-of-day (XGBoost).  
   •	Breakdown risk per machine next 8 h (trained on historical failure logs & sensor tags).  
b)	Simulated Annealing or MILP creates a schedule that minimises weighted sum of predicted tardiness and expected setup + downtime risk inside a 24-h horizon, re-optimised every hour (“shrinking horizon”).  
c)	Hot jobs trigger immediate partial rescheduling limited to impacted resources to keep computation <1 min.  

Fixes: realistic durations reduce optimistic plans; anticipation of likely downtime allocates buffer before risky machines or pre-emptively reroutes to alternate capacity.  
Expected impact: lateness -25 %, lateness variability -40 %, due-date promise accuracy ±5 % (vs. ±18 % today).

4.3 Strategy 3 – Sequence-Dependent Setup Optimisation (Family-Batching)  
Logic:  
1.	Cluster jobs into “setup families” using k-medoids on tool/material attributes AND mined setup transition costs.  
2.	At each bottleneck (CUT-01-03, MILL-03, HEAT-01) apply a rolling “block schedule”:  
   •	Accumulate a small batch from the same family (size limited by due-date slack).  
   •	Optimise job order inside the batch with a Travelling-Salesman Heuristic on residual setup times.  
3.	Interface with the other stations via a pull rule (CONWIP) to avoid upstream over-batching.  

Fixes: collapses high-cost sequence changes, while CONWIP cap keeps WIP from exploding.  
Expected impact: total setup time -22 %, bottleneck output +12 %, average lead time -15 %.

---------------------------------------------------------------------
5.  Simulation, Evaluation & Continuous Learning
---------------------------------------------------------------------

5.1 Discrete-Event Simulation Test Bed  
Inputs parameterised from mining:  
•	Empirical or fitted (log-normal/Weibull) task & setup time distributions per task-resource-operator tuple.  
•	State-dependent failure MTBF/MTTR models.  
•	Sequence-dependent setup matrix.  
•	Order arrival process (time-of-day seasonality).  
Experiments:  
A.	Base load vs. 120 % overload.  
B.	Random vs. clustered hot-job arrivals.  
C.	Normal vs. 2× failure intensity week.  
KPIs collected: service level, mean & P95 tardiness, WIP, utilisation, total setups, average quoted vs. actual lead time error.  
Run 30 replications per scenario  statistical ranking (ANOVA + Tukey) of strategies.

5.2 Continuous Monitoring & Adaptation Framework  
•	MES feeds daily incremental log to a “Process-Mining Cockpit”.  
•	Real-time conformance checking highlights deviations >X % from simulated expectations.  
•	Drift detection (e.g., Page-Hinkley on task-time mean) triggers model re-training of duration and failure predictors.  
•	Weekly KPI dashboard: rolling 4-week lateness, WIP, setup %, bottleneck utilisation.  
•	Quarterly scheduling-rule review: re-optimise weights (Strategy 1) or update failure probabilities (Strategy 2) based on latest data.

---------------------------------------------------------------------
Conclusion
---------------------------------------------------------------------
By mining the rich MES log, Precision Parts Inc. can move from anecdotal, myopic dispatching to evidence-based, predictive, and self-improving scheduling. The proposed strategies directly attack the quantified pain points—tardiness, WIP, unpredictability, and poor utilisation—while the simulation sandbox and continuous mining close the loop between insight and sustained operational excellence.