# Advanced Process Mining-Driven Scheduling Optimization for Precision Parts Inc.

---

## Section 1: Analyzing Historical Scheduling Performance and Dynamics

### 1.1 Event Log Preparation and Process Model Reconstruction

The foundation of any meaningful process mining analysis is a correctly structured event log. From the MES data, I would construct a case-centric event log where each **Case ID** represents a job (e.g., JOB-7001), each **Activity** represents a discrete task event (Queue Entry, Setup Start, Setup End, Task Start, Task End), and **Timestamps** provide the temporal backbone. Critical attribute enrichment is necessary: each event must carry contextual attributes including `Resource`, `Operator ID`, `Order Priority`, `Due Date`, `Setup Required`, `Previous Job on Resource`, `Planned Duration`, and `Actual Duration`.

**Preprocessing steps include:**
- **Timestamp alignment:** Synchronize MES clock signals across machines to eliminate spurious ordering artifacts.
- **Event correlation:** Link setup events to their parent task and capture the `previous_job` attribute from machine state transitions — essential for sequence-dependent setup analysis.
- **Case augmentation:** Enrich each trace with job-level attributes (order complexity, material type, customer tier) that may correlate with performance variation.
- **Noise filtering:** Remove duplicate sensor events, handle missing timestamps via interpolation, and flag incomplete traces (e.g., jobs still in-process at log extraction time).

Using tools such as **ProM**, **Celonis**, **Disco**, or **PM4Py**, I would apply the **Inductive Miner** or **Split Miner** algorithms to discover the *actual* process model from traces. In a high-mix job shop, this will not yield a simple linear flow but rather a complex **spaghetti model** reflecting diverse routings. To manage this, I would apply **clustering of traces by routing variant** (using edit distance on activity sequences or process tree similarity) before discovering per-cluster models. This reveals that JOB-7001's routing (Cutting  Milling  Grinding  Inspection) differs fundamentally from JOB-7005's (CNC Milling  Lathing  Heat Treatment  Inspection), and that treating them identically under the same dispatching rule is fundamentally flawed.

---

### 1.2 Quantifying Core Performance Metrics

#### **Flow Times, Lead Times, and Makespan Distributions**

- **Flow time per job** = Timestamp(`Job Completed`)  Timestamp(`Job Released`). I would extract this for all completed jobs over the 12-month log, compute distributional statistics (mean, median, P90, P99), and segment by job complexity tier, routing length, and priority class.
- **Lead time analysis** uses **Performance Spectrum** visualization in ProM to show where time accumulates across the process — revealing whether delays are concentrated in early operations (upstream bottlenecks) or late ones (downstream contention).
- **Makespan distribution** for the shop as a whole is computed by examining daily/weekly aggregate completion counts and comparing against capacity targets.
- **Sojourn time decomposition** at each work center: Total time at a workstation = Queue Wait + Setup Time + Processing Time + Post-processing Wait. I would extract each component using the four-event boundary (Queue Entry  Setup Start  Setup End  Task Start  Task End  Next Queue Entry).

#### **Task Waiting Times (Queue Times)**

Queue time for each task instance = Timestamp(`Setup Start` or `Task Start` if no setup)  Timestamp(`Queue Entry`). I would aggregate these by:
- **Work center level:** Mean and coefficient of variation (CV) of queue times at CUT-01, MILL-03, etc.
- **Temporal patterns:** Queue time heatmaps by hour-of-day and day-of-week to identify systematic congestion periods.
- **Job priority conditioning:** Compare queue times for High vs. Medium vs. Low priority jobs at each resource. If High priority jobs wait as long as Medium priority jobs at bottleneck machines, this directly evidences poor prioritization.

Using **queue-time dotted charts** (time on X-axis, cases on Y-axis, colored by queue duration), I can visually identify periods of systemic congestion versus isolated incidents.

#### **Resource Utilization Analysis**

For each machine resource, I compute a **time-state model** with four mutually exclusive states:

| State | Derivation from Log |
|-------|-------------------|
| **Productive Processing** | Task Start  Task End |
| **Setup** | Setup Start  Setup End |
| **Idle (Available)** | Task End  Next Queue Entry or Setup Start on same machine |
| **Unavailable (Breakdown/Maintenance)** | Breakdown Start  Breakdown End (Resource events) |

Utilization metrics:
- **Productive Utilization** = (Processing Times) / Total Available Time
- **Setup Ratio** = (Setup Times) / Total Available Time
- **Effective Utilization** = (Processing + Setup) / Available Time
- **Idle Ratio** = (Idle Times) / Available Time

These are computed per machine per shift, allowing identification of machines with chronically high effective utilization (>85%, candidates for bottlenecks) versus machines with high idle ratios interspersed with utilization spikes (indicating starvation and irregular feeding patterns).

#### **Sequence-Dependent Setup Time Analysis**

This is one of the richest analytical opportunities. The event log captures `Previous Job on Resource` at Setup Start events. I construct a **Setup Time Matrix** as follows:

**Step 1:** For each Setup Start event on machine M, extract: `(job_from, job_to, setup_duration_actual, machine_id)`.

**Step 2:** Encode job attributes relevant to setup complexity: material type (Steel vs. Aluminum vs. Titanium), part family (prismatic vs. rotational vs. sheet), tooling requirements, dimensional class (tolerance tier), and surface finish requirement.

**Step 3:** Build a **setup time model** using the job attribute pairs. Rather than a naïve job-to-job transition matrix (which would be sparse given high mix), I construct a **feature-based regression model**:

```
Setup_Duration = f(_material_type, _part_family, _tolerance_tier, 
                   _tooling_group, machine_id, operator_id)
```

Using gradient boosted trees or random forests on historical setup events (a year of logs should provide thousands of setup observations), I can predict setup time for any job pair with reasonable accuracy. The **SHAP values** from this model reveal which transition attributes drive the longest setups — informing the clustering strategy in Section 4.

**Step 4:** Visualize setup duration distributions as a **heat map** of average setup times across material-type transitions for each machine. Cells with dark colors (long setups) at bottleneck machines represent high-value optimization targets.

#### **Schedule Adherence and Tardiness**

- **Tardiness per job** = max(0, Actual_Completion_Time  Due_Date)
- **Lateness** = Actual_Completion_Time  Due_Date (signed, allowing measurement of earliness)
- **Tardiness rate** = Number of late jobs / Total jobs completed
- **Weighted tardiness** = (Priority_weight × Tardiness_i) — applying higher weights to High-priority jobs

I would compute these across the full 12-month dataset, then segment by:
- **Job priority class:** Are High-priority jobs proportionally less late? If not, the priority mechanism is ineffective.
- **Routing length:** Do longer routings (more operations) accumulate disproportionately more delay?
- **Release-to-due-date window:** Jobs with tight windows (e.g., <5 days) vs. loose windows — is tightness alone predictive of lateness?
- **Temporal clustering:** Are late jobs clustered around specific periods (peak demand, breakdown events)?

**Conformance checking** against planned schedules (if available from ERP) using **token replay** or **alignments** in ProM quantifies deviation from planned sequence and timing at each step, decomposing total tardiness into contributions from individual operations.

#### **Impact of Disruptions**

I create a **disruption event overlay** on the performance timeline:

1. **Breakdown impact analysis:** For each `Breakdown Start/End` event on machine M, identify all jobs that were in M's queue or whose scheduled task on M was disrupted. Measure the ripple effect: How many downstream tasks were delayed? What was the incremental tardiness contribution? I use **difference-in-differences analysis** comparing queue lengths and lead times in the 4-hour windows before vs. after breakdowns.

2. **Hot job impact analysis:** For each `Priority Change` event (e.g., JOB-7005 becoming urgent), trace its effect on queue re-ordering. Measure: (a) how much time JOB-7005 gained by priority promotion, and (b) how much time other jobs in the same queues lost — the **opportunity cost of hot job insertion**.

3. **MTBF and MTTR estimation:** From the Resource event log, compute Mean Time Between Failures and Mean Time To Repair per machine. These feed directly into the simulation model in Section 5.

---

## Section 2: Diagnosing Scheduling Pathologies

### 2.1 Bottleneck Identification and Quantification

**Approach:** I apply the **Active Period Method** and **Throughput Analysis** from process mining. A bottleneck resource is characterized by:
- Consistently high effective utilization (>85-90%)
- Long and growing queues (high queue time with increasing trend)
- Downstream starvation patterns correlated with its processing completions

Using PM4Py's **resource performance analysis** module, I compute the **Waiting Time Contribution Index** for each resource:

```
WTCI(R) = (Queue_times_at_R across all jobs) / (Total_flow_times across all jobs)
```

A resource with WTCI > 0.20 (contributing more than 20% of total flow time through waiting) is a primary bottleneck candidate. In high-mix job shops, I would expect CNC Milling or Heat Treatment to emerge as critical bottlenecks based on typical patterns.

**Bottleneck migration analysis:** Using a **sliding window** (e.g., weekly), I track whether the bottleneck resource changes over time (e.g., CUT-01 is bottleneck in high-volume periods, MILL-03 in complex-job periods). This reveals that the current static dispatching rules are doubly inappropriate: not only are they simplistic, but they're applied identically regardless of which resource is currently constraining throughput.

**Quantified impact:** If the primary bottleneck (say MILL-03) has 23 minutes of average queue time per job and processes 180 jobs/month, the aggregate queue time is 69 hours/month. If 40% of this is attributable to suboptimal job sequencing (evidence from setup time analysis), then 27.6 hours of productive capacity is being consumed by avoidable waiting.

### 2.2 Evidence of Poor Task Prioritization

**Variant analysis** comparing **on-time job traces** versus **late job traces** is the primary diagnostic tool here. I filter the event log into two populations (late: tardiness > 0, on-time: tardiness  0) and compare:

- **Queue time distributions at each resource:** If late jobs spent disproportionately long time waiting at a particular resource despite having high priority, this directly evidences the prioritization rule's failure.
- **Priority inversions:** Using **conformance checking**, I detect cases where a Low-priority job was processed before a High-priority job at the same resource despite the latter arriving first (i.e., FCFS failing to respect priority) or where EDD was misapplied because the local resource had no visibility into downstream queue states.

A specific diagnostic: Extract all cases where a High-priority job (like JOB-7005 post-upgrade) was behind a Medium-priority job in a queue. Count these **priority inversions** per resource. If MILL-03 shows 47 priority inversions in a month, this is concrete evidence of dispatching rule failure.

### 2.3 Suboptimal Sequencing and Excessive Setup Times

Using the sequence-dependent setup time model constructed in Section 1, I compute the **counterfactual setup time**: what would total setup time at each bottleneck machine have been under an optimized sequence (e.g., solving a Traveling Salesman Problem on the setup time matrix for actual jobs processed each day)?

```
Setup_inefficiency_ratio = (Actual_total_setup_time  Optimal_total_setup_time) / Optimal_total_setup_time
```

If this ratio is 35-50% at the primary bottleneck machine, it means that nearly half of all setup time at the constraint is avoidable through better sequencing — a massive finding that directly translates to increased throughput capacity without any capital investment.

**Process mining evidence:** Dotted charts showing the actual job processing sequence on MILL-03 with setup durations color-coded reveal visual patterns of "setup-heavy" days versus "setup-light" days, correlating with whether similar jobs were processed consecutively.

### 2.4 Downstream Starvation

I measure **starvation events** at each resource: periods where the machine is available and no jobs are in its queue. Cross-correlating starvation at resource R(downstream) with processing completions at resource R(upstream) using **cross-correlation analysis** reveals causal links. For example:

- GRIND-01 starves for 45 minutes every time MILL-03 completes a large batch and the next batch hasn't been released yet  evidence of upstream batch-release timing misalignment.
- The **bullwhip effect** in WIP is measured by computing WIP level time series at each workstation (WIP = jobs queued + in-process) and calculating the **amplification ratio**: if demand (job releases) has CV of 0.2 but WIP at MILL-03 has CV of 0.55, the scheduling policy is amplifying variability by a factor of 2.75x.

### 2.5 Root Evidence via Resource Contention Analysis

Using **resource contention graphs** from process mining — showing which jobs compete for the same resource at overlapping times — I identify patterns of systematic over-commitment. The current ERP/MES release policy (releasing jobs as soon as they're ready) creates **release storms** that flood the shop floor simultaneously, creating artificial contention. Evidence: correlation between job release frequency and WIP spike events.

---

## Section 3: Root Cause Analysis of Scheduling Ineffectiveness

### 3.1 Fundamental Limitations of Static Local Dispatching Rules

The current mix of FCFS and EDD applied locally at each work center suffers from several structural deficiencies that process mining can explicitly demonstrate:

**FCFS failure mode:** FCFS ignores due dates entirely, creating situations where a job due tomorrow waits behind a job due next week simply because it arrived 20 minutes later. The process mining evidence: computing the correlation between queue arrival order rank and due date urgency at each resource. Near-zero or negative correlation (urgent jobs arriving early but still waiting behind non-urgent ones) exposes the FCFS failure.

**EDD failure mode (locally applied):** EDD considers only the due date of the current task but ignores: remaining processing time (a job with due date in 5 days but 8 days of remaining work should be treated differently from one with 5 days due date and 1 day remaining work), downstream bottleneck status (processing a job quickly only to have it starve at MILL-03 for 4 hours), and setup time costs (switching to an urgent job when a similar job is next in queue wastes bottleneck capacity).

**Process mining differentiation test:** I run a **what-if replay** on historical data: replay the event log applying a theoretically optimal priority rule (e.g., Shortest Remaining Processing Time with due date urgency weighting) and measure what tardiness would have been. The gap between this theoretical performance and actual performance quantifies the **scheduling logic deficit** — the improvement achievable through better rules alone, without any capacity changes.

### 3.2 Lack of Real-Time Visibility

The current state: each work center sees only its own queue. No mechanism exists for a machine operator to know that making a setup-efficient sequencing choice at CUT-01 will starve MILL-03 in 3 hours.

**Process mining evidence for visibility gaps:** Using **token flow replay**, I identify cases where a locally "optimal" decision (e.g., processing the job with shortest setup time at Machine A) resulted in a globally suboptimal outcome (cascading delays downstream). These represent **hidden coordination failures** invisible to operators but traceable in the event log.

### 3.3 Inaccurate Duration Estimation

Comparing `Planned Task Duration` to `Actual Task Duration` across all events reveals systematic bias:

- **Bias quantification:** Mean percentage error = mean((Actual  Planned) / Planned) across all task instances.
- **Variance underestimation:** If the coefficient of variation of actual durations significantly exceeds that of planned durations, the scheduling system is operating on an unrealistically optimistic variability model.
- **Operator and complexity effects:** Regression analysis may reveal that OP-105 consistently executes tasks 15% faster than planned while OP-108 consistently runs 12% slower — information currently not incorporated into scheduling.
- **Job complexity effects:** Jobs with unique material specifications may have 40% higher duration variance than standard jobs.

These patterns, extracted from the event log, directly explain why schedules become infeasible within hours of being published — they were built on fundamentally incorrect time estimates.

### 3.4 Differentiating Scheduling Logic vs. Capacity vs. Variability Issues

This is the most analytically sophisticated question. I use the following decomposition framework:

**Capacity-limited issues** are identified when:
- The bottleneck machine operates at >95% effective utilization even under optimal sequencing (no amount of scheduling improvement can resolve this — capacity addition is needed).
- Process mining test: Under the optimal simulated sequence (Section 5), does the bottleneck still show >90% utilization? If yes  capacity issue. If no  scheduling issue.

**Scheduling logic issues** are identified when:
- Bottleneck utilization drops significantly under simulated optimal policies.
- Priority inversions are frequent and their resolution improves tardiness substantially.
- Setup time inefficiency ratio (actual vs. optimal sequence setup time) is high.

**Inherent process variability issues** are identified when:
- Even under optimal scheduling and adequate capacity in simulation, tardiness remains non-zero due to the stochastic nature of task durations and breakdowns.
- Control charts on actual task durations show out-of-control points (assignable causes) versus common-cause variation.

This three-way decomposition prevents misdiagnosis: investing in scheduling optimization when the real problem is insufficient MILL-03 capacity would fail; investing in new machines when the real problem is poor sequencing would waste capital.

---

## Section 4: Developing Advanced Data-Driven Scheduling Strategies

### Strategy 1: Multi-Factor Dynamic Composite Dispatching (MFCD)

#### Core Logic

Replace the static FCFS/EDD mix with a **dynamically weighted composite priority index** computed in real-time for each job waiting at a work center. The priority index for job j at machine M at time t is:

```
Priority_Index(j, M, t) = w·CR(j,t) + w·SRPT(j,t) + w·(1/Setup_Cost(prev_job, j, M)) 
                          + w·Downstream_Load_Factor(j, M, t) + w·Priority_Class(j)
```

Where:
- **CR(j,t)** = Critical Ratio = (Remaining time to due date) / (Remaining processing time). CR < 1 indicates the job is already behind schedule; CR = 1 is exactly on track.
- **SRPT(j,t)** = Shortest Remaining Processing Time factor = 1 / (sum of remaining planned/predicted task durations).
- **Setup_Cost(prev, j, M)** = Predicted setup time if job j follows the current job on machine M — derived from the regression model trained on historical setup events. A short predicted setup elevates the priority of job j.
- **Downstream_Load_Factor(j, M, t)** = A factor that penalizes rushing a job through M if the next machine in j's routing is already heavily loaded (queue length > threshold), preventing WIP buildup at downstream bottlenecks. Computed from real-time MES queue state.
- **Priority_Class(j)** = Numeric encoding of business priority (High=3, Medium=2, Low=1).

The weights **w through w** are not manually set but learned via **offline simulation-based optimization** (Section 5): run thousands of simulated weeks varying the weights and selecting the combination that minimizes weighted tardiness plus WIP cost. Process mining provides the calibration data for this optimization.

#### Dynamic Weight Adjustment

Critically, the weights are not static but adjusted based on shop floor state:
- **During high-load periods** (WIP > 150% of baseline): increase w (CR urgency) to prevent late jobs.
- **At bottleneck machines**: increase w (setup minimization) because setup time directly consumes constrained capacity.
- **When downstream starvation is detected**: reduce w (downstream load penalty) and push jobs through to feed starved resources.

#### Process Mining Informing the Strategy

- Historical queue time analysis identifies the work centers where the current rule performs worst — these are priority targets for MFCD deployment.
- The setup time regression model provides the Setup_Cost term in real-time.
- Variant analysis of on-time vs. late jobs reveals which factors (CR vs. priority vs. setup) were most predictive of tardiness in historical data — informing initial weight selection.

#### Expected KPI Impact

| KPI | Current Estimate | MFCD Target |
|-----|-----------------|-------------|
| Weighted Tardiness | Baseline | 40 to 50% |
| Average WIP | Baseline | 20 to 30% |
| Setup time at bottleneck | Baseline | 15 to 25% |
| High-priority job tardiness rate | ~35% | <10% |

---

### Strategy 2: Predictive-Schedule-Driven Dispatching with Proactive Bottleneck Management

#### Core Logic

Rather than purely reactive dispatching, this strategy generates a **rolling predictive schedule** for the next 8-24 hours using a combination of:
1. **Stochastic task duration models** from process mining.
2. **Machine reliability models** (MTBF/MTTR distributions from breakdown event log).
3. **Real-time shop floor state** from MES.

The predictive schedule is not a rigid Gantt chart (which becomes invalid within hours) but a **probabilistic flow model** that estimates, for each job, the probability distribution of its completion time. This enables proactive intervention before problems materialize.

#### Predictive Duration Modeling

For each task type × machine combination, I fit a parametric distribution to historical actual durations. Lognormal distributions typically fit well for manufacturing tasks (right-skewed due to occasional complications):

```
Actual_Duration ~ LogNormal((job_complexity, operator, machine_age), )
```

Using the mined data, I identify that:
- Operator identity explains ~12% of duration variance  build operator-specific duration adjustment factors.
- Machine age/cycles since last maintenance explain ~8% of variance  incorporate maintenance state.
- Job complexity features (unique material, tight tolerances) explain ~20% of variance  build complexity-based duration scaling.

The result is a **task duration prediction engine** that provides P50, P80, and P90 completion time estimates for each task, updated dynamically as tasks complete and actual durations become known.

#### Predictive Bottleneck Detection

At any point in time, the system simulates the next N hours of shop floor operation (Monte Carlo with 200+ replications) using the current queue state, predicted task durations, and machine availability distributions. The output is:
- **Predicted queue length distribution** at each resource over time.
- **Early warning alerts** when MILL-03 queue is predicted to exceed threshold (e.g., >4 jobs) in 3 hours  trigger preemptive re-routing or expediting.
- **Completion probability estimates** for each job: P(JOB-7001 completes before due date) = 73%. This can be shared with customers for realistic lead time promises.

#### Predictive Maintenance Integration

From the resource event log, I compute MTBF distributions per machine. Using **survival analysis** (Weibull or Cox proportional hazard models), I estimate P(Machine M fails in next 4 hours | hours since last maintenance = X, recent performance anomalies = Y). When breakdown probability exceeds a threshold (e.g., 30%), the scheduler preemptively:
- Avoids assigning the last job of a tight-deadline sequence to that machine.
- Creates buffer capacity by slightly advancing other jobs.
- Alerts maintenance to perform opportunistic inspection during planned idle periods.

#### Process Mining Informing the Strategy

- Historical breakdown event analysis provides MTBF/MTTR estimates and variance.
- The correlation between task duration variance and specific job features (mined from the log) enables intelligent uncertainty modeling rather than fixed safety factors.
- Analysis of which jobs benefited most from early warning interventions in historical data (jobs where a 2-hour earlier start would have prevented tardiness) calibrates intervention thresholds.

#### Expected KPI Impact

| KPI | Expected Improvement |
|-----|---------------------|
| On-time delivery rate | +25 to +35 percentage points |
| Schedule predictability (forecast accuracy at D-2) | Improve from ~40% to ~75% |
| Bottleneck machine productive utilization | +8 to +12% (through reduced idle time from predictive feeding) |
| Customer lead time promise accuracy | Improve from wide uncertainty to ±15% error |

---

### Strategy 3: Intelligent Batch Sequencing for Setup Time Minimization (IBSS)

#### Core Logic

This strategy is specifically designed to attack the sequence-dependent setup time problem at bottleneck machines. It operates on a **planning horizon** (e.g., jobs expected to arrive at MILL-03 in the next 4-6 hours) and determines the optimal processing sequence to minimize total setup time while satisfying due date constraints.

#### Job Family Clustering

Using the mined setup time data, I apply **agglomerative hierarchical clustering** on job attribute vectors (material type, part family, tooling group, dimensional class) to create **setup families** — groups of jobs that can be processed with minimal transitions between them. The cluster linkage criterion is the average pairwise setup time from the historical setup time model.

For example, the analysis might reveal 6 distinct setup families at MILL-03:
- **Family A:** Aluminum prismatic parts, Group-2 tooling
- **Family B:** Steel rotational parts, Group-5 tooling  
- **Family C:** Titanium aerospace components, Group-8 tooling
- etc.

The setup time **within a family** averages 8 minutes; **between families** averages 31 minutes — a 4x differential that makes family-aware sequencing highly valuable.

#### Constrained Sequence Optimization at Bottleneck

Given the set of jobs arriving at the bottleneck machine within the planning horizon, I formulate a **constrained sequencing problem**:

**Objective:** Minimize total setup time =  Setup(job_i  job_j) for consecutive pairs in sequence.

**Constraints:**
- Jobs with CR < 1.1 (nearly late) must be processed within a priority window.
- The sequence must not cause any job in the current queue to exceed its due date (feasibility constraint, checked via the predictive schedule model from Strategy 2).
- Hot jobs (urgent priority) are inserted immediately before the next same-family job to minimize setup cost while respecting urgency.

For small planning horizons (10-15 jobs), this can be solved near-optimally using **Lin-Kernighan heuristics** (TSP-based) with due date feasibility checks. For larger horizons, I use **beam search** or **simulated annealing** guided by the setup time model.

#### Family Batching with Controlled WIP

A naive implementation of family batching would hold low-priority jobs waiting for more same-family jobs to accumulate — creating large WIP bubbles and unpredictable lead times. The controlled version uses:

- **Maximum wait time constraint:** A job can be held in a virtual pre-queue for at most `W_max` minutes waiting for family consolidation. `W_max` is set based on the job's slack time (due date  predicted remaining processing time).
- **Batch size limit:** No family batch exceeds `N_max` jobs to prevent downstream starvation during long same-family runs.
- **Dynamic family boundaries:** When shop load is very high (WIP > threshold), family boundaries are loosened (clustering threshold increased) to process jobs faster even if setup times are slightly higher.

#### Process Mining Informing the Strategy

- **Setup time model** (built in Section 1.2) provides the pairwise setup time predictions essential for sequence optimization.
- **Historical setup time distributions per family transition** (mined from the log) validate the clustering and provide variance estimates for simulation testing.
- **Analysis of actual vs. theoretically optimal sequences** in historical data quantifies the achievable improvement and validates the job family clustering.
- **WIP pattern analysis** identifies the correct `W_max` parameter: it should be below the threshold where WIP accumulation begins to cascade into downstream starvation.

#### Integration with Hot Jobs

When JOB-7005 is suddenly elevated to urgent status, the IBSS system:
1. Checks whether JOB-7005 belongs to the same family as the currently processing job.
2. If yes: insert JOB-7005 immediately after current job (minimal setup cost).
3. If no: evaluate whether the setup cost of immediate insertion is justified by the urgency, or whether a brief delay (1-2 slots) for a family-compatible job can be processed first without violating JOB-7005's due date.
4. Recomputes the sequence for remaining jobs in the planning horizon.

#### Expected KPI Impact

| KPI | Expected Improvement |
|-----|---------------------|
| Total setup time at bottleneck | 30 to 40% |
| Bottleneck throughput (additional jobs/shift) | +15 to +20% |
| Average WIP at bottleneck queue | 25% |
| Lead time for jobs processed at bottleneck | 10 to 18% |

---

### Integration of the Three Strategies

In practice, these three strategies operate as complementary layers:

- **IBSS (Strategy 3)** operates at bottleneck machines, sequencing jobs within planning horizons to minimize setup time subject to urgency constraints.
- **MFCD (Strategy 1)** operates at all non-bottleneck machines, providing real-time dispatching that considers the holistic shop state.
- **Predictive Scheduling (Strategy 2)** provides the real-time intelligence layer that both MFCD and IBSS consume: current queue states, predicted durations, machine availability forecasts, and bottleneck migration detection.

The three strategies together constitute an **Intelligent Shop Floor Orchestration System (ISOS)** that replaces the current blind local dispatching with globally coordinated, data-driven decision-making.

---

## Section 5: Simulation, Evaluation, and Continuous Improvement

### 5.1 Discrete-Event Simulation Framework

#### Simulation Model Construction

I would build a **discrete-event simulation (DES) model** of the entire Precision Parts Inc. shop floor using tools such as **AnyLogic**, **Simio**, **Arena**, or an open-source alternative like **SimPy** (Python-based, enabling tight integration with the process mining pipeline).

**Model parametrization from process mining:**

| Model Component | Data Source | Parametrization Method |
|----------------|-------------|----------------------|
| Job arrival process | Event log job release timestamps | Fit inter-arrival time distribution (likely non-stationary Poisson with hourly/weekly seasonality) |
| Routing probabilities | Trace variant analysis | Routing probability matrix per job type/family |
| Task duration distributions | Historical actual vs. planned durations | Lognormal fit per task×machine×complexity tier |
| Setup time model | Pairwise setup duration analysis | Regression model (job attribute pairs  setup time distribution) |
| Machine breakdown model | Resource event MTBF/MTTR data | Weibull MTBF distribution, Lognormal MTTR distribution per machine |
| Hot job arrival rate | Priority change event frequency | Poisson process rate per week |
| Operator performance factors | Operator×task duration analysis | Multiplicative adjustment factors per operator |

**Baseline validation:** Before testing new strategies, the simulation must reproduce historical KPIs within acceptable tolerance (e.g., ±10% on mean flow time, ±15% on tardiness rate). I use the first 10 months of the event log for parametrization and validate against the final 2 months of data.

#### Experimental Design for Strategy Evaluation

I would design a **full factorial experiment** with the following dimensions:

**Scheduling strategies tested (treatments):**
- **Baseline:** Current FCFS/EDD mix
- **MFCD only**
- **MFCD + Predictive Scheduling**
- **MFCD + IBSS**
- **Full ISOS (all three strategies combined)**

**Scenario conditions (factors):**
1. **Load level:** Normal (80% capacity), High (95% capacity), Overloaded (110% capacity)
2. **Disruption frequency:** Low (1 breakdown/week), Medium (3/week), High (7/week)
3. **Hot job injection rate:** Low (1/week), High (5/week)
4. **Mix complexity:** Standard mix (mined historical proportions), High-variety mix (more unique routings)

Each treatment×scenario combination is replicated 30 times (using different random seeds) to achieve statistical significance. This yields 5 × 12 × 30 = 1,800 simulation runs.

**Primary KPIs measured in simulation:**
- Mean and P90 weighted tardiness
- Mean WIP (jobs in system)
- Machine utilization rates (productive, setup, idle, breakdown)
- Lead time variability (CV of job flow times)
- Throughput rate (jobs completed per week)

**Statistical analysis:** ANOVA to test main effects and interactions, followed by Tukey HSD post-hoc tests to compare specific strategy pairs. The goal is to identify not just which strategy is best on average, but which strategy is most robust across diverse conditions.

#### Specific Scenarios of Interest

**Scenario 1: "Perfect Storm" — High Load + Multiple Breakdowns + Hot Jobs**
This stress-tests the resilience of each strategy. MFCD and Predictive Scheduling should demonstrate superior performance because they can dynamically re-prioritize; purely setup-optimized strategies may be brittle if bottleneck breakdowns destroy planned batches.

**Scenario 2: "Gradual Overload" — Capacity Ramp-Up**
Starting at 75% load and gradually increasing to 110%, this identifies the load threshold at which each strategy degrades. The aim is to find strategies that exhibit **graceful degradation** rather than sudden collapse.

**Scenario 3: "Setup Storm" — High Variety Mix**
A mix dominated by dissimilar jobs maximally challenges IBSS. This tests whether family-based batching remains beneficial when family clusters are small and sparse.

**Scenario 4: "Predictive vs. Reactive" — Controlled Breakdown Scenario**
MTBF distributions are held constant but maintenance predictability is varied. This isolates the value of the predictive maintenance integration in Strategy 2.

### 5.2 Continuous Monitoring and Adaptive Improvement Framework

#### Real-Time Process Mining Dashboard

Post-deployment, I establish a **continuous process mining pipeline** with automated daily/weekly analysis:

**Data flow:**
```
MES Event Log (live)  Event Log Extraction  Automated Process Mining  
KPI Dashboard  Drift Detection  Alert/Adaptation Engine
```

#### KPI Tracking and Drift Detection

Using **Statistical Process Control (SPC)** applied to process mining outputs:
- Individual and Moving Range (I-MR) charts on weekly tardiness rate, WIP levels, and setup ratios.
- **CUSUM charts** for detecting sustained shifts in key metrics (more sensitive than Shewhart charts for detecting small, persistent changes).
- **EWMA charts** on bottleneck machine utilization to detect gradual drift.

When a KPI control chart signals (exceeds control limits or shows trending patterns), an automated **root cause attribution module** is triggered:
- Diagnose whether the shift correlates with a change in job mix (process mining detects new routing variants appearing).
- Diagnose whether a new bottleneck has emerged (WTCI analysis on recent events).
- Diagnose whether a specific machine's duration distribution has shifted (suggesting tool wear or need for calibration).

#### Adaptive Scheduling Logic

The scheduling system incorporates **adaptive learning loops** at three timescales:

**Operational level (every shift):**
- MFCD weights are re-calibrated based on previous shift's performance.
- Setup time model receives new observations and updates regression coefficients via online learning (warm-start refit).
- Job family clustering is checked for new job types that don't fit existing families.

**Tactical level (weekly):**
- Full process mining analysis run on previous week's event log.
- IBSS batch parameters (W_max, N_max) adjusted based on observed WIP accumulation patterns.
- Predictive model MTBF/MTTR estimates updated with new breakdown data.
- Simulation model re-parametrized with updated distributions and re-run if significant parameter drift is detected.

**Strategic level (monthly/quarterly):**
- Full variant analysis comparing recent routing patterns to historical baseline  detect product mix shifts requiring fundamental strategy reassessment.
- Capacity analysis: If bottleneck utilization under optimal scheduling exceeds 90%, recommend capacity addition.
- Strategy effectiveness review: Are all three strategies still delivering expected improvements, or has one become obsolete due to changed conditions?

#### Feedback Loop Architecture

```

           CONTINUOUS IMPROVEMENT LOOP        
                                             
  Live MES Events  Process Mining Engine   
                                           
  KPI Dashboard + SPC Monitoring            
                                           
  Drift/Anomaly Detection                   
                                           
  Root Cause Attribution                    
                                           
  Parameter Recalibration + Simulation Test 
                                           
  Strategy Update  Scheduling Engine       
                                           
  [Feedback from shop floor outcomes]       

```

This architecture ensures the system **learns from its own operations**, continuously refining its models as the shop floor evolves — addressing the fundamental limitation of the current static-rule approach, which cannot adapt to changing conditions by design.

#### Human-in-the-Loop Elements

Despite the sophistication of the automated system, I would retain human oversight at key decision points:
- **Scheduler review of major sequence changes:** When IBSS proposes holding a job for more than 60 minutes awaiting family consolidation, a scheduler receives a notification with cost/benefit analysis and can override.
- **Anomaly investigation:** When drift detection triggers an alert, a human analyst reviews the process mining diagnostic before automated parameter changes are applied.
- **Strategic reviews:** Monthly KPI reviews with shop floor supervisors ensure that quantitative optimization is grounded in operational reality and operator experience.

---

## Summary: Integrated Framework Overview

The approach described above constitutes a complete transformation of Precision Parts Inc.'s scheduling capability, progressing through five integrated phases:

1. **Discover** (Section 1): Mine historical event logs to create accurate, data-grounded models of actual shop floor behavior — not how it was planned, but how it actually operates.

2. **Diagnose** (Section 2): Use process mining analytics to identify specific, quantified pathologies: bottlenecks consuming X hours of capacity, priority inversions occurring Y times per week, setup inefficiency of Z%.

3. **Understand** (Section 3): Distinguish scheduling logic failures from capacity constraints from process variability, preventing misallocation of improvement resources.

4. **Redesign** (Section 4): Deploy three complementary scheduling strategies — dynamic multi-factor dispatching, predictive schedule management, and setup-optimized batch sequencing — each directly addressing specific diagnosed pathologies and calibrated with process mining data.

5. **Sustain** (Section 5): Validate with simulation before deployment, then continuously monitor and adapt using ongoing process mining to ensure performance is maintained as conditions evolve.

The result: a manufacturing operation where scheduling decisions are made with complete visibility, historical intelligence, and predictive capability — fundamentally unlike the current myopic, static, locally-applied rules that have created the tardiness, WIP, and unpredictability crises Precision Parts Inc. currently faces.