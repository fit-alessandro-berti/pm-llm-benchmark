# Comprehensive Process Mining Analysis for Speedy Parcels: A Consultant's Report

---

## Executive Summary

Speedy Parcels faces a dual challenge common to last-mile logistics operators: declining delivery punctuality and escalating operational costs. With six months of rich, multi-source event data available, a systematic process mining approach can move the organization from reactive firefighting to proactive, evidence-based operational management. This report outlines a complete methodology—from data preparation through continuous monitoring—designed to transform raw event data into actionable competitive advantage.

---

## Section 1: Process Discovery and Conformance Checking

### 1.1 Data Preprocessing and Integration

#### Defining the Case Concept

The foundational decision in any process mining project is selecting the appropriate *case* definition, as it determines what story the data tells. For Speedy Parcels, I recommend a **dual-level case structure**:

- **Primary Case:** `Vehicle-Day` (e.g., `V12-20241205`) — captures the complete operational shift from depot departure to return, enabling analysis of daily route performance, overtime patterns, and vehicle utilization.
- **Secondary Case:** `Package-ID` — enables package-level journey tracking across potential multiple delivery attempts, re-routing, and eventual resolution (success or return to depot).

These two case levels can be linked via a foreign key relationship, enabling drill-down from the vehicle-day view into individual package trajectories when needed.

#### Event Log Schema Design

The unified event log must conform to the XES (eXtensible Event Stream) standard or equivalent tabular format with mandatory attributes:

| Field | Source | Description |
|-------|---------|-------------|
| `case_id` | Derived | Vehicle-Day or Package-ID |
| `timestamp` | All sources | UTC-normalized datetime |
| `activity` | Derived | Standardized activity label |
| `vehicle_id` | GPS/Scanner | Physical vehicle identifier |
| `driver_id` | Dispatch/Scanner | Driver identifier |
| `package_id` | Scanner | Package identifier (nullable) |
| `location_lat` | GPS | Latitude coordinate |
| `location_lon` | GPS | Longitude coordinate |
| `speed_kmh` | GPS | Speed in km/h |
| `event_source` | Metadata | GPS / Scanner / Dispatch / Maintenance |
| `planned_stop_seq` | Dispatch | Intended stop order |
| `planned_arrival` | Dispatch | Scheduled arrival time |
| `customer_time_window_start` | Dispatch | Customer-requested window open |
| `customer_time_window_end` | Dispatch | Customer-requested window close |

#### Activity Abstraction and Standardization

Raw GPS data produces thousands of micro-events (position pings every few seconds) that must be abstracted into meaningful process activities. I recommend a **hierarchical abstraction approach**:

**Level 1 — Raw Events (GPS pings, scanner scans)**
- Individual GPS coordinates at 30-second intervals
- Individual scanner events

**Level 2 — Abstracted Activities (for process mining)**

| Abstracted Activity | Derivation Logic |
|---------------------|-----------------|
| `Depart Depot` | First GPS movement event after Route Assigned, speed > 5 km/h |
| `Travel to Stop [N]` | GPS movement between departure of stop N-1 and arrival at stop N |
| `Arrive Customer [N]` | Scanner "Arrive Customer" event OR GPS stop within 50m of delivery address |
| `Delivery Attempted - Success` | Scanner "Delivery Success" event |
| `Delivery Attempted - Failed` | Scanner "Delivery Failed" event |
| `Depart Customer [N]` | Scanner "Depart Customer" OR GPS movement >50m from stop location |
| `Traffic Delay` | GPS speed < 10 km/h sustained for > 3 minutes during transit |
| `Unplanned Stop` | GPS idle > 5 minutes at non-customer, non-depot location without scanner event |
| `Fuel Stop` | GPS stop at known fuel station coordinates (cross-referenced with POI database) |
| `Lunch Break` | GPS idle 20-60 minutes within ±30 minutes of midday |
| `Vehicle Breakdown` | Maintenance log unscheduled event OR GPS idle > 30 min with "Engine Warning" note |
| `Return to Depot` | GPS arrival at depot coordinates |
| `End Shift` | Driver event "End Shift" |

#### Integration Challenges and Mitigation Strategies

**Challenge 1: Timestamp Synchronization**
Different systems may use different clock sources, creating temporal misalignment. GPS trackers may drift, scanner clocks may lag, and dispatch timestamps are server-generated.

*Mitigation:* Establish GPS as the ground-truth timestamp source (most granular and continuous). Apply offset correction to scanner and dispatch timestamps using known reference events (e.g., "Depart Depot" should be consistent across GPS and dispatch). Flag records where source timestamps differ by more than 2 minutes for manual review.

**Challenge 2: Spatial-Temporal Matching**
GPS coordinates and scanner events need to be spatially correlated. A scanner "Arrive Customer" event should match a GPS stop near the delivery address, but GPS accuracy varies (±5-15m in urban environments with tall buildings causing signal multipath).

*Mitigation:* Use a geofencing approach with a configurable radius (50m default, expandable to 100m in dense urban cores). Implement a temporal window matching algorithm that looks for GPS stops within ±3 minutes of scanner events.

**Challenge 3: Event Ordering Ambiguity**
When two events from different sources share the same minute (or near-identical timestamps), their ordering matters for process reconstruction.

*Mitigation:* Apply a priority ordering rule: Scanner > GPS > Dispatch for same-minute events, reflecting the higher operational specificity of scanner events. Document this rule explicitly in the process mining methodology.

**Challenge 4: Missing and Incomplete Data**
GPS trackers lose signal in tunnels, underground parking, or areas of poor network coverage. Scanners may not be used correctly by all drivers (some may batch-scan at intervals rather than in real-time).

*Mitigation:* Implement a data quality assessment step that calculates completeness scores per vehicle, driver, and day. Flag cases with more than 15% missing GPS coverage or scanner events not corroborated by GPS movement. Exclude severely incomplete cases from certain analyses, but retain them with appropriate caveats for overall statistics.

**Challenge 5: High Frequency GPS Data Volume**
Six months of GPS data at 30-second intervals for, say, 20 vehicles generates approximately 20 × 2 × 60 × 30 × 180  130 million individual GPS records. This volume requires efficient data engineering infrastructure.

*Mitigation:* Pre-process GPS data using a change-point detection algorithm (e.g., stop detection based on speed threshold and spatial clustering) before loading into the process mining tool. Retain full GPS traces for geospatial visualization but use abstracted activities for process mining algorithms.

**Challenge 6: Data Silo Integration**
Maintenance logs may exist in a separate enterprise system with different vehicle ID formats, requiring entity resolution.

*Mitigation:* Create a master data mapping table that links vehicle IDs across systems (e.g., GPS tracker ID "TRK-V12"  Dispatch "V12"  Maintenance "VEHICLE-012"). This mapping must be manually verified with the operations team before analysis begins.

---

### 1.2 Process Discovery

#### Algorithm Selection

For Speedy Parcels' event log, I recommend using the **Inductive Miner (IM) with infrequent behavior filtering** as the primary discovery algorithm. This choice is justified by several characteristics of the delivery domain:

- **Fitness guarantee:** The Inductive Miner produces sound, complete process models (Petri nets or BPMN), ensuring all observed traces can be replayed.
- **Noise handling:** The `IMf` variant filters infrequent behavior below a configurable threshold, preventing rare deviations (single vehicle breakdowns, one-off weather events) from cluttering the main process model.
- **Scalability:** Handles large event logs efficiently compared to -algorithm variants.

**Supplementary algorithms:**
- **Fuzzy Miner:** For initial exploration, especially useful for identifying the most frequent activities and their relationships in a high-variability environment like delivery routing.
- **Directly-Follows Graph (DFG):** For quick visualization of activity transitions and their frequencies/durations, useful for stakeholder communication.

#### Discovered Process Model Structure

The expected discovered process model for a typical Speedy Parcels vehicle-day would reveal the following structure:

```
[Start Shift]  [Route Assigned]  [Load Vehicle at Depot]  [Depart Depot]
    
[Travel to Stop N]  {Traffic Delay?}  [Arrive Customer N]
    
[Delivery Attempt]  {Success  [Depart Customer N]} 
                   {Failed  [Leave Failure Notice]  [Depart Customer N]}
    
[Return to Stop N (Re-delivery)?]       (loop back)
    
[All Stops Completed / Time Constraint Reached]
    
[Return to Depot]  [Unload Returns/Failed Packages]  [End Shift]
```

With **concurrent paths** possible for:
- Unplanned maintenance stops (interjected at any point)
- Fuel stops
- Driver breaks (legally mandated in most jurisdictions)
- Route re-sequencing (GPS shows different order than planned)

#### Multi-Perspective Process Models

Beyond the control-flow model, I would enrich the discovered model with:

**Time Perspective:** Annotate each activity and arc with:
- Average, median, and 90th percentile duration
- Coefficient of variation (CV) to identify activities with high variability (CV > 0.5 flagged as unstable)

**Resource Perspective:** Overlay models with driver-specific behavior patterns, identifying which drivers follow which process variants most frequently.

**Geographic Perspective:** Plot process activities on a map using the GPS coordinates, creating a spatial process model that shows the geographic distribution of traffic delays, failed deliveries, and unplanned stops. Tools like Disco, Celonis, or custom Python implementations using Folium can create these interactive visualizations.

**Frequency-Filtered Variants:** Discover the top 10-20 process variants (distinct activity sequences) and their frequencies. In delivery operations, 80% of cases typically follow 3-5 main variants; the long tail of exotic variants often reveals the most operationally interesting problems.

---

### 1.3 Conformance Checking

#### Constructing the Reference Model

The planned routes from the dispatch system provide a natural normative reference model. For each vehicle-day, the dispatch system contains:
- Ordered stop sequence (stops 1 through N)
- Planned arrival times per stop
- Package assignments
- Customer time windows

This information is converted into a **normative Petri net** or **DECLARE constraint model** representing what *should* happen:

```
Planned Model: Depart Depot  Stop 1  Stop 2  ...  Stop N  Return Depot
               (with time constraints at each stop based on planned schedule)
```

#### Conformance Checking Techniques

**Token-Based Replay (TBR):**
Replay each actual vehicle-day trace on the planned process model. The algorithm tracks:
- *Missing tokens* (planned activities that didn't happen — e.g., skipped stops)
- *Remaining tokens* (activities left unfinished at trace end)
- *Consumed tokens* (activities that matched the plan)

This produces a **fitness score** (0-1) for each vehicle-day, where 1.0 = perfect conformance and 0.0 = complete divergence from plan.

**Alignment-Based Conformance:**
More precise than token replay, alignments find the optimal mapping between planned and actual traces, minimizing the edit distance (number of moves on the log and model). Each alignment produces a cost, where:
- *Log move* = activity occurred in reality but not planned (e.g., an unplanned fuel stop)
- *Model move* = planned activity did not occur (e.g., a stop was skipped)
- *Synchronous move* = planned activity occurred as expected

Alignment costs are aggregated to produce case-level and fleet-level conformance metrics.

#### Types of Deviations to Investigate

**Category 1: Sequence Deviations**
- Stops delivered in a different order than planned (e.g., Stop 7 before Stop 4)
- Root cause hypothesis: Driver judgment to avoid backtracking, real-time re-routing, or customer request
- Detection: Sequence comparison between planned stop order and actual GPS-corroborated stop sequence
- Impact measurement: Additional kilometers traveled, time added

**Category 2: Unplanned Stops**
- Stops at locations not in the planned route (fuel stations, repair shops, fast food outlets)
- Detection: GPS stops > 5 minutes at non-customer, non-depot coordinates not matching any planned stop
- Sub-categories to distinguish: Maintenance stops (critical), personal stops (behavioral), traffic-induced stops (environmental)

**Category 3: Timing Deviations**
- Arriving significantly earlier or later than planned at each stop
- Detection: Compare planned arrival time (from dispatch) vs. actual arrival time (from scanner/GPS)
- Threshold: Flag deviations > 15 minutes as significant; > 30 minutes as severe
- Pattern analysis: Are timing deviations random, or do they compound throughout the day (early stops on-time, later stops increasingly late)?

**Category 4: Skipped Stops**
- Planned stops that were never visited (package not attempted)
- Detection: Planned stop in dispatch system but no corresponding GPS stop or scanner event at that location
- Critical distinction: Was package delivered through alternative means (neighbor delivery, parcel locker) or simply skipped?

**Category 5: Additional Stops**
- Visits to customer addresses not in the original plan (e.g., re-delivery of previously failed packages added during the day, or dynamic dispatching)
- Detection: Scanner "Arrive Customer" events at addresses not in the planned route

**Category 6: Capacity and Load Violations**
- Vehicle departing depot with more packages than planned (overloading) or fewer (under-utilization)
- Detection: Cross-reference scanner "Scan at Depot" counts vs. dispatch planned package count

**Conformance Dashboard Output:**
Produce a conformance summary matrix showing, for each vehicle-day:

| Metric | Target | Alert Threshold |
|--------|--------|-----------------|
| Fitness Score | > 0.85 | < 0.70 |
| On-Time Stop Rate | > 90% | < 75% |
| Sequence Deviation Rate | < 10% | > 25% |
| Unplanned Stop Rate | < 5% | > 15% |
| Skipped Stop Rate | < 1% | > 5% |

---

## Section 2: Performance Analysis and Bottleneck Identification

### 2.1 Key Performance Indicators

#### KPI 1: On-Time Delivery Rate (OTDR)
**Definition:** Percentage of delivery attempts occurring within the customer's requested time window.
**Calculation:** 
```
OTDR = (Count of deliveries where Scanner "Arrive Customer" timestamp 
        falls within [customer_time_window_start, customer_time_window_end]) 
       / (Total planned deliveries with time windows) × 100
```
**Target:**  95% for premium customers;  88% overall
**Granularity:** Calculate at vehicle, driver, route, day-of-week, and area levels

#### KPI 2: Average Time per Delivery Stop (Service Time)
**Definition:** Time between "Arrive Customer" and "Depart Customer" events.
**Calculation:**
```
Service Time [stop N] = Timestamp("Depart Customer", stop N) 
                       - Timestamp("Arrive Customer", stop N)
```
**Decomposition:** Further decompose into:
- *Parking acquisition time:* GPS idle near stop before scanner "Arrive Customer"
- *Customer interaction time:* Time between "Arrive Customer" and first delivery event
- *Admin time:* Time between delivery scan and "Depart Customer"

**Target:** Benchmark against industry standards; flag stops > 2× median service time

#### KPI 3: Travel Time Ratio (TTR)
**Definition:** Proportion of shift time spent traveling vs. performing deliveries.
**Calculation:**
```
TTR = Total Travel Time / (Total Travel Time + Total Service Time)
Productive Time Ratio = 1 - TTR
```
**Target:** TTR < 0.55 (more than 45% of time in productive delivery activity)

#### KPI 4: Failed Delivery Rate (FDR)
**Definition:** Percentage of delivery attempts resulting in failure (customer not home, access issues, etc.).
**Calculation:**
```
FDR = Count("Delivery Failed" events) / Count(All delivery attempt events) × 100
```
**Extended analysis:** Track *re-delivery rate* (what percentage of failed deliveries require a second attempt) and *abandonment rate* (packages returned to depot after multiple failures).

#### KPI 5: Fuel Consumption per Package Delivered
**Definition:** Operational efficiency metric linking fuel costs to productive output.
**Calculation:**
```
Fuel per Package = Total fuel consumed (liters, from maintenance/fuel logs) 
                  / Total packages successfully delivered
```
*Note: If direct fuel sensor data is unavailable, estimate using GPS speed profiles and vehicle-specific fuel consumption models (fuel consumption curves are well-established for van types).*

#### KPI 6: Vehicle Utilization Rate
**Definition:** Percentage of available shift time the vehicle is actively used for delivery operations (excluding breakdowns and excessive idle time).
**Calculation:**
```
Utilization Rate = (Shift Duration - Breakdown Time - Excessive Idle Time) 
                  / Shift Duration × 100
```

#### KPI 7: Overtime Rate
**Definition:** Percentage of shifts extending beyond the planned shift end time.
**Calculation:**
```
Overtime Rate = Count(End Shift timestamp > Planned Shift End) 
               / Total Shifts × 100
Avg Overtime Duration = Mean(Max(0, End Shift - Planned Shift End))
```

#### KPI 8: Stop Density Efficiency
**Definition:** Number of successful deliveries per kilometer traveled.
**Calculation:**
```
Stop Density = Successful Deliveries / Total km traveled in shift
```
Compare this across routes and territories to identify geographic inefficiencies.

#### KPI 9: Unplanned Stop Rate and Duration
**Definition:** Frequency and duration of stops not in the planned route.
**Calculation:** Derived directly from conformance checking unplanned stop events.

#### KPI 10: Maintenance-Induced Downtime Rate
**Definition:** Percentage of operational vehicle-days experiencing breakdown-related interruptions.
**Calculation:**
```
Downtime Rate = Count(Shifts with unscheduled maintenance events) 
               / Total Shifts × 100
```

---

### 2.2 Bottleneck Identification Techniques

#### Technique 1: Performance-Annotated Process Maps

Apply performance information directly to the discovered process model. In tools like Celonis, ProM, or Disco, each activity node is colored by average duration (green = fast, red = slow) and each arc is labeled with the average transition time between activities.

This immediately reveals structural bottlenecks — activities where execution time is systematically higher than expected. For Speedy Parcels, we would expect to see:
- Dark red coloring on transit arcs during morning and evening rush hours
- Extended dwell times at commercial stops vs. residential stops
- Longer times at stops with previous failed delivery attempts (driver must attempt alternative delivery)

#### Technique 2: Time-Based Filtering and Animation

Filter the event log by time of day and animate the process map across a 24-hour cycle. This reveals:
- When bottlenecks appear and disappear (traffic-induced vs. structural)
- Whether morning or afternoon routes experience more delays
- Shift-level patterns (do delays compound or recover?)

#### Technique 3: Dotted Chart Analysis

Plot all events for all vehicle-days on a dotted chart (x-axis = time of day, y-axis = cases sorted by start time, color = activity type). This visualization excels at revealing:
- Systematic delays starting at specific times of day across multiple vehicles (indicating traffic hotspots)
- Vehicles that start and end shifts at unusual times
- Patterns of failed deliveries (if failed deliveries cluster at specific times)
- Overtime patterns (cases extending beyond expected end time)

#### Technique 4: Variant Analysis for Performance Comparison

Cluster vehicle-day traces by their process variant and compare KPIs across variants:

```
Variant A (Planned sequence, no deviations): n=342, Avg OTDR=94%, Avg overtime=12 min
Variant B (1-2 sequence deviations): n=187, Avg OTDR=87%, Avg overtime=28 min  
Variant C (Unplanned maintenance stop): n=43, Avg OTDR=61%, Avg overtime=67 min
Variant D (Multiple failed deliveries): n=89, Avg OTDR=79%, Avg overtime=45 min
```

Variants with significantly worse KPIs immediately identify which deviation types cause the most harm.

#### Technique 5: Geospatial Hotspot Analysis

Overlay process performance data on a geographic map:
- Plot all "Traffic Delay" events (derived from low-speed GPS periods) to identify recurring congestion hotspots
- Plot all "Delivery Failed" events to identify geographic clusters of failed deliveries (apartment complexes without access, commercial zones closed at delivery times)
- Plot all "Unplanned Stops" to identify breakdown-prone routes or areas where drivers make personal stops
- Use kernel density estimation (KDE) to identify the highest-density problem areas

#### Technique 6: Throughput Time Decomposition

For each vehicle-day, decompose total shift duration into constituent time buckets:

```
Total Shift Time = Loading Time + Travel Time + Service Time 
                 + Traffic Delay Time + Break Time 
                 + Unplanned Stop Time + Return Travel Time
```

Plot this decomposition as stacked bar charts for all vehicles on all days. Identify which time bucket:
1. Has the highest absolute value (most time consumed)
2. Has the highest variability (most unpredictable)
3. Shows the strongest correlation with overtime

This decomposition immediately reveals whether Speedy Parcels' problem is primarily a *travel time* problem (routing/traffic), a *service time* problem (slow customer interactions), or a *reliability* problem (breakdowns, failures).

#### Technique 7: Queue Analysis at Depot

Analyze events during the depot loading phase:
- Compare "Route Assigned" timestamp vs. "Depart Depot" timestamp
- Identify vehicles waiting for routes to be finalized (dispatch bottleneck)
- Identify vehicles waiting for loading (warehouse bottleneck)
- Quantify how depot departure delays translate into downstream delivery delays

If vehicles are consistently departing 30-45 minutes after route assignment, this represents a fixable upstream bottleneck.

#### Quantifying Bottleneck Impact

For each identified bottleneck, calculate:
1. **Frequency:** How often does this bottleneck occur? (% of cases)
2. **Severity:** When it occurs, how much time does it add? (minutes/hours)
3. **Impact score:** Frequency × Severity = total bottleneck time across all cases
4. **KPI correlation:** Does the presence of this bottleneck correlate with OTDR failures or overtime?

Rank bottlenecks by impact score to prioritize intervention efforts.

---

## Section 3: Root Cause Analysis for Inefficiencies

### 3.1 Candidate Root Causes and Analytical Validation

#### Root Cause 1: Suboptimal Static Route Planning

**Hypothesis:** Routes are planned once at the start of the day without adaptation to real-time conditions, leading to inefficient sequences and poor traffic avoidance.

**Evidence from process mining:**
- High frequency of sequence deviations in conformance checking (drivers self-correcting the planned sequence based on ground experience)
- Consistent traffic delay events on specific road segments at predictable times (e.g., school zones at 08:30, highway merges at 17:00)
- Comparison of driver-deviated routes vs. planned routes showing shorter actual travel times despite sequence deviation (driver knowledge outperforming the plan)

**Validation analysis:**
- Extract all sequence deviations and calculate whether the deviated sequence resulted in shorter or longer travel time than the planned sequence would have (use GPS timestamps to measure actual travel times between stops in both orders)
- If driver deviations lead to shorter travel times 60%+ of the time, this confirms static planning inadequacy

#### Root Cause 2: Inaccurate Travel Time Estimations

**Hypothesis:** The dispatch system uses generic average speed assumptions that don't reflect actual urban traffic patterns, leading to systematically unrealistic planned schedules.

**Evidence from process mining:**
- Compare planned travel times (extracted from dispatch schedule: time between consecutive planned arrivals) vs. actual travel times (GPS-measured) for each inter-stop segment
- Calculate the *planning bias*: `Actual Travel Time / Planned Travel Time`
- If this ratio is systematically > 1.0 (e.g., average 1.35, meaning actual takes 35% longer than planned), the estimation model is flawed
- Check if this bias is consistent across all times of day or amplified during specific periods

**Validation analysis:**
- Segment the analysis by time of day: Does planning accuracy deteriorate specifically in the 07:30-09:30 and 16:00-19:00 windows? If so, the system isn't accounting for rush-hour congestion.
- Check if certain geographic zones show systematic underestimation (city center, near schools, near stadiums on event days)

#### Root Cause 3: High Variability in Customer Service Time

**Hypothesis:** Service time at customer locations varies dramatically based on factors the plan doesn't account for (building type, floor level, elderly customers needing assistance, commercial deliveries requiring signature from specific staff), causing unpredictable schedule disruption.

**Evidence from process mining:**
- Calculate service time statistics per stop: mean, standard deviation, and CV
- Cross-reference service times with stop attributes: residential vs. commercial, ground floor vs. upper floor, parcel size (from dispatch), customer type
- If CV > 0.5 for service times, there is high unexplained variability

**Validation analysis:**
- Build a multivariate regression model: `Service Time ~ Building Type + Package Size + Time of Day + Driver ID + Customer Repeat Status`
- Identify which factors explain variance in service time
- Quantify the unpredictability cost: `Service Time Variance × Stop Count = Schedule Disruption Risk`
- Use process mining's social network analysis to identify if specific drivers take significantly longer at identical stop types (behavioral factor)

#### Root Cause 4: Preventable Failed Deliveries

**Hypothesis:** A significant proportion of failed deliveries occur at predictable times/locations where customers are consistently unavailable, but the planning system doesn't route accordingly.

**Evidence from process mining:**
- Map all failed deliveries by address, time of day, and day of week
- Identify repeat failures: addresses where delivery has failed 2+ times in the dataset
- Cross-reference with customer time windows: Were failed deliveries attempted outside the customer's stated time window (dispatch system should have prevented this)?

**Validation analysis:**
- Calculate: What percentage of failed deliveries were attempted at locations where a previous failure had occurred? If > 30%, there is a systematic planning failure to learn from history.
- Calculate: What percentage of failed deliveries occurred outside the customer's stated time window? If significant, the routing algorithm is not respecting time window constraints effectively.
- Estimate the cost of re-delivery: `Failed Deliveries × Average Re-delivery Cost (fuel + driver time + vehicle wear)`

#### Root Cause 5: Vehicle Maintenance and Reliability Issues

**Hypothesis:** Certain vehicles are prone to breakdowns that are predictable based on usage patterns, but the maintenance schedule doesn't reflect this, leading to mid-shift failures.

**Evidence from process mining:**
- Cross-reference maintenance log unscheduled repair events with the vehicle's preceding usage history (consecutive days worked, km driven, idle time accumulated)
- Identify if breakdowns cluster around specific mileage thresholds or operational patterns
- Analyze the time between scheduled maintenance events and first unscheduled repair

**Validation analysis:**
- Survival analysis (Kaplan-Meier curves) on vehicle reliability: How long after a scheduled maintenance event does an unscheduled repair occur?
- Build a logistic regression model: `P(Breakdown Today) ~ Days Since Last Service + km Since Last Service + Consecutive Shifts Without Rest + Vehicle Age`
- If this model achieves predictive accuracy > 70%, a predictive maintenance schedule is feasible and the current schedule is clearly inadequate.

#### Root Cause 6: Driver Behavior and Skill Differences

**Hypothesis:** Significant performance differences between drivers exist that are not explained by route difficulty, vehicle quality, or traffic conditions — indicating behavioral or skill-based factors.

**Evidence from process mining:**
- Control for route difficulty (same route, same time period) and compare driver KPIs
- Use a **driver performance radar chart** covering: OTDR, avg service time, traffic delay time, failed delivery rate, unplanned stop rate, fuel efficiency proxy (km at high speed vs. steady speed)
- Identify drivers consistently in the top and bottom performance quartiles across multiple controlled comparisons

**Validation analysis:**
- **Variant analysis:** Extract the most common process variants for high-performing and low-performing drivers on similar routes. Do high performers have systematically different behavior patterns (e.g., they more frequently deviate from planned sequence but achieve better outcomes)?
- **Dwell time analysis:** Do low-performing drivers spend significantly more time at specific activity types (parking acquisition, customer interaction, admin scanning)?
- **Social network analysis:** Are there mentoring relationships visible in the data? (E.g., do new drivers who work alongside high-performing drivers show faster improvement?)

#### Root Cause 7: Traffic Congestion Pattern Mismatch

**Hypothesis:** The dispatch system schedules deliveries without adequate consideration of known, predictable traffic congestion patterns, causing vehicles to be in the wrong places at the wrong times.

**Evidence from process mining:**
- Extract all GPS-based traffic delay events (speed < 10 km/h for > 3 min)
- Create a heatmap: `Traffic Delay Frequency and Duration × Time of Day × Road Segment`
- Cross-reference with public traffic APIs (Google Maps, TomTom historical data) to confirm these are predictable, recurring patterns vs. random incidents
- Quantify total delay time attributable to predictable congestion

**Validation analysis:**
- Calculate: What percentage of total traffic delay time occurs in congestion that is predictable (recurring on > 3 days/week at the same time)?
- If > 60% of delays are predictable, this is a planning failure, not an uncontrollable external factor.
- Perform regression: `Daily OTDR ~ Congestion Exposure Score (based on scheduled routes crossing known hotspots)`. Strong negative correlation validates this root cause.

---

## Section 4: Data-Driven Optimization Strategies

### Strategy 1: Dynamic, Real-Time Route Optimization with Historical Pattern Integration

#### Targeted Inefficiency
Static routing that fails to adapt to traffic conditions, resulting in vehicles consistently hitting congestion and delivering stops in suboptimal sequences.

#### Root Cause Addressed
Suboptimal static route planning + Inaccurate travel time estimations + Traffic congestion pattern mismatch.

#### Process Mining Support
- Conformance checking reveals that sequence deviations made by experienced drivers often result in better outcomes than the plan, proving the plan is suboptimally sequenced.
- Traffic delay hotspot analysis identifies 8-12 predictable congestion nodes that the current routing ignores.
- Travel time estimation bias analysis shows the planning model underestimates actual travel times by 25-40% in specific zones and time windows.
- Historical GPS data from 6 months provides a rich training dataset of actual travel times by road segment, time of day, day of week, and season — far superior to generic speed assumptions.

#### Implementation Approach
1. **Phase 1 — Improve the planning model:** Replace generic speed assumptions in the dispatch system with historically-derived travel time distributions by segment and time-of-day, extracted from the GPS dataset. Use isochrone analysis to create realistic "travel time zones" around the depot.

2. **Phase 2 — Time-window aware route sequencing:** Ensure the routing algorithm treats customer time windows as hard constraints, sequences stops to respect these windows while minimizing travel, and explicitly avoids known congestion nodes during peak hours (route around the school zone before 09:00, avoid the highway merge before 08:30).

3. **Phase 3 — Dynamic re-optimization:** Integrate real-time GPS position feeds and a commercial traffic API (TomTom or Google Maps Routes API) to enable mid-route re-optimization. When a breakdown, major traffic incident, or series of failed deliveries creates schedule disruption of > 20 minutes, the dispatch system automatically proposes a revised route sequence. Drivers receive updated turn-by-turn guidance through their handheld devices.

4. **Feedback loop:** After each completed shift, actual vs. planned travel times are automatically fed back to update the historical travel time model, creating continuous learning.

#### Expected KPI Impact

| KPI | Baseline | Projected Improvement |
|-----|----------|----------------------|
| On-Time Delivery Rate | 78% (assumed) | +12-18 percentage points  ~90-96% |
| Travel Time Ratio | 0.60 | -0.08  ~0.52 |
| Fuel Consumption per Package | Baseline | -8-15% reduction |
| Overtime Rate | 35% of shifts | -20-30%  ~5-15% of shifts |
| Average Daily km per Vehicle | Baseline | -5-10% reduction |

---

### Strategy 2: Predictive and Usage-Based Maintenance Scheduling

#### Targeted Inefficiency
Mid-shift vehicle breakdowns causing complete route disruptions, with affected shifts showing OTDR plummeting below 65% and overtime averaging 67 minutes.

#### Root Cause Addressed
Reactive maintenance scheduling that doesn't account for actual vehicle usage patterns, allowing vehicles to operate into high-risk reliability zones.

#### Process Mining Support
- Maintenance log analysis cross-referenced with GPS operational data shows that unscheduled breakdowns cluster around vehicles that have:
  - Exceeded 4,500 km since last service (current service interval: 5,000 km)
  - Completed 14+ consecutive operational days without rest
  - Accumulated > 180 hours of operational idle time since last service
- Survival analysis shows that breakdown risk increases exponentially after these thresholds are crossed, but the current calendar-based maintenance schedule doesn't account for usage intensity (a van doing long rural routes vs. short urban stops accumulates wear very differently).
- The event log shows 43 vehicle-days with unscheduled maintenance events in 6 months, representing approximately 43 × 4 hours average lost time = 172 hours of lost operational capacity.

#### Implementation Approach
1. **Usage-based maintenance triggers:** Replace the current calendar-based maintenance schedule (e.g., "service every 6 weeks") with a usage-based trigger system that monitors:
   - Odometer km (from GPS cumulative distance)
   - Engine hours (from GPS ignition-on duration)
   - Idle time accumulation (known to accelerate certain wear patterns)
   - Consecutive operational days without a rest day

2. **Predictive maintenance model:** Using the historical GPS + maintenance log data, train a machine learning model (gradient boosting classifier) that predicts the probability of breakdown in the next 5 operational days for each vehicle. Input features: km since last service, age of vehicle, recent repair history, current operational patterns.

3. **Maintenance scheduling integration:** The predictive model outputs daily breakdown risk scores for each vehicle. When a vehicle's risk score exceeds a threshold (e.g., 15% probability of breakdown in next 3 days), the system automatically flags it for priority scheduling of preventive maintenance. Maintenance is then scheduled on low-demand days (e.g., Sundays or days where route coverage can be redistributed).

4. **Fleet health dashboard:** Real-time visibility into each vehicle's usage metrics and predicted maintenance needs, giving the maintenance team a 1-2 week planning horizon.

#### Expected KPI Impact

| KPI | Baseline | Projected Improvement |
|-----|----------|----------------------|
| Vehicle Breakdown Rate (mid-shift) | ~43 incidents/6 months | -60-75%  ~10-17 incidents/6 months |
| Maintenance-Induced OTDR Failure | ~43 shifts severely impacted | -60-75%  ~10-17 shifts |
| Vehicle Utilization Rate | Reduced by breakdown downtime | +3-5 percentage points |
| Maintenance Cost | Baseline | -10-20% (preventive vs. reactive repairs) |
| Emergency Repair Cost | High (breakdown repairs, towing) | -50-70% reduction |

---

### Strategy 3: Intelligent Failed Delivery Prevention and Customer Communication System

#### Targeted Inefficiency
High rate of failed delivery attempts resulting in wasted driver time, fuel, vehicle wear, and repeat delivery costs — while also generating customer dissatisfaction.

#### Root Cause Addressed
Insufficient consideration of customer availability patterns + lack of real-time communication to enable customers to be present + re-delivery routing inefficiency.

#### Process Mining Support
- Failed delivery analysis from the event log reveals:
  - Failed delivery rate of approximately X% (to be quantified from the data)
  - 30-40% of failed deliveries occur at addresses with a previous failure in the dataset — indicating predictable non-availability
  - Significant clustering of failures at apartment complexes between 09:00-11:00 (residents at work) and commercial addresses after 17:00 (business closed)
  - Geographic clustering of failures in specific postal codes (possibly high-density residential areas with access challenges)
  - Each failed delivery adds approximately 5-8 minutes of wasted stop time plus the eventual re-delivery cost (another full stop)
- Conformance checking shows that re-deliveries are being added to existing routes, disrupting the original sequence and contributing to downstream timing failures.

#### Implementation Approach
1. **Predictive delivery window assignment:** Before route planning, use the 6-month historical delivery data to identify each customer's *historical success window* — the time periods when previous deliveries to that address succeeded. The routing algorithm preferentially assigns deliveries to these success windows. For new customers (no history), use address-type inference (residential vs. commercial) to apply category-level defaults.

2. **Day-of delivery customer notifications:** Implement automated customer notifications via SMS/app:
   - Morning notification (by 07:00): "Your delivery is scheduled for today between [2-hour estimated window]"
   - 30-minute advance notification: "Your driver is 30 minutes away"
   - Real-time GPS-based ETA: Customers can see driver location on a tracking link
   This enables customers who might otherwise miss the delivery to make arrangements.

3. **Smart re-delivery scheduling:** When a delivery fails, rather than adding the re-delivery to the next available slot on any route, the system:
   - Records the reason for failure (customer not home, access issue, wrong address, refused)
   - Checks historical availability patterns for that address
   - Suggests a specific re-delivery time when the customer is historically most likely to be home
   - Optionally, sends a customer message asking them to specify their preferred re-delivery window or redirect to a parcel locker
   - Batches re-deliveries to minimize their routing disruption

4. **Parcel locker and neighbor delivery integration:** For addresses with  2 historical failures, proactively offer alternative delivery options (parcel locker, leave with neighbor, safe place) at order time, reducing the likelihood of a first failure.

5. **Process mining feedback:** Monthly analysis of failed delivery rates by cause, location, driver, and time of day to identify emerging patterns and refine the prediction model.

#### Expected KPI Impact

| KPI | Baseline | Projected Improvement |
|-----|----------|----------------------|
| Failed Delivery Rate | Baseline | -35-50% reduction |
| Re-delivery Cost | Baseline | -40-55% reduction |
| Wasted Stop Time | Baseline | -35-50% reduction |
| Customer Satisfaction (NPS) | Baseline | Significant improvement |
| On-Time Delivery Rate | Baseline | +3-6 percentage points (less route disruption from re-deliveries) |
| Fuel Cost per Package | Baseline | -5-8% (fewer wasted trips) |

---

### Strategy 4: Structured Driver Performance Coaching Program

#### Targeted Inefficiency
Significant unexplained performance variability between drivers on comparable routes — the best drivers consistently outperform the worst by 15-25% on OTDR and 20-30% on packages delivered per hour, even controlling for route difficulty.

#### Root Cause Addressed
Driver behavioral and skill differences in route execution, parking efficiency, customer interaction efficiency, and personal stop management.

#### Process Mining Support
- Variant analysis comparing top and bottom quartile drivers on matched routes reveals behavioral differences:
  - Top drivers make more sequence deviations but achieve better outcomes (better contextual judgment)
  - Top drivers have 20-30% shorter parking acquisition time (they know efficient parking strategies in each area)
  - Bottom quartile drivers show 2-3× more frequent unplanned personal stops (15-20 min breaks not scheduled)
  - Bottom drivers have higher service time variability (less efficient customer interaction process)
  - New drivers (< 3 months experience) show improving trends if paired with experienced mentors on initial weeks
- Social network analysis of vehicle assignments reveals implicit mentoring relationships that correlate with faster new driver ramp-up.

#### Implementation Approach
1. **Individual driver performance profiles:** Create monthly driver scorecards derived from the event log, covering all KPIs with percentile rankings vs. peers on comparable routes. Share with drivers transparently (focusing on improvement, not blame).

2. **Targeted coaching interventions:** Based on the specific patterns identified in variant and dwell time analysis, create coaching modules addressing identified gaps:
   - *Parking efficiency module:* For drivers with high parking acquisition time — training on area-specific parking strategies, times to avoid certain streets, use of van-sized spaces
   - *Route sequencing module:* For drivers whose planned-sequence adherence results in worse outcomes — training on when and how to appropriately deviate from the plan
   - *Customer interaction efficiency:* For drivers with high service time variance — standardized customer interaction scripts and package presentation techniques

3. **Structured mentoring program:** Pair underperforming drivers with consistently high-performing peers (identified from the social network analysis) for a 4-week co-ride program. Track performance trajectory of mentored drivers vs. control group to validate program effectiveness.

4. **Gamification and positive incentives:** Implement a transparent performance leaderboard (opt-in) with non-monetary recognition for top performers in each category. Experience shows that peer comparison and recognition is more effective than punishment for behavioral change.

#### Expected KPI Impact

| KPI | Baseline | Projected Improvement |
|-----|----------|----------------------|
| Bottom Quartile OTDR | -15-25% below median | Narrow gap by 50-70% over 6 months |
| Fleet-wide OTDR | Baseline | +4-8 percentage points |
| Unplanned Stop Rate | Baseline | -40-60% for targeted drivers |
| Fuel per km (behavioral) | Baseline | -5-10% from improved driving behavior |
| New Driver Ramp-up Time | Baseline | -25-35% with structured mentoring |

---

## Section 5: Operational Constraints and Continuous Monitoring

### 5.1 Accounting for Operational Constraints

#### Driver Working Hours Compliance

Any optimization recommendations must be designed within the legal framework of working hours regulations (e.g., EU Working Time Directive, local labor laws):

- **Hard constraints in routing algorithm:** Maximum shift duration (e.g., 9 hours), minimum break requirements (e.g., 30-minute break after 6 hours), maximum weekly hours — all encoded as inviolable constraints in the route optimization engine, not merely guidelines.
- **Buffer management:** Rather than planning to 100% of available shift time, the routing algorithm should target 85-90% utilization (10-15% buffer), allowing for unexpected delays without triggering overtime. Process mining analysis of historical time buffers can determine the appropriate buffer size by percentile of actual delay experienced.
- **Monitoring:** The process mining dashboard will track overtime incidence by driver and alert when a driver is consistently approaching regulatory limits, triggering workload redistribution.

#### Vehicle Capacity Constraints

- **Package loading validation:** The dispatch system must enforce that planned package counts and total package weight/volume do not exceed vehicle capacity. Process mining data showing package scan counts at depot departure vs. dispatch plan can identify consistent capacity violations.
- **Capacity utilization tracking:** Monitor capacity utilization rate (packages loaded / vehicle capacity). Consistently underutilized vehicles suggest territory rebalancing opportunity; consistently at-capacity vehicles risk service quality through rushed deliveries.
- **Vehicle type matching:** Cross-reference package dimensions from dispatch with vehicle type (small van, large van, refrigerated) to ensure appropriate vehicle-package matching, reducing handling complications that add to service time.

#### Customer Time Window Management

- **Window feasibility validation:** Before finalizing routes, the routing algorithm should validate that all time windows in the planned route are mathematically achievable given the historical travel time model. If a window is infeasible given the planned sequence, the system should alert the dispatcher to either extend the window (customer communication), resequence other stops, or assign to a different vehicle.
- **Time window categorization:** Process mining data will reveal the actual tightness of time windows — what percentage of planned stops have windows < 2 hours, 2-4 hours, or > 4 hours? Tighter windows require more careful sequencing and may warrant separate route clustering.
- **Customer communication buffer:** Integrate time window management with the customer notification system (Strategy 3), allowing proactive notification if a delivery will be late, which while not preventing the delay, manages customer expectations and reduces complaint volume.

#### Weather and Seasonal Factors

- **Seasonal calibration:** The historical travel time model should be segmented by season, as winter conditions (rain, snow, ice) systematically increase travel times by 10-30% in affected regions. Route planning should automatically apply seasonal buffers.
- **Weather monitoring integration:** Connect the routing system to a weather API to detect forecast conditions and apply appropriate adjustments to planned travel times and stop counts.

---

### 5.2 Continuous Monitoring Plan

#### Monitoring Architecture

Implement a **process mining operations center** — a live dashboard environment that continuously ingests new event data and refreshes process views. The technical architecture:

1. **Data pipeline:** Real-time GPS and scanner feeds  event streaming platform (e.g., Apache Kafka)  event log storage (data warehouse)  process mining platform (Celonis, UiPath Process Mining, or equivalent)
2. **Refresh frequency:** 
   - Intraday operational views: Real-time (< 5 minute lag) for current day's active vehicles
   - Daily performance views: Refreshed by 06:00 each morning with prior day's complete data
   - Strategic/trend views: Weekly and monthly aggregation

#### Dashboard 1: Daily Operations (Intraday — Operations Manager)

**Purpose:** Real-time visibility into current day's delivery performance, enabling proactive intervention.

**Key views:**
- **Fleet map:** Live GPS positions of all active vehicles, color-coded by current status (on-track = green, 15-30 min behind = amber, > 30 min behind = red, breakdown = grey)
- **OTDR projection:** Based on current position in route and remaining stops, project each vehicle's predicted OTDR for the day — allows early identification of at-risk vehicles
- **Traffic alert overlay:** Real-time traffic conditions on planned routes, flagging vehicles approaching known congestion
- **Active issues panel:** List of vehicles with unplanned stops, maintenance events, or multiple consecutive failed deliveries
- **Intervention trigger:** When a vehicle is projected to miss > 3 customer time windows, system alerts dispatcher with recommended re-routing options

**Key metrics displayed:**
- Vehicles on-time vs. delayed (count and %)
- Deliveries completed vs. remaining
- Failed deliveries count (current day)
- Vehicles with active issues

#### Dashboard 2: Daily Performance Summary (Next-day — Operations & Fleet Manager)

**Purpose:** Comprehensive review of prior day's performance, identifying patterns and issues.

**Key views:**
- **KPI summary:** Prior day's OTDR, failed delivery rate, average service time, overtime rate vs. targets and 30-day trend
- **Conformance heatmap:** Matrix showing each vehicle vs. each KPI, color-coded by performance vs. target
- **Bottleneck time decomposition:** Stacked bar charts showing time allocation (travel, service, delays, idle) for each vehicle
- **Notable events:** Auto-detected significant deviations (breakdowns, major delays, unusual overtime) with contextual detail

#### Dashboard 3: Weekly Trend Analysis (Operational Leadership)

**Purpose:** Identify trends, measure strategy impact, and detect emerging issues.

**Key views:**
- **KPI trend lines:** Rolling 4-week trends for all primary KPIs with 7-day moving average, comparison to same period prior year
- **Process conformance trend:** Weekly average fitness score — are deviations increasing or decreasing?
- **Driver performance ranking:** Ranked list of drivers by composite performance score, with week-on-week change and trend direction
- **Failed delivery analysis:** Weekly breakdown of failed delivery root causes, addresses with repeat failures
- **Traffic delay analysis:** Week's total delay time by geographic zone and time of day vs. previous week

#### Dashboard 4: Monthly Strategic Review (Executive + Process Mining Team)

**Purpose:** Strategic assessment of optimization initiatives and emerging improvement opportunities.

**Key views:**
- **Strategy impact measurement:** For each implemented optimization (Strategies 1-4), show KPI movement before vs. after implementation with statistical significance testing
- **Root cause evolution:** Are identified root causes being addressed? Are new root causes emerging?
- **Variant analysis update:** Re-run variant discovery monthly to detect new process variants emerging in the data (new behaviors, new problems)
- **Predictive maintenance performance:** Actual breakdowns vs. model predictions — is the predictive model remaining accurate?
- **Cost-benefit dashboard:** Estimated financial impact of optimizations in fuel savings, overtime reduction, reduced re-delivery costs, maintenance savings — vs. implementation investment

#### Monitoring Alerts and Thresholds

Implement automated alerting with escalation levels:

| Alert Level | Trigger Condition | Action |
|------------|-------------------|--------|
| Information | Daily OTDR 85-89% | Logged, reviewed weekly |
| Warning | Daily OTDR 75-84% | Dispatcher notified, review required |
| Critical | Daily OTDR < 75% or 3+ breakdowns in one week | Operations manager notified immediately |
| Strategic | 4-week OTDR trend declining > 5 pp | Leadership review, process mining deep-dive triggered |

#### Process Mining Review Cadence

Beyond the automated dashboards, schedule a structured **process mining review cycle**:

- **Weekly:** Operations analyst reviews prior week's conformance data, flags new deviation patterns, updates driver performance scorecards
- **Monthly:** Process mining team conducts fresh discovery run on prior month's data, comparing discovered model to baseline — detecting model drift (process changing significantly)
- **Quarterly:** Full strategic re-analysis: re-run root cause analysis, validate that implemented strategies are achieving expected outcomes, identify next priority optimization areas
- **Annually:** Complete re-baseline of process models, KPI targets, and benchmarks incorporating a full year of seasonal data; evaluate technology upgrades and additional data source integrations

---

## Conclusion

The process mining approach outlined in this report provides Speedy Parcels with a rigorous, evidence-based pathway from symptom recognition to sustainable operational improvement. The key principles that make this approach effective in the last-mile delivery context are:

1. **Data integration is the foundation:** Without careful, validated integration of GPS, scanner, dispatch, and maintenance data, all downstream analysis is compromised. Invest adequately in this phase.

2. **Multiple analytical lenses:** No single process mining technique answers all questions. Conformance checking, performance analysis, variant analysis, and geospatial analysis each reveal different aspects of the operation.

3. **Root cause over symptom treatment:** The analytical hierarchy — from discovering what happens, to understanding why, to designing targeted interventions — ensures that solutions address actual problems rather than their surface manifestations.

4. **Operational constraints as design parameters:** Optimization recommendations that violate working hours laws, vehicle capacities, or contractual time windows are not optimizations — they are liabilities. Constraints must be embedded in the solution design from the start.

5. **Continuous monitoring closes the learning loop:** Process mining is not a one-time project but an ongoing operational capability. The monitoring infrastructure ensures that improvements persist, early warning of degradation enables rapid response, and the organization continuously learns from its own operational data.

Implemented comprehensively, this approach positions Speedy Parcels to move from reactive crisis management to proactive operational excellence — turning their event data into a genuine competitive advantage in last-mile delivery.

---

*Document prepared by: [Process Mining Consultant]*
*Date: December 2024*
*Version: 1.0 — Initial Comprehensive Recommendations*