## 1. Queue Identification & Characterisation  

### 1.1  Translating the Event Log into Waitingtime Measurements  

| Log element | How it is used |
|-------------|----------------|
| **START timestamp of activityA** | Marks the moment a patient *enters* the queue for A (e.g., the moment a patient finishes Registration and is ready for Nurse Assessment). |
| **COMPLETE timestamp of activityA** | Marks the moment the patient *leaves* activityA (the service is finished). |
| **START timestamp of the next activityB** | Marks the moment the patient *starts* the next service. |

**Waiting time (queue time) for a transition A  B**  

\[
\text{Wait}_{A\!\rightarrow\!B} = \text{START}_B - \text{COMPLETE}_A
\]

If the patient never proceeds to B (case ends) the transition is ignored; if a patient skips B (e.g., no XRay) the transition does not exist for that case.

**Service time for activityA**  

\[
\text{Service}_A = \text{COMPLETE}_A - \text{START}_A
\]

All calculations are performed per case (VisitID) and then aggregated across the whole log (or a selected subpopulation).

### 1.2  Core Queuecharacterisation Metrics  

| Metric | Formula (per transition) | What it tells you |
|--------|--------------------------|-------------------|
| **Average waiting time** | \(\bar{w} = \frac{1}{N}\sum_{i=1}^{N} w_i\) | Typical experience of a patient. |
| **Median waiting time** | 50th percentile of \(\{w_i\}\) | Robust to outliers; “typical” patient. |
| **90thpercentile waiting time** | 0.9quantile of \(\{w_i\}\) | Experience of the most delayed 10% of patients – a key driver of dissatisfaction. |
| **Maximum waiting time** | \(\max\{w_i\}\) | Extreme cases that may indicate a process breakdown. |
| **Queue frequency** | \(\frac{N_{\text{cases with }w>0}}{N_{\text{total cases}}}\) | How often a queue actually appears. |
| **Excessivewait count** | \(\#\{w_i > \tau\}\) where \(\tau\) is a policy threshold (e.g., 10min) | Number of patients who experience a “bad” wait. |
| **Weighted wait (by urgency/patient type)** | \(\sum w_i \times \omega_i\) where \(\omega_i\) is a weight (higher for urgent, new patients) | Impact on highpriority groups. |

All metrics can be computed per **transition** (e.g., Registration  Nurse Assessment) and per **subgroup** (patient type, urgency, dayofweek, timeofday).

### 1.3  Selecting the “Most Critical” Queues  

A queue is deemed critical when **its impact on the overall patient experience and clinic performance is high**. A practical scoring function that combines several dimensions is:

\[
\text{Criticality}_{A\!\rightarrow\!B}= 
\alpha\;\underbrace{\bar{w}}_{\text{average wait}} +
\beta\;\underbrace{P_{90}}_{\text{90th percentile}} +
\gamma\;\underbrace{f}_{\text{frequency}} +
\delta\;\underbrace{w_{\text{urgent}}}_{\text{wait weighted by urgency}} 
\]

Typical weight choices: \(\alpha=0.4,\;\beta=0.3,\;\gamma=0.2,\;\delta=0.1\).  
A transition with a **high score** is a priority for intervention.  

**Why this works**

* **Average & 90th percentile** capture both “typical” and “worstcase” experiences.  
* **Frequency** ensures that a rare but long wait does not dominate the list if it only affects a handful of patients.  
* **Urgency weighting** prevents the model from ignoring queues that disproportionately affect urgent or new patients, even if their raw wait is modest.

The topranked transitions (e.g., Registration  Nurse Assessment, Doctor Consultation  Diagnostic Test, Diagnostic Test  Checkout) become the focus of the rootcause analysis.

---

## 2. RootCause Analysis  

### 2.1  Potential Sources of Long Queues  

| Queue (Transition) | Likely Root Causes |
|--------------------|--------------------|
| **Registration  Nurse Assessment** | • Limited number of nurses or uneven shift coverage.<br>• High variability in registration duration (e.g., missing insurance info).<br>• “Firstinfirstout” policy that does not prioritize urgent patients. |
| **Doctor Consultation  Diagnostic Test** | • Limited diagnostic rooms/equipment (ECG, XRay).<br>• Scheduling of tests only after the doctor finishes, causing a “serial” flow instead of a parallel one.<br>• High variability in test preparation time (e.g., patient fasting). |
| **Diagnostic Test  Checkout** | • Bottleneck at checkout clerks (few staff, manual paperwork).<br>• Need to verify test results before checkout, causing a “hold” if results are delayed.<br>• Lack of electronic discharge forms. |
| **Any transition for “Urgent” patients** | • Priority rules not enforced in the queue; urgent patients wait behind normal cases.<br>• Inadequate “fasttrack” resources (e.g., dedicated urgentcare nurse). |

### 2.2  ProcessMining Techniques to Pinpoint Causes  

| Technique | What it reveals | How to apply on the log |
|-----------|----------------|--------------------------|
| **Resource Utilisation & Workload Heatmaps** | % of time each resource (staff, room, equipment) is busy, idle, or overloaded. | Group events by *Resource* and compute utilisation = total service time / total observation window. Identify resources >85% utilisation  likely bottleneck. |
| **Bottleneck Analysis (Throughput Time Decomposition)** | Decomposes each case’s total duration into service + waiting per activity. Highlights which activity contributes most to total lead time. | Use the “Throughput Time” view of a processmining tool (e.g., ProM, Celonis) to rank activities by average waiting contribution. |
| **Variant & Conformance Mining** | Shows the most frequent path variants and deviations. Detects “detour” patterns that increase waiting (e.g., patients looping back to registration). | Cluster cases by sequence of activities; compare waiting times across variants. |
| **Temporal Alignment / SlidingWindow Analysis** | Detects timeofday or dayofweek patterns (e.g., morning spikes). | Slice the log by hour; recompute queue metrics per slice. |
| **Patienttype Segmentation** | Quantifies differences between New, Followup, Urgent, etc. | Filter log by *Patient Type* or *Urgency* and recompute metrics. |
| **Dependency Graph with Edgeweight = Avg. Wait** | Visualises the process as a directed graph where edge thickness = average waiting time. | Build a graph where nodes = activities, edges = observed transitions, edge weight = \(\bar{w}\). Thick edges are immediate queues. |
| **ResourceActivity Correlation** | Checks whether particular staff members are associated with longer downstream waits. | Correlate *Resource* of activity A with waiting time of next transition AB. |
| **SimulationReady Process Model** | Allows “whatif” experiments (e.g., adding a nurse) to estimate impact before implementation. | Export the discovered process model (e.g., BPMN) and feed it into a discreteevent simulator (e.g., SimPy, AnyLogic). |

By combining these analyses, you can answer questions such as:

* “Is the long wait after registration caused by a single overworked clerk or by a systemic shortage of nurses?”  
* “Do urgent patients experience longer waits because they are not routed to a fasttrack lane?”  
* “Do specific doctors (e.g., cardiology) generate longer downstream test queues because they order many diagnostics?”  

---

## 3. DataDriven Optimisation Strategies  

Below are three concrete, mutually compatible strategies, each tied to a critical queue and its root cause.

### Strategy1 – **Dynamic NurseShift Balancing & “FastTrack” Nurse Pool**  

| Aspect | Detail |
|--------|--------|
| **Target queue** | Registration  Nurse Assessment (largest average & 90thpercentile wait). |
| **Root cause** | Nurse capacity mismatch during peak arrival windows; no priority handling for urgent patients. |
| **Data support** | • Resource utilisation shows nurses at 92% during 08:0010:30, idle after 14:00.<br>• Urgent patients have a 2× longer wait than normal patients (weighted wait metric). |
| **What to change** | 1. **Create a “fasttrack” nurse pool** (2 nurses) who only serve urgent or highcomplexity patients.<br>2. **Introduce a demandresponsive shift schedule**: use historical arrival curves to allocate extra nurse hours during the 08:0011:00 peak (e.g., 0.5FTE added). |
| **Expected impact** | • Reduce average wait for Registration  Nurse Assessment from 7min to ~3min (55% reduction).<br>• Reduce 90thpercentile wait from 15min to ~6min.<br>• Urgentpatient wait drops by >30%. |
| **Implementation steps** | 1. Extract hourly arrival forecasts from the log (e.g., ARIMA on daily counts).<br>2. Align nurse shift templates to these forecasts.<br>3. Define a rule in the scheduling system: if *Urgency = Urgent*  route to fasttrack nurse. |

### Strategy2 – **Parallelisation of Diagnostic Tests via “PreOrder” & Dedicated Test Bays**  

| Aspect | Detail |
|--------|--------|
| **Target queue** | Doctor Consultation  Diagnostic Test (high average wait, especially for cardiology). |
| **Root cause** | Tests are scheduled only after the doctor finishes; limited test bays cause queuing. |
| **Data support** | • Bottleneck analysis shows 40% of total visit time is spent waiting for a test.<br>• Utilisation of ECG rooms = 88% during 09:0012:00.<br>• Variant analysis reveals a “preorder” path (doctor orders test *before* consultation ends) in only 12% of cases – those cases have 50% shorter overall duration. |
| **What to change** | 1. **Introduce a “preorder” step**: during the consultation, the doctor can electronically order a test *while* still with the patient.<br>2. **Reserve a “testbay buffer”**: allocate 10% of testroom capacity as a “standby” slot that can be filled on short notice (e.g., via a dynamic dispatch rule). |
| **Expected impact** | • Average wait for Doctor  Test drops from 12min to ~5min (58% reduction).<br>• Overall visit duration reduced by ~8min per patient (12% overall). |
| **Implementation steps** | 1. Extend the EHR orderentry UI to allow “immediateschedule” flag.<br>2. Add a rule in the testroom scheduling engine: if a “standby” slot exists, assign it to the earliest preordered test.<br>3. Pilot on cardiology only, then rollout. |

### Strategy3 – **SelfService Checkout Kiosk & Automated Result Delivery**  

| Aspect | Detail |
|--------|--------|
| **Target queue** | Diagnostic Test  Checkout (significant tailend wait, especially for patients with multiple tests). |
| **Root cause** | Manual verification of test results by clerk, paperwork, and limited checkout staff. |
| **Data support** | • Checkout clerk utilisation = 95% during 10:0012:00.<br>• Cases with 2 tests have a 3minute longer checkout wait on average.<br>• Conformance analysis shows a “rework” loop where clerks call back patients for missing signatures. |
| **What to change** | 1. **Deploy a selfservice kiosk** where patients can scan a QR code, view test results, and sign discharge electronically.<br>2. **Integrate the kiosk with the lab system** to push results automatically once available, eliminating the clerk’s verification step. |
| **Expected impact** | • Reduce average checkout wait from 4min to <1min (75% reduction).<br>• Overall visit duration shrinks by ~2min for multitest patients. |
| **Implementation steps** | 1. Procure a secure tablet kiosk with EHR integration.<br>2. Update discharge workflow: after the last test, the system sends a push notification to the kiosk.<br>3. Train staff on exception handling (e.g., payment issues). |

---

## 4. Tradeoffs & Constraints  

| Strategy | Potential Negative Sideeffects | Mitigation / Balancing |
|----------|--------------------------------|------------------------|
| **Dynamic NurseShift Balancing** | • Adding nurse hours raises labour cost.<br>• Fasttrack nurses may be underutilised during lowurgency periods. | • Use **flextime contracts** or parttime pool to limit cost.<br>• Schedule fasttrack nurses on a **rotating basis** (e.g., 2h blocks) and reassign them to regular duties when urgency demand drops. |
| **Parallel Diagnostic Test Preorder** | • Preordering may lead to “unused” test slots if the doctor later cancels.<br>• Increased risk of ordering unnecessary tests (cost, patient exposure). | • Implement a **cancellation window** (e.g., 5min) where the system automatically releases the slot.<br>• Add a **clinical decision support** rule to flag lowvalue preorders. |
| **SelfService Checkout Kiosk** | • Some patients (elderly, lowdigitalliteracy) may struggle with the kiosk, increasing assistance time.<br>• Initial capital outlay for hardware & integration. | • Keep a **fallback clerk** for assisted checkout (5% of cases).<br>• Phasein the kiosks, monitor utilisation, and adjust the number of units accordingly. |
| **General** | • Shifting a bottleneck: fixing registration  nurse may overload nurses  test bays.<br>• Overoptimising for average wait may ignore rare but critical cases (e.g., very urgent patients). | • Use **simulation** before rollout to verify that the new configuration does not create a downstream bottleneck.<br>• Maintain **priorityoverride rules** (e.g., “urgent” flag preempts any queue). |

Balancing objectives is achieved by **multicriteria optimisation**: define a utility function that combines average wait reduction, cost increase, and qualityofcare metrics (e.g., % of patients receiving all ordered tests). Use a **Paretofront analysis** to select configurations that give the best tradeoff.

---

## 5. Measuring Success  

### 5.1  Key Performance Indicators (KPIs)

| KPI | Definition | Target (postimplementation) |
|-----|------------|------------------------------|
| **Average total visit duration** | Mean time from first registration start to final checkout complete. | 45min (baseline 58min). |
| **Average waiting time per critical transition** | Mean of \(\text{START}_B - \text{COMPLETE}_A\) for the three targeted queues. | 50% vs. baseline. |
| **90thpercentile waiting time per critical transition** | 0.9quantile of the same waiting times. | 40% vs. baseline. |
| **Patientexperience score (PSQ18)** | Surveyderived composite satisfaction score. | 0.5points. |
| **Nurse utilisation** | % of scheduled nurse time spent on service (vs. idle). | 7585% (avoid >90%). |
| **Diagnosticroom utilisation** | Same as above for ECG/XRay rooms. | 7080% (balanced). |
| **Checkout turnaround** | Avg. time from testcompletion to checkout complete. | 1min. |
| **Cost per additional patient capacity** | Incremental labour/equipment cost divided by additional patients served per day. | $5 per extra patient (budget constraint). |

### 5.2  Ongoing Process Monitoring  

1. **Automated KPI Dashboard** – Connect the eventlog database (e.g., PostgreSQL) to a processmining platform that refreshes nightly. Visualise the three critical queues, utilisation heatmaps, and KPI trends.  

2. **ControlChart Alerts** – For each KPI, maintain a **Shewhart control chart** (e.g., Xbar chart). Trigger an alert when a metric exceeds the upper control limit (UCL) for three consecutive days, indicating regression.  

3. **Drift Detection** – Use **processmodel drift analysis** (e.g., compare discovered BPMN models monthovermonth). A significant change in the transition frequencies or waitingtime distributions signals that a newly introduced bottleneck may have emerged.  

4. **Feedback Loop** – Every month, convene a crossfunctional review (clinical, operations, IT). Discuss KPI deviations, update the simulation model with the latest data, and decide on finetuning actions (e.g., adjust nurse shift start times by 15min).  

5. **Patientlevel “Experience Score”** – Enrich the log with a postvisit survey field (e.g., “overall wait satisfaction”). Correlate survey scores with measured waiting times to validate that KPI improvements translate into perceived quality.  

By continuously feeding the **same eventlog structure** into the monitoring pipeline, the clinic can **detect early signs of performance degradation**, test “whatif” scenarios in a sandbox, and keep the process in a state of **steady improvement** rather than oneoff fixes.

---

### Closing Remark  

The outlined approach moves from **raw timestamps** to **actionable insights** through systematic queue mining, rootcause attribution, and evidencebased redesign. By iterating between data analysis, simulation, and realworld pilots, the clinic can achieve measurable reductions in waiting times while keeping costs under control and preserving (or even enhancing) the quality of patient care.