# Comprehensive Process Mining Analysis for Speedy Parcels: A Consulting Framework

---

## Section 1: Process Discovery and Conformance Checking

### 1.1 Data Preprocessing and Integration

The foundation of any effective process mining engagement is a well-constructed, unified event log. Speedy Parcels' data comes from four heterogeneous sources — GPS trackers, handheld scanners, the dispatch system, and maintenance records — each with its own schema, granularity, temporal resolution, and semantic conventions. Transforming these into a single, analysis-ready event log requires a disciplined data engineering approach.

#### Defining the Case Notion

The most fundamental decision is choosing the *case* — the unit of analysis around which process instances are constructed. For Speedy Parcels, there are at least three meaningful case granularities:

- **Vehicle-Day (e.g., V12-20241205):** Captures the full operational shift of a vehicle — depot departure, all delivery stops, any unscheduled events, and depot return. This is the primary case notion for route-level analysis.
- **Package-Level (e.g., P9876):** Tracks the life-cycle of a single parcel from depot scan through delivery (or failed attempt and re-delivery). This supports analysis of failed delivery rates and re-delivery loops.
- **Driver-Day (e.g., D105-20241205):** Useful if a driver operates multiple vehicles or is reassigned during a shift, or for driver-centric performance benchmarking.

For the primary analysis, **Vehicle-Day** is recommended as the main case identifier. Package-level analysis can be conducted as a subsidiary process view, linked back to vehicle-day cases via a shared key.

#### Event Log Schema Design

Each row in the unified log must contain, at minimum:

| Field | Source | Notes |
|---|---|---|
| Case ID | Derived | Vehicle ID + Date (e.g., V12-20241205) |
| Timestamp | All sources | Normalized to UTC, millisecond precision |
| Activity Label | Derived | Semantic label (see below) |
| Resource | Driver/Vehicle | Driver ID and/or Vehicle ID |
| Package ID | Scanner | Nullable for non-delivery events |
| Location | GPS/Scanner | Lat/Lon coordinates |
| Speed | GPS | km/h |
| Planned vs. Actual Flag | Dispatch | For conformance enrichment |
| Source System | Derived | GPS, Scanner, Dispatch, Maintenance |

#### Activity Labeling Strategy

Raw events from different systems use different terminology. A semantic mapping layer must be applied to produce a coherent activity vocabulary:

- **GPS "Ignition On" at Depot  "Depart Depot"** (when subsequent movement detected)
- **GPS "Speed < 5 km/h for > 3 minutes in non-depot zone"  "Unplanned Stop / Traffic Delay"**
- **GPS "Speed < 5 km/h for > 10 minutes at non-customer location"  "Unscheduled Stop"**
- **Scanner "Arrive Customer"  "Arrive at Stop"**
- **Scanner "Delivery Success"  "Package Delivered"**
- **Scanner "Delivery Failed"  "Delivery Attempted - Failed"**
- **Scanner "Depart Customer"  "Depart Stop"**
- **Dispatch "Route Assigned"  "Route Planned"**
- **Maintenance "Repair Start/End"  "Vehicle Under Repair"**

GPS data, because it is generated at very high frequency (potentially every 5–30 seconds), must be **aggregated and semantically elevated** rather than used at raw granularity. A dedicated GPS processing pipeline should:
1. Identify stop events (periods where speed  0 and ignition is on, for > N minutes)
2. Classify stops as: depot dwell, customer stop, traffic idle, unscheduled stop (cross-referencing with scanner events and known addresses)
3. Reconstruct travel segments between stops, recording distance, duration, and average speed

#### Temporal Alignment and Clock Synchronization

A critical and often underestimated challenge is **clock drift** between systems. The GPS trackers, handheld scanners, and dispatch system may have independent clocks that diverge by seconds or even minutes. This is particularly problematic when trying to merge events from different sources into a single timeline for the same case.

Mitigation strategies:
- **Anchor events:** Identify events that should logically be simultaneous across systems (e.g., a scanner "Arrive Customer" should coincide with a GPS stop detection within a small tolerance window). Use these to estimate and correct per-device clock offsets.
- **Plausibility checks:** Flag cases where scanner timestamps are outside GPS-detected stop intervals (e.g., a "Delivery Success" scan recorded while the GPS shows the vehicle moving at 40 km/h — likely a clock error or data quality issue).
- **Network Time Protocol (NTP) logs:** If available from device metadata, use these to apply systematic corrections.

#### Key Integration Challenges

| Challenge | Description | Mitigation |
|---|---|---|
| **Granularity Mismatch** | GPS generates thousands of events per case; scanner generates ~2-5 per stop; dispatch generates 1-2 per day | Aggregate GPS before merging; use event abstraction |
| **Missing Data** | GPS gaps (tunnel, parking garage), scanner misses (driver forgets to scan), dispatch system outages | Flag missing data explicitly; use imputation for minor gaps (e.g., interpolate GPS path); do not impute for major gaps |
| **Duplicate Events** | GPS may log multiple "stop" events for the same physical stop due to brief movements (e.g., adjusting parking) | Deduplication with temporal proximity threshold (e.g., merge stops < 60 seconds apart at same location) |
| **Location Ambiguity** | Customer address in dispatch vs. GPS coordinates may not match exactly (GPS accuracy, address geocoding errors) | Fuzzy spatial matching with a tolerance radius (e.g., 50m) |
| **Semantic Inconsistencies** | "Unscheduled Stop" could mean a traffic jam, a personal break, a mechanical issue, or an informal delivery attempt | Use additional context (maintenance logs, Notes field, duration, location type) to disambiguate |
| **Data Privacy** | Fine-grained driver location and behavior data raises GDPR/privacy concerns | Anonymize driver IDs in analytical outputs; establish data governance protocols |

#### The Resulting Event Log

After preprocessing, the event log for a single Vehicle-Day case (V12-20241205) might contain approximately 80–150 events, abstracting down to 20–40 semantically meaningful activities. Across six months and a fleet of, say, 30 vehicles, this could yield approximately 50,000–100,000 case instances of moderate complexity — a substantial but computationally tractable dataset for modern process mining tools (Celonis, ProM, Disco, PM4Py).

---

### 1.2 Process Discovery

With a clean event log, process discovery algorithms can automatically synthesize a model of *how deliveries actually happen*, as opposed to how they are planned to happen.

#### Algorithm Selection

**Alpha Miner** is too simplistic for real-world data with loops, parallelism, and noise. Instead, the following approaches are recommended:

- **Inductive Miner (Infrequent variant):** Best suited for this scenario. It handles noise and infrequent behavior gracefully, producing a sound process model (a process tree / Petri net) that covers the majority of cases while not being distorted by rare anomalies. The "infrequent" variant has a noise threshold parameter that filters out activities appearing in fewer than X% of cases, allowing the discovery of the "happy path" without losing the main variants.

- **Heuristic Miner:** Useful for a frequency-weighted map that visually emphasizes the most common paths. Good for stakeholder communication because it produces a simple, intuitive "spaghetti map" that can be progressively simplified.

- **Directly-Follows Graph (DFG):** As a lightweight exploratory starting point before committing to a formal model. Shows which activities directly follow which others, and with what frequency and average time.

- **BPMN Discovery (via PM4Py or Celonis):** For producing a business-readable model to share with operational management.

#### What the Discovered Model Would Reveal

The end-to-end delivery process model would capture:

1. **The Happy Path:** Depot departure  Sequential travel and delivery stops (each comprising: Travel, Arrive, Deliver/Attempt, Depart)  Depot return
2. **Loops:** Failed delivery  continue route  (potentially) retry same stop later
3. **Unplanned Branches:** Traffic delay episodes inserted between stops; unscheduled stops (mechanical, personal); re-routing events
4. **Maintenance Interruptions:** A branch leaving the main flow for a vehicle repair stop, then (if resolved) rejoining
5. **Route Sequence Variations:** Some drivers servicing stops in different orders than planned

A key insight from the DFG/Heuristic map would be the **variance in inter-stop travel time** — the same nominal "Stop 3  Stop 4" transition might show an average of 8 minutes but a standard deviation of 15 minutes, indicating high variability driven by traffic or parking.

#### Filtering and Variant Analysis at Discovery Stage

Rather than discovering a single model for all cases, it is analytically powerful to discover **separate models for distinct subsets**, for example:
- Cases where all deliveries succeeded vs. cases with 1 failed delivery
- Peak-hour departure cases vs. early-morning departure cases
- Individual drivers or driver cohorts
- Different geographic territories (urban core vs. suburban)

This comparative discovery immediately surfaces structural differences in how the process unfolds across contexts.

---

### 1.3 Conformance Checking

Conformance checking compares the *discovered actual process* against the *planned process* derived from the dispatch system. The dispatch system contains the normative reference: planned stop sequence, planned departure time, planned arrival time windows per stop, and planned total route duration.

#### Constructing the Reference Model

The dispatch system's planned route for each vehicle-day can be translated into a **normative process model** (e.g., a sequential Petri net or BPMN process): Depart Depot at T0  Arrive Stop 1 at T1_plan  Deliver Stop 1  Depart at T2_plan  …  Arrive Depot at Tn_plan. This becomes the "reference trace" for conformance calculation.

#### Conformance Checking Techniques

**Token Replay (Fitness Calculation):** "Plays" each actual trace through the normative model. Tokens that cannot be consumed (because an activity happened out of order or a step was skipped) or are left over at the end generate fitness penalties. The resulting **fitness score** (0 to 1) quantifies how closely actual execution followed the plan. A fleet-wide average fitness score below, say, 0.75 would be a significant finding.

**Alignment-Based Conformance:** A more sophisticated technique that computes the minimum edit distance between the actual trace and the normative model. Each deviation (a "move on log only" or "move on model only") is quantified and classified. This yields precise deviation diagnostics per case and per activity.

#### Types of Deviations to Look For

| Deviation Type | Description | Business Implication |
|---|---|---|
| **Sequence Deviation** | Driver visits stops in different order than planned | May indicate driver judgment (avoiding traffic) or may be suboptimal |
| **Skipped Stop** | A planned delivery stop has no scanner event | Package not attempted — likely re-scheduled, or scanning missed |
| **Unplanned Stop** | An event occurs at a location not on the planned route | Fuel stop, personal break, vehicle issue, informal errand |
| **Timing Deviation (+)** | Arrive at stop significantly later than planned | Traffic, previous stop took longer, route deviation upstream |
| **Timing Deviation (-)** | Arrive at stop significantly earlier than planned (rare) | Prior stop was skipped; planning was over-conservative |
| **Activity Loop** | Same stop visited twice (failed delivery then re-attempt) | Not accounted for in original plan; wastes time and fuel |
| **Extended Dwell Time** | Time at customer location far exceeds planned service time | Parking difficulty, complex delivery (multiple items), customer interaction |
| **Depot Departure Delay** | Vehicle departs significantly after planned time | Late loading, administrative delays, vehicle check issues |
| **Early/Late Return** | Return to depot substantially off schedule | Reflects cumulative deviation across all stops |

#### Conformance Metrics to Report

- **Per-Case Fitness Score:** Distribution across all vehicle-days
- **Deviation Frequency Matrix:** Which planned activities are most frequently deviated from, and in what direction
- **Timing Conformance by Stop Number:** Do deviations accumulate (stop #1 on-time, stop #35 severely late)? This would indicate cascading delay propagation
- **Deviation Hotspots by Geography:** Map of where sequence deviations and unplanned stops concentrate spatially

---

## Section 2: Performance Analysis and Bottleneck Identification

### 2.1 Key Performance Indicators

The following KPIs are directly calculable from the integrated event log and are aligned with Speedy Parcels' stated goals of punctuality and cost reduction:

#### Punctuality / Delivery Quality KPIs

**1. On-Time Delivery Rate (OTDR)**
- *Definition:* Percentage of deliveries where "Delivery Success" timestamp falls within the customer-requested time window defined in the dispatch system.
- *Calculation:* From the event log, for each Package ID with a "Delivery Success" event, compare the timestamp to the [TW_start, TW_end] from the dispatch record. `OTDR = Count(deliveries within window) / Count(all attempted deliveries) × 100%`
- *Target Benchmark:* Industry leaders typically achieve >95%

**2. Failed Delivery Rate (FDR)**
- *Definition:* Percentage of delivery attempts that result in "Delivery Failed" rather than "Delivery Success"
- *Calculation:* `FDR = Count("Delivery Failed" events) / Count("Arrive Customer" events) × 100%`
- *Note:* Segment by time of day, day of week, route, and package type to identify patterns

**3. Re-Delivery Rate**
- *Definition:* Percentage of packages requiring more than one delivery attempt
- *Calculation:* Count Package IDs with more than one "Arrive Customer" event across cases / Total Package IDs

#### Operational Efficiency KPIs

**4. Average Time per Delivery Stop (Service Time)**
- *Definition:* Mean time elapsed between "Arrive Customer" and "Depart Customer" events
- *Calculation:* For each stop in the event log: `Service_Time = Timestamp("Depart Customer") - Timestamp("Arrive Customer")`
- *Segmentation:* By driver, package type, time of day, address type (residential vs. commercial)

**5. Travel Time vs. Service Time Ratio**
- *Definition:* Proportion of total shift duration spent traveling vs. actively delivering
- *Calculation:* 
  - `Total Travel Time = Sum of all inter-stop travel durations + depot-to-first-stop + last-stop-to-depot`
  - `Total Service Time = Sum of all stop dwell times`
  - `Ratio = Total Travel Time / Total Service Time`
- *Insight:* A high ratio (e.g., 3:1) indicates routes may be geographically inefficient

**6. Stop Completion Rate**
- *Definition:* Actual stops completed as a fraction of planned stops
- *Calculation:* `Stops Completed / Stops Planned per vehicle-day`

**7. Depot Departure Delay**
- *Definition:* Minutes between planned departure time (from dispatch) and actual "Depart Depot" event
- *Calculation:* Directly from event log timestamps

#### Cost-Related KPIs

**8. Fuel Consumption per Delivered Package**
- *Definition:* Liters of fuel consumed per successfully delivered package
- *Calculation:* Requires integration with fuel card/telematics data (speed and engine-on data from GPS can estimate fuel consumption using vehicle-specific consumption curves). `Fuel/Package = Estimated_Daily_Fuel / Count(Delivery Success per vehicle-day)`

**9. Idle Time / Engine Idle Rate**
- *Definition:* Percentage of shift time where the engine is running but the vehicle is stationary (not at a customer stop or depot)
- *Calculation:* From GPS data: `Idle_Time = Duration where Speed=0 AND Ignition=On AND Location  Depot AND Location  Known Customer`
- *Note:* Idling in traffic is distinct from idling at an unscheduled stop — spatial context is needed for disambiguation

**10. Unscheduled Maintenance Downtime Rate**
- *Definition:* Hours lost per vehicle per month due to unscheduled repairs
- *Calculation:* From maintenance logs: `Sum(Repair_End - Repair_Start) for Unscheduled events / Vehicle-months observed`

**11. Vehicle Utilization Rate**
- *Definition:* Percentage of available shift time the vehicle is actively used (moving or at customer stop) vs. idle/unavailable
- *Calculation:* `(Active Time / Total Shift Duration) × 100%`

**12. Traffic Delay Frequency and Duration**
- *Definition:* How often and how long vehicles experience significant speed reduction due to traffic
- *Calculation:* From GPS: episodes where speed < threshold (e.g., 10 km/h) for > 5 minutes in known traffic-prone zones, not at customer locations. Aggregate by time-of-day, day-of-week, and geographic zone.

---

### 2.2 Bottleneck Identification Techniques

#### Performance Spectrum and Duration Analysis

In process mining, **performance annotation** overlays timing information onto discovered process models. Each transition (arc) in the process map is colored and sized to reflect:
- **Average waiting time** before the activity starts (waiting = bottleneck indicator)
- **Average activity duration** (service time)
- **Variance** in timing (high variance = unpredictability)

In Celonis or Disco, this produces a "performance map" where thick, red-colored arcs immediately highlight where time accumulates. For Speedy Parcels, we would expect to see:

- **Depot departure  Stop 1:** Morning rush hour traffic creating a bottleneck in this first segment, particularly for routes leaving depot after 8:00 AM
- **Stop N  Stop N+1 in certain geographic zones:** Urban core transitions taking 3–4x longer than suburban transitions despite similar planned distances
- **"Arrive Customer"  "Delivery Success":** High variance suggesting parking and customer interaction as variable bottlenecks

#### Dotted Chart Analysis

The **dotted chart** (time-activity matrix) plots all events for all cases on a single chart, with time on the x-axis and cases on the y-axis. Each event is a dot. This visualization immediately reveals:
- **Synchronization patterns:** If many vehicles arrive at certain areas at the same time (causing congestion)
- **Late starts:** Cases where the first event of the day is consistently later than others
- **Long-running cases:** Cases that extend significantly beyond the normal shift end (overtime)
- **Dead time:** Periods within a shift where no events occur for a case (unexplained idle time)

#### Variant Analysis for Bottleneck Attribution

Process variants are distinct traces (unique sequences of activities). By comparing **high-frequency, low-duration variants** (efficient routes) against **low-frequency, high-duration variants** (problematic routes), we can identify what structural differences account for performance gaps.

For example:
- **Variant A (accounts for 40% of cases, average duration 8.5h):** Depot  35 stops  Depot, no failed deliveries, no unscheduled stops
- **Variant B (accounts for 15% of cases, average duration 10.2h):** Depot  28 stops  Unscheduled Stop (40 min)  7 remaining stops  Depot
- **Variant C (accounts for 12% of cases, average duration 9.8h):** Depot  stops  3× Failed Delivery loops  Depot

Variant B points to vehicle reliability as a bottleneck; Variant C points to failed delivery management as a bottleneck.

#### Spatial Bottleneck Mapping

By projecting GPS-derived delay events onto a geographic map, we can identify **traffic hotspots** — specific road segments or intersections where cumulative delay across all vehicles is highest. Overlaying this with time-of-day heat maps reveals:
- **Which zones to avoid during morning rush (7:30–9:30 AM)**
- **Which zones have chronic parking problems (high "Arrive Customer"  "Delivery Success" duration)**
- **Which zones generate disproportionate failed deliveries** (residential areas where occupancy during delivery hours is low)

#### Quantifying Bottleneck Impact

For each identified bottleneck, calculate:
- **Frequency:** How many cases are affected
- **Average time lost per occurrence** (compared to the case without this bottleneck)
- **Total time lost = Frequency × Avg time lost × Cases affected / time period**
- **Knock-on cost:** Express in driver overtime cost (hourly rate × total hours), fuel cost (idle fuel consumption × fuel price), and missed delivery cost (redelivery cost × affected packages)

For instance, if traffic delays in Zone X add an average of 25 minutes to 60% of all Vehicle-Days, and there are 30 vehicles × 6 months = ~3,900 vehicle-days, then total delay = 3,900 × 0.60 × 25 min = ~97,500 driver-minutes = ~1,625 driver-hours  at €25/hour = ~€40,600 in labor cost alone, not counting overtime and fuel.

---

## Section 3: Root Cause Analysis for Inefficiencies

### 3.1 Candidate Root Causes

#### Root Cause 1: Suboptimal Static Route Planning

The dispatch system likely generates routes once, in the morning, without real-time adaptation. Static routes are optimized at planning time based on average historical conditions, but if actual traffic deviates significantly from assumptions, the entire route plan becomes suboptimal.

**Process Mining Validation:** Conformance checking will show whether the planned route sequence is routinely deviated from by drivers (who may self-correct based on local knowledge). If drivers who deviate from planned sequences perform *better* than those who follow them faithfully (measured by OTDR and total shift duration), this is evidence that static planning is suboptimal. **Variant analysis** comparing "deviate from plan" vs. "follow plan" traces on identical routes can test this hypothesis.

#### Root Cause 2: Inaccurate Travel Time Estimation

The dispatch system's planned arrival times may use oversimplified travel time models (e.g., distance ÷ assumed average speed) that don't account for time-of-day traffic variation, day-of-week patterns, or seasonal effects.

**Process Mining Validation:** Calculate the **systematic bias** in planned vs. actual travel times: `Bias = Mean(Actual_Travel_Time - Planned_Travel_Time)` segmented by departure time slot and route zone. A consistent positive bias (actual slower than planned) in morning slots and certain zones, but near-zero bias in off-peak slots, would confirm inaccurate time-of-day modeling. Six months of data provides sufficient sample sizes for statistical significance testing.

#### Root Cause 3: Traffic Congestion Patterns Not Accounted For

Closely related to Route 2, but distinct: even if average speeds are correct, the *variance* introduced by traffic congestion may not be modeled. A stop that is on average 10 minutes away may range from 5 to 35 minutes depending on traffic, creating unpredictable cascading delays.

**Process Mining Validation:** Correlate GPS-derived travel times for each road segment with external traffic data (e.g., Google Maps historical traffic API or HERE Traffic). Identify which segments show the highest *coefficient of variation* (CV = std deviation / mean) in travel time. Routes with high concentrations of high-CV segments will systematically underperform on OTDR because early stops may consume time windows intended for later stops.

#### Root Cause 4: High Service Time Variability at Customer Locations

Average service time might be acceptable, but high variance creates unpredictability. Some stops might take 2 minutes (parcel left at door) while others take 15 minutes (signature required, customer needs help with heavy item, access code issues).

**Process Mining Validation:** Analyze the distribution of "Arrive Customer"  "Depart Customer" durations. A normal distribution would suggest random variability; a multimodal distribution would suggest distinct sub-types of delivery interactions that could be planned for separately. Correlate dwell time with: package size/weight (from dispatch system), delivery type (residential vs. business), time of day, and specific addresses. Addresses with chronically high service time can be flagged.

#### Root Cause 5: Vehicle Reliability and Unscheduled Maintenance

The event log shows at least one "Unscheduled Stop - Engine Warning Light" event. If this is common, it creates severe mid-shift disruptions — the vehicle may need to be recovered, packages redistributed, and drivers left waiting.

**Process Mining Validation:** Cross-reference the maintenance logs with vehicle usage patterns in the GPS data. Calculate:
- **Mileage between unscheduled failures** per vehicle (MTBF - Mean Time Between Failures)
- **Correlation between pre-failure usage intensity** (hours driven, idle time, stop-start frequency) and subsequent failure probability
- **Time since last scheduled maintenance** at point of failure (were failures preceded by delayed maintenance?)

If certain vehicles have dramatically lower MTBF and those vehicles also have deferred scheduled maintenance, this is strong evidence for a predictive maintenance opportunity.

#### Root Cause 6: Driver Behavior and Skill Differences

Drivers have different levels of route familiarity, different driving styles (aggressive acceleration and braking increase fuel consumption and wear), different tendencies regarding parking (circling vs. using a more distant spot), and different customer interaction approaches.

**Process Mining Validation:** 
- **Driver performance benchmarking:** For drivers who have operated on the same routes (ensuring comparability), compare OTDR, average service time, failed delivery rate, and estimated fuel consumption.
- **Dwell time analysis by driver:** Decompose dwell time into "time from arrival to first scan" (finding parking/access), "time from first scan to delivery success scan" (actual customer interaction), and "time from delivery scan to depart scan" (administrative/preparation). Drivers with consistently high "arrival to first scan" times likely struggle with parking or access, suggesting need for guidance or route redesign.
- **Driving style from GPS:** Calculate average speed variance, frequency of hard braking events (rapid speed decrease), and engine idling duration per driver. High idling + high speed variance = aggressive driving style with increased fuel and wear costs.

#### Root Cause 7: Failed Delivery Management

Failed deliveries are expensive: the first attempt consumes full travel cost, and re-delivery doubles the cost per package. If 15% of packages require re-delivery and each re-delivery adds 20 minutes plus fuel, the cost impact across a fleet is substantial.

**Process Mining Validation:**
- Analyze when failed deliveries occur (time of day, day of week) — are they concentrated in morning slots when residential customers are at work?
- Identify repeat-offender addresses (same address failing delivery multiple times across different days) — these need proactive intervention
- Check whether any pre-notification was sent (from dispatch system communication logs) for failed deliveries vs. successful ones — if pre-notification is correlated with success, the data makes the business case for customer notification systems

---

## Section 4: Data-Driven Optimization Strategies

### Strategy 1: Dynamic, Real-Time Route Optimization with Traffic Integration

#### Targeted Inefficiency
Static route plans that do not account for real-time traffic conditions, causing cascading delays when early-route congestion propagates to late-day delivery failures.

#### Root Cause Addressed
Suboptimal static routing + inaccurate travel time estimation + traffic congestion patterns.

#### Process Mining Support
Conformance checking has quantified the magnitude and frequency of timing deviations. The performance map has identified which route segments and time-of-day windows consistently generate delays. Six months of GPS data provides a rich historical dataset of actual travel times by segment, time-of-day, and day-of-week — a training dataset for a predictive travel time model.

#### Implementation Detail
1. **Historical Model:** Build a segment-level travel time model using 6 months of GPS trajectory data, segmented by 30-minute time slots and day of week. This gives realistic expected travel times that replace the oversimplified dispatch system estimates.
2. **Real-Time Integration:** Integrate a real-time traffic data feed (Google Maps Platform, HERE, or TomTom Traffic API) into the dispatch system. At route-start and at each completed stop, the remaining route should be recalculated using current traffic conditions.
3. **Re-Sequencing Logic:** When real-time data indicates that the next planned stop is in a congested zone, the optimization engine should evaluate whether reordering the remaining stops (within feasibility constraints like time windows and vehicle load) would improve total route completion time.
4. **Driver App Integration:** Provide the optimized sequence to drivers via a mobile app with turn-by-turn navigation, replacing static paper/PDF route sheets.

#### Expected KPI Impact
- **OTDR:** Estimated +8 to +15 percentage points (literature on dynamic routing shows 10–20% improvement in punctuality for urban last-mile operations)
- **Travel Time Ratio:** Reduced by rerouting around congestion (estimated -10 to -15% in total travel time)
- **Fuel Consumption per Package:** Reduced due to less stop-and-go traffic time (estimated -5 to -10%)
- **Overtime Hours:** Reduced as routes complete more predictably within shift (estimated -20%)

---

### Strategy 2: Proactive Failed Delivery Prevention through Customer Pre-Notification and Time Window Optimization

#### Targeted Inefficiency
High failed delivery rate (assumed 10–20% based on the log snippet showing failed attempts as a visible variant), consuming double the delivery cost per affected package.

#### Root Cause Addressed
Customers not home during delivery windows; time windows not aligned with customer availability patterns; lack of pre-notification.

#### Process Mining Support
Analysis of failed delivery events in the event log reveals:
- **Time-of-day pattern:** If >70% of failed deliveries occur before 12:00 PM on residential addresses, this indicates customers are at work during morning delivery windows.
- **Address repeat rate:** If 30% of failed deliveries occur at addresses that have failed previously, a persistent unavailability problem is present at specific locations.
- **Time window analysis:** Packages with specific time windows requested in dispatch have lower failure rates than those with no time window specified (if the data supports this, it validates that customer-specified windows are more reliable).

#### Implementation Detail
1. **Pre-Delivery SMS/Email Notification:** Implement automated customer notification 60–90 minutes before estimated arrival (using the real-time route system from Strategy 1, which can provide reliable ETAs). Include a link allowing the customer to: confirm availability, designate a safe drop location, redirect to a neighbor, or reschedule.
2. **Smart Time Window Assignment:** Analyze the historical failed delivery data by address type, neighborhood, and time-of-day. For residential addresses with historically high morning failure rates, default the time window to afternoon slots or prompt customers to choose their window during order placement.
3. **Dynamic Redelivery Scheduling:** When a delivery fails, the package should automatically be re-queued into the next day's route for the same area, pre-inserted into an optimal position — rather than being added arbitrarily to whatever route happens to return that direction.
4. **Persistent Problem Addresses:** Addresses with 3 failed attempts over 6 months should trigger a workflow: contact customer to arrange an alternative (PUDO - Pick-Up/Drop-Off location, locker, neighbor authorization), and flag address in the dispatch system to always be scheduled with a customer-confirmed time window.

#### Expected KPI Impact
- **Failed Delivery Rate:** Estimated reduction from, say, 15% to 5–7% (industry data shows pre-notification reduces failed deliveries by 30–50%)
- **Re-Delivery Rate:** Proportional reduction
- **Cost per Delivered Package:** Significant reduction (re-delivery cost elimination)
- **OTDR:** Improvement because fewer unplanned re-delivery loops consume time from the primary route
- **Customer Satisfaction:** Measurable improvement in net promoter score / customer ratings

---

### Strategy 3: Predictive Vehicle Maintenance Scheduling Based on Usage Patterns

#### Targeted Inefficiency
Unscheduled vehicle breakdowns during operational shifts, causing mid-route disruptions, incomplete delivery rounds, and significant recovery costs.

#### Root Cause Addressed
Reactive maintenance practices that don't account for actual vehicle usage intensity; potential deferred scheduled maintenance; correlation between usage patterns and failure risk.

#### Process Mining Support
Cross-referencing GPS data (engine hours, idle time, hard acceleration/braking events, mileage by road type) with maintenance records reveals:
- **MTBF analysis by vehicle:** Which vehicles have below-average time/mileage between failures?
- **Pre-failure signatures:** Do failures tend to occur after periods of intensive use (e.g., multiple long shifts with high stop frequency and heavy urban driving)?
- **Maintenance schedule adherence:** Are unscheduled failures correlated with exceeding recommended mileage or time intervals since last service?
- **Vehicle age and failure rate correlation:** Are older vehicles in the fleet disproportionately responsible for unscheduled downtime?

Using the event log, we can calculate for each vehicle and each day:
- **Daily engine hours and total mileage**
- **Stop-start frequency** (proxy for brake and clutch wear)
- **Idle time** (engine heat stress)
- **Hard braking events** (from GPS speed profile)

These features become inputs to a **logistic regression or survival analysis model** that predicts the probability of a breakdown event within the next 7–14 days, conditioned on recent usage history.

#### Implementation Detail
1. **Usage-Based Maintenance Triggers:** Replace fixed-interval (every N days or M km) maintenance schedules with usage-intensity-adjusted schedules. A vehicle doing dense urban stop-start delivery at 35 stops per day degrades faster than one doing suburban highway driving — the maintenance interval should reflect this.
2. **Predictive Risk Scoring:** Implement a weekly maintenance risk score per vehicle, calculated from the usage pattern model. Vehicles above a threshold risk score are flagged for preventive inspection before the threshold mileage is reached.
3. **Pre-Shift Vehicle Health Checks:** Use OBD-II diagnostic data (if available from GPS units) to flag vehicles with active fault codes before shift start, preventing the "Engine Warning Light" mid-route event in the first place.
4. **Fleet Composition Review:** If certain vehicle makes/models show substantially higher unscheduled repair rates, the data makes a quantitative business case for accelerated replacement of high-failure vehicles.
5. **Maintenance Scheduling Optimization:** Schedule maintenance on vehicles during naturally low-utilization periods (weekends, public holidays) to minimize operational disruption.

#### Expected KPI Impact
- **Unscheduled Maintenance Downtime:** Estimated reduction of 40–60% (predictive maintenance literature commonly reports this range)
- **Vehicle Utilization Rate:** Improvement as fewer vehicles are unavailable due to mid-shift breakdowns
- **Maintenance Cost per Vehicle:** Reduction in expensive emergency repairs vs. planned preventive maintenance (emergency repairs typically cost 3–5× planned maintenance for the same intervention)
- **OTDR:** Improvement as mid-route breakdown variants are eliminated
- **Fuel Consumption:** Improvement as well-maintained vehicles operate more efficiently

---

### Strategy 4 (Bonus): Targeted Driver Coaching Based on Performance Variant Analysis

#### Targeted Inefficiency
Performance variance between drivers on comparable routes, suggesting best practices are not uniformly applied.

#### Root Cause Addressed
Driver behavior and skill differences in parking, customer interaction efficiency, driving style, and route adaptation.

#### Process Mining Support
After controlling for route and traffic conditions, driver-level variant analysis may reveal that the top quartile of drivers complete routes 15–20% faster than the bottom quartile. Decomposing where this time difference comes from (dwell time, inter-stop travel time, number of failed deliveries) allows targeted coaching.

#### Implementation Detail
1. **Anonymized Performance Benchmarking:** Share aggregate performance metrics (service time, delivery success rate, route completion rate) with drivers, showing where they stand relative to anonymized peers. Gamification elements (leaderboards, achievements) can increase engagement.
2. **Peer Learning Program:** Pair underperforming drivers with high-performing peers for shadowing shifts, specifically targeting the activities where the performance gap is largest (e.g., if an underperformer's bottleneck is parking/access time, pair them with a driver who excels at this).
3. **Driving Style Coaching:** Use GPS-derived driving behavior metrics (harsh braking, excessive idling, high-speed driving) to provide personalized feedback. Eco-driving training has been shown to reduce fuel consumption by 5–15%.
4. **Route Familiarity Program:** Assign drivers to consistent geographic territories rather than random daily reassignment, allowing them to build local knowledge (parking spots, access codes, building layouts) that reduces service time.

#### Expected KPI Impact
- **Average Service Time:** -10 to -20% for coached drivers
- **Fuel Consumption:** -5 to -15% improvement in driving style metrics
- **Failed Delivery Rate:** -5 to -10% improvement from better customer interaction practices
- **Driver Satisfaction/Retention:** Positive effect from investment in professional development (important in a tight labor market)

---

## Section 5: Operational Constraints and Continuous Monitoring

### 5.1 Accounting for Operational Constraints

Any optimization strategy must be implemented within the hard and soft constraints that govern last-mile delivery operations. Ignoring these constraints renders recommendations impractical and potentially illegal.

#### Driver Working Hours and Regulations
- **Legal constraint:** EU Working Time Directive and/or national transport regulations impose limits on daily driving time (e.g., 9 hours), mandatory break periods (45 minutes after 4.5 hours of driving), and minimum rest periods between shifts (11 hours).
- **Process Mining Integration:** The event log, combined with GPS, allows automatic detection of working time regulation compliance. Any route optimization algorithm (Strategy 1) must include these constraints as hard limits. The process mining analysis can also identify how frequently the current operations approach or breach these limits (the "Overtime logged" note in the example log is a red flag).
- **Practical safeguard:** Dynamic re-routing (Strategy 1) should never extend a driver's working day beyond the legal maximum, even if it means deferring remaining stops to the next day. The system should alert dispatchers when a driver's route is projected to breach working time limits, allowing proactive package redistribution.

#### Vehicle Capacity Constraints
- **Constraint:** Each vehicle has a maximum capacity in volume (m³) and weight (kg). Route optimization must ensure that at any point on the route, the remaining load does not exceed capacity.
- **Process Mining Insight:** If the event log reveals that some vehicles consistently depart the depot at near-maximum capacity while others depart underloaded, this suggests load balancing opportunities. Better load distribution could reduce the number of vehicles needed (fewer vehicle-days, lower fleet cost) or enable more stops per vehicle-day.
- **Constraint in Strategy 1:** The dynamic re-sequencing algorithm must maintain load feasibility — it cannot re-route a van to pick up additional packages if this would exceed weight limits.

#### Customer Time Windows
- **Constraint:** Customer-requested delivery windows are contractual commitments. Any route optimization must treat these as hard constraints (packages with narrow windows must be delivered within them) or soft constraints with penalty costs.
- **Process Mining Insight:** The conformance analysis will have classified time windows into high-compliance and low-compliance categories. Windows that are consistently missed, despite driver best efforts, may indicate that the window is unrealistically narrow given traffic conditions — an insight that should feed back into the commercial team's commitments to customers.
- **Strategy 2 Enhancement:** Customer notification (Strategy 2) can convert some "hard window" packages into "flexible window" packages (if customers agree), giving the routing algorithm more flexibility and improving overall route efficiency.

#### Fleet Availability and Maintenance Windows
- **Constraint:** Predictive maintenance (Strategy 3) creates maintenance windows when specific vehicles are unavailable. Route planning must account for the reduced available fleet on maintenance days.
- **Process Mining Integration:** Maintenance scheduling should be coordinated with dispatch planning — vehicles scheduled for maintenance on a Monday should not be assigned to Monday's heaviest routes. This requires the maintenance scheduling system and dispatch system to share data (currently they likely operate in silos).

---

### 5.2 Continuous Monitoring Plan

Post-implementation monitoring is essential to verify that changes achieve intended effects, that no unintended consequences emerge, and that the organization can detect and respond to new issues as the operating environment evolves.

#### Monitoring Infrastructure

Deploy a **real-time process mining dashboard** (e.g., Celonis EMS, SAP Signavio, or a custom PM4Py-based solution) that continuously ingests the event log streams and refreshes KPIs daily or weekly. The dashboard should have role-specific views:

**Operational Manager View (Daily):**
- Today's fleet OTDR (live, updating as deliveries complete)
- Vehicles currently running late (deviation from planned schedule > threshold)
- Active unscheduled stops or warning events requiring intervention
- Predicted end-of-shift completion status for each vehicle

**Performance Analyst View (Weekly):**
- KPI trend charts for all 12 defined KPIs over the past 4/8/12 weeks
- Conformance score trend (are deviations increasing or decreasing post-implementation?)
- Failed delivery rate by zone, time window, and driver
- Variant distribution shift (are efficient variants becoming more common?)

**Strategic Leadership View (Monthly):**
- KPI vs. pre-implementation baseline comparison (quantified value delivered)
- Cost savings tracking (fuel, overtime, re-delivery, maintenance) with financial attribution
- Fleet health overview (MTBF trends, maintenance cost per vehicle)
- Driver performance distribution (identifying new outliers for coaching)

#### Specific Metrics and Thresholds for Alert Triggers

| Metric | Alert Threshold | Action |
|---|---|---|
| Daily OTDR | < 90% | Investigate: root cause drill-down by route, driver, zone |
| Failed Delivery Rate (weekly) | > 10% | Review notification system performance; check time window assignments |
| Vehicle Breakdown Events | Any unscheduled stop > 30 min | Immediate check: was vehicle flagged by predictive model? Update model if not |
| Driver working hours | > 9.5h actual shift | Compliance review; route rebalancing |
| Conformance Score (weekly avg) | < 0.80 | Review whether new route plans are realistic; retrain routing model |
| Fuel consumption per package | +10% vs. baseline | Investigate: new traffic patterns? Vehicle health? Driver behavior? |
| Service time (weekly avg) | +15% vs. baseline | Investigate: operational changes? New package types? Seasonal customer behavior? |

#### Process View Monitoring

Beyond KPIs, monitor the **process model itself** for drift using **concept drift detection** techniques. The process structure may change as:
- New residential developments change route geography
- Seasonal volume peaks (Christmas, summer) alter the variant distribution
- New service types are introduced (e.g., oversized parcel handling)
- Traffic infrastructure changes (new road, construction works) alter travel time patterns

A process mining system that re-discovers models quarterly and compares them to the previous model can automatically flag structural changes that require route or process redesign.

#### Feedback Loop to Optimization Systems

The monitoring system should close the loop: insights from ongoing monitoring should automatically feed back into the optimization systems:
- **Dynamic routing model:** Continuously retrained with new GPS travel time observations, ensuring the historical model remains current as road conditions, traffic patterns, and urban geography evolve.
- **Predictive maintenance model:** Continuously updated with new failure/no-failure observations, improving prediction accuracy over time.
- **Customer notification system:** Delivery success rates by time window and zone feed back into the ETA estimation engine, improving the accuracy of pre-notification messages.

#### Baseline Establishment and A/B Testing

Before full deployment of any strategy, establish **pre-implementation baselines** for all KPIs using the six months of historical data. Implement changes in a **controlled manner** — applying new routing to a subset of vehicles/routes while maintaining old processes for a comparable control group. This allows rigorous attribution of observed KPI improvements to specific interventions, rather than to external factors (weather, economic conditions, seasonal patterns). After 4–6 weeks, compare test and control groups on all KPIs; only then scale successful interventions fleet-wide.

---

## Summary: Integrated Value Creation

The process mining approach described here transforms Speedy Parcels' rich operational data into a continuous improvement engine. The five-stage framework — from event log construction through conformance checking, bottleneck identification, root cause analysis, and targeted optimization — creates a coherent, data-grounded narrative about *where* inefficiencies occur, *why* they occur, and *what* to do about them.

The three primary strategies (dynamic routing, proactive failed delivery prevention, and predictive maintenance) are not independent: they reinforce each other. Dynamic routing reduces the cascading delays that make customer notification ETAs unreliable; reliable ETAs make customer pre-notification effective; reduced failed deliveries and unplanned stops make vehicle-day durations more predictable, which makes maintenance scheduling less disruptive. Together, conservative estimates suggest potential to improve OTDR by 15–20 percentage points, reduce operational costs by 10–20%, and significantly enhance both customer satisfaction and driver working conditions.

The monitoring framework ensures these gains are sustained and that Speedy Parcels continues to improve as its operating environment evolves — making process mining not a one-time project but an enduring operational capability.